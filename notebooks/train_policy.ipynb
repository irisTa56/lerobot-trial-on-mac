{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on:\n",
    "https://github.com/huggingface/lerobot/blob/main/examples/3_train_policy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gym_pusht  # noqa: F401\n",
    "import torch\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from lerobot.common.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.configs.types import FeatureType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Device 'None' is not available. Switching to 'mps'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiffusionPolicy(\n",
       "  (normalize_inputs): Normalize(\n",
       "    (buffer_observation_image): ParameterDict(\n",
       "        (mean): Parameter containing: [torch.mps.FloatTensor of size 3x1x1]\n",
       "        (std): Parameter containing: [torch.mps.FloatTensor of size 3x1x1]\n",
       "    )\n",
       "    (buffer_observation_state): ParameterDict(\n",
       "        (max): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "        (min): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "    )\n",
       "  )\n",
       "  (normalize_targets): Normalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (max): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "        (min): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "    )\n",
       "  )\n",
       "  (unnormalize_outputs): Unnormalize(\n",
       "    (buffer_action): ParameterDict(\n",
       "        (max): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "        (min): Parameter containing: [torch.mps.FloatTensor of size 2]\n",
       "    )\n",
       "  )\n",
       "  (diffusion): DiffusionModel(\n",
       "    (rgb_encoder): DiffusionRgbEncoder(\n",
       "      (center_crop): CenterCrop(size=(84, 84))\n",
       "      (maybe_random_crop): RandomCrop(size=(84, 84), padding=None)\n",
       "      (backbone): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (4): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(16, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pool): SpatialSoftmax(\n",
       "        (nets): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (unet): DiffusionConditionalUnet1d(\n",
       "      (diffusion_step_encoder): Sequential(\n",
       "        (0): DiffusionSinusoidalPosEmb()\n",
       "        (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (2): Mish()\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (down_modules): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=4096, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=4096, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (mid_modules): ModuleList(\n",
       "        (0-1): 2 x DiffusionConditionalResidualBlock1d(\n",
       "          (conv1): DiffusionConv1dBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "              (2): Mish()\n",
       "            )\n",
       "          )\n",
       "          (cond_encoder): Sequential(\n",
       "            (0): Mish()\n",
       "            (1): Linear(in_features=260, out_features=4096, bias=True)\n",
       "          )\n",
       "          (conv2): DiffusionConv1dBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(2048, 2048, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "              (2): Mish()\n",
       "            )\n",
       "          )\n",
       "          (residual_conv): Identity()\n",
       "        )\n",
       "      )\n",
       "      (up_modules): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(4096, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(4096, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=2048, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): ConvTranspose1d(1024, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (1): DiffusionConditionalResidualBlock1d(\n",
       "            (conv1): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (cond_encoder): Sequential(\n",
       "              (0): Mish()\n",
       "              (1): Linear(in_features=260, out_features=1024, bias=True)\n",
       "            )\n",
       "            (conv2): DiffusionConv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (residual_conv): Identity()\n",
       "          )\n",
       "          (2): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (final_conv): Sequential(\n",
       "        (0): DiffusionConv1dBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "            (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "            (2): Mish()\n",
       "          )\n",
       "        )\n",
       "        (1): Conv1d(512, 2, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a directory to store the training checkpoint.\n",
    "output_directory = Path(\"outputs/train/example_pusht_diffusion\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Select your device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Number of offline training steps (we'll only do offline training for this example.)\n",
    "# Adjust as you prefer. 5000 steps are needed to get something worth evaluating.\n",
    "training_steps = 5000\n",
    "log_freq = 1\n",
    "\n",
    "# When starting from scratch (i.e. not from a pretrained policy), we need to specify 2 things before\n",
    "# creating the policy:\n",
    "#   - input/output shapes: to properly size the policy\n",
    "#   - dataset stats: for normalization and denormalization of input/outputs\n",
    "dataset_metadata = LeRobotDatasetMetadata(\"lerobot/pusht\")\n",
    "features = dataset_to_policy_features(dataset_metadata.features)\n",
    "output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "input_features = {key: ft for key, ft in features.items() if key not in output_features}\n",
    "\n",
    "# Policies are initialized with a configuration class, in this case `DiffusionConfig`. For this example,\n",
    "# we'll just use the defaults and so no arguments other than input/output features need to be passed.\n",
    "cfg = DiffusionConfig(input_features=input_features, output_features=output_features)\n",
    "\n",
    "# We can now instantiate our policy with this config and the dataset stats.\n",
    "policy = DiffusionPolicy(cfg, dataset_stats=dataset_metadata.stats)\n",
    "policy.train()\n",
    "policy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb936d3e1fe47d0b2755292a7e62654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another policy-dataset interaction is with the delta_timestamps. Each policy expects a given number frames\n",
    "# which can differ for inputs, outputs and rewards (if there are some).\n",
    "delta_timestamps = {\n",
    "    \"observation.image\": [i / dataset_metadata.fps for i in cfg.observation_delta_indices],\n",
    "    \"observation.state\": [i / dataset_metadata.fps for i in cfg.observation_delta_indices],\n",
    "    \"action\": [i / dataset_metadata.fps for i in cfg.action_delta_indices],\n",
    "}\n",
    "\n",
    "# In this case with the standard configuration for Diffusion Policy, it is equivalent to this:\n",
    "assert delta_timestamps == {\n",
    "    # Load the previous image and state at -0.1 seconds before current frame,\n",
    "    # then load current image and state corresponding to 0.0 second.\n",
    "    \"observation.image\": [-0.1, 0.0],\n",
    "    \"observation.state\": [-0.1, 0.0],\n",
    "    # Load the previous action (-0.1), the next action to be executed (0.0),\n",
    "    # and 14 future actions with a 0.1 seconds spacing. All these actions will be\n",
    "    # used to supervise the policy.\n",
    "    \"action\": [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4],\n",
    "}\n",
    "\n",
    "# We can then instantiate the dataset with these delta_timestamps configuration.\n",
    "dataset = LeRobotDataset(\"lerobot/pusht\", delta_timestamps=delta_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we create our optimizer and dataloader for offline training.\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 1.070\n",
      "step: 1 loss: 6.440\n",
      "step: 2 loss: 1.135\n",
      "step: 3 loss: 1.418\n",
      "step: 4 loss: 1.061\n",
      "step: 5 loss: 0.978\n",
      "step: 6 loss: 1.109\n",
      "step: 7 loss: 1.026\n",
      "step: 8 loss: 0.976\n",
      "step: 9 loss: 0.948\n",
      "step: 10 loss: 0.974\n",
      "step: 11 loss: 1.041\n",
      "step: 12 loss: 0.991\n",
      "step: 13 loss: 0.982\n",
      "step: 14 loss: 0.932\n",
      "step: 15 loss: 0.900\n",
      "step: 16 loss: 0.906\n",
      "step: 17 loss: 0.902\n",
      "step: 18 loss: 0.939\n",
      "step: 19 loss: 0.886\n",
      "step: 20 loss: 0.838\n",
      "step: 21 loss: 0.807\n",
      "step: 22 loss: 0.759\n",
      "step: 23 loss: 0.642\n",
      "step: 24 loss: 0.557\n",
      "step: 25 loss: 0.488\n",
      "step: 26 loss: 0.473\n",
      "step: 27 loss: 0.439\n",
      "step: 28 loss: 0.414\n",
      "step: 29 loss: 0.333\n",
      "step: 30 loss: 0.390\n",
      "step: 31 loss: 0.339\n",
      "step: 32 loss: 0.304\n",
      "step: 33 loss: 0.230\n",
      "step: 34 loss: 0.272\n",
      "step: 35 loss: 0.248\n",
      "step: 36 loss: 0.214\n",
      "step: 37 loss: 0.231\n",
      "step: 38 loss: 0.198\n",
      "step: 39 loss: 0.225\n",
      "step: 40 loss: 0.233\n",
      "step: 41 loss: 0.223\n",
      "step: 42 loss: 0.218\n",
      "step: 43 loss: 0.197\n",
      "step: 44 loss: 0.198\n",
      "step: 45 loss: 0.193\n",
      "step: 46 loss: 0.248\n",
      "step: 47 loss: 0.188\n",
      "step: 48 loss: 0.207\n",
      "step: 49 loss: 0.150\n",
      "step: 50 loss: 0.150\n",
      "step: 51 loss: 0.194\n",
      "step: 52 loss: 0.156\n",
      "step: 53 loss: 0.138\n",
      "step: 54 loss: 0.159\n",
      "step: 55 loss: 0.154\n",
      "step: 56 loss: 0.147\n",
      "step: 57 loss: 0.131\n",
      "step: 58 loss: 0.142\n",
      "step: 59 loss: 0.130\n",
      "step: 60 loss: 0.134\n",
      "step: 61 loss: 0.124\n",
      "step: 62 loss: 0.093\n",
      "step: 63 loss: 0.159\n",
      "step: 64 loss: 0.141\n",
      "step: 65 loss: 0.122\n",
      "step: 66 loss: 0.145\n",
      "step: 67 loss: 0.129\n",
      "step: 68 loss: 0.125\n",
      "step: 69 loss: 0.147\n",
      "step: 70 loss: 0.133\n",
      "step: 71 loss: 0.107\n",
      "step: 72 loss: 0.122\n",
      "step: 73 loss: 0.113\n",
      "step: 74 loss: 0.157\n",
      "step: 75 loss: 0.148\n",
      "step: 76 loss: 0.107\n",
      "step: 77 loss: 0.147\n",
      "step: 78 loss: 0.094\n",
      "step: 79 loss: 0.126\n",
      "step: 80 loss: 0.113\n",
      "step: 81 loss: 0.147\n",
      "step: 82 loss: 0.094\n",
      "step: 83 loss: 0.127\n",
      "step: 84 loss: 0.104\n",
      "step: 85 loss: 0.138\n",
      "step: 86 loss: 0.098\n",
      "step: 87 loss: 0.139\n",
      "step: 88 loss: 0.128\n",
      "step: 89 loss: 0.084\n",
      "step: 90 loss: 0.142\n",
      "step: 91 loss: 0.123\n",
      "step: 92 loss: 0.113\n",
      "step: 93 loss: 0.087\n",
      "step: 94 loss: 0.108\n",
      "step: 95 loss: 0.091\n",
      "step: 96 loss: 0.086\n",
      "step: 97 loss: 0.085\n",
      "step: 98 loss: 0.088\n",
      "step: 99 loss: 0.082\n",
      "step: 100 loss: 0.101\n",
      "step: 101 loss: 0.085\n",
      "step: 102 loss: 0.075\n",
      "step: 103 loss: 0.073\n",
      "step: 104 loss: 0.101\n",
      "step: 105 loss: 0.119\n",
      "step: 106 loss: 0.087\n",
      "step: 107 loss: 0.092\n",
      "step: 108 loss: 0.097\n",
      "step: 109 loss: 0.081\n",
      "step: 110 loss: 0.074\n",
      "step: 111 loss: 0.081\n",
      "step: 112 loss: 0.108\n",
      "step: 113 loss: 0.126\n",
      "step: 114 loss: 0.091\n",
      "step: 115 loss: 0.097\n",
      "step: 116 loss: 0.073\n",
      "step: 117 loss: 0.099\n",
      "step: 118 loss: 0.119\n",
      "step: 119 loss: 0.094\n",
      "step: 120 loss: 0.063\n",
      "step: 121 loss: 0.079\n",
      "step: 122 loss: 0.101\n",
      "step: 123 loss: 0.095\n",
      "step: 124 loss: 0.103\n",
      "step: 125 loss: 0.113\n",
      "step: 126 loss: 0.082\n",
      "step: 127 loss: 0.115\n",
      "step: 128 loss: 0.071\n",
      "step: 129 loss: 0.064\n",
      "step: 130 loss: 0.108\n",
      "step: 131 loss: 0.075\n",
      "step: 132 loss: 0.080\n",
      "step: 133 loss: 0.087\n",
      "step: 134 loss: 0.088\n",
      "step: 135 loss: 0.076\n",
      "step: 136 loss: 0.080\n",
      "step: 137 loss: 0.076\n",
      "step: 138 loss: 0.075\n",
      "step: 139 loss: 0.086\n",
      "step: 140 loss: 0.063\n",
      "step: 141 loss: 0.065\n",
      "step: 142 loss: 0.066\n",
      "step: 143 loss: 0.063\n",
      "step: 144 loss: 0.068\n",
      "step: 145 loss: 0.060\n",
      "step: 146 loss: 0.069\n",
      "step: 147 loss: 0.070\n",
      "step: 148 loss: 0.089\n",
      "step: 149 loss: 0.058\n",
      "step: 150 loss: 0.077\n",
      "step: 151 loss: 0.069\n",
      "step: 152 loss: 0.092\n",
      "step: 153 loss: 0.128\n",
      "step: 154 loss: 0.085\n",
      "step: 155 loss: 0.067\n",
      "step: 156 loss: 0.055\n",
      "step: 157 loss: 0.091\n",
      "step: 158 loss: 0.091\n",
      "step: 159 loss: 0.096\n",
      "step: 160 loss: 0.075\n",
      "step: 161 loss: 0.064\n",
      "step: 162 loss: 0.078\n",
      "step: 163 loss: 0.072\n",
      "step: 164 loss: 0.092\n",
      "step: 165 loss: 0.068\n",
      "step: 166 loss: 0.098\n",
      "step: 167 loss: 0.073\n",
      "step: 168 loss: 0.086\n",
      "step: 169 loss: 0.085\n",
      "step: 170 loss: 0.074\n",
      "step: 171 loss: 0.056\n",
      "step: 172 loss: 0.069\n",
      "step: 173 loss: 0.095\n",
      "step: 174 loss: 0.118\n",
      "step: 175 loss: 0.108\n",
      "step: 176 loss: 0.073\n",
      "step: 177 loss: 0.091\n",
      "step: 178 loss: 0.046\n",
      "step: 179 loss: 0.084\n",
      "step: 180 loss: 0.075\n",
      "step: 181 loss: 0.081\n",
      "step: 182 loss: 0.084\n",
      "step: 183 loss: 0.068\n",
      "step: 184 loss: 0.061\n",
      "step: 185 loss: 0.105\n",
      "step: 186 loss: 0.051\n",
      "step: 187 loss: 0.071\n",
      "step: 188 loss: 0.078\n",
      "step: 189 loss: 0.092\n",
      "step: 190 loss: 0.079\n",
      "step: 191 loss: 0.063\n",
      "step: 192 loss: 0.069\n",
      "step: 193 loss: 0.067\n",
      "step: 194 loss: 0.065\n",
      "step: 195 loss: 0.082\n",
      "step: 196 loss: 0.071\n",
      "step: 197 loss: 0.077\n",
      "step: 198 loss: 0.086\n",
      "step: 199 loss: 0.078\n",
      "step: 200 loss: 0.044\n",
      "step: 201 loss: 0.102\n",
      "step: 202 loss: 0.091\n",
      "step: 203 loss: 0.080\n",
      "step: 204 loss: 0.069\n",
      "step: 205 loss: 0.091\n",
      "step: 206 loss: 0.093\n",
      "step: 207 loss: 0.086\n",
      "step: 208 loss: 0.088\n",
      "step: 209 loss: 0.059\n",
      "step: 210 loss: 0.077\n",
      "step: 211 loss: 0.064\n",
      "step: 212 loss: 0.080\n",
      "step: 213 loss: 0.067\n",
      "step: 214 loss: 0.053\n",
      "step: 215 loss: 0.057\n",
      "step: 216 loss: 0.084\n",
      "step: 217 loss: 0.059\n",
      "step: 218 loss: 0.090\n",
      "step: 219 loss: 0.060\n",
      "step: 220 loss: 0.065\n",
      "step: 221 loss: 0.057\n",
      "step: 222 loss: 0.086\n",
      "step: 223 loss: 0.074\n",
      "step: 224 loss: 0.075\n",
      "step: 225 loss: 0.082\n",
      "step: 226 loss: 0.079\n",
      "step: 227 loss: 0.064\n",
      "step: 228 loss: 0.067\n",
      "step: 229 loss: 0.078\n",
      "step: 230 loss: 0.091\n",
      "step: 231 loss: 0.082\n",
      "step: 232 loss: 0.066\n",
      "step: 233 loss: 0.073\n",
      "step: 234 loss: 0.074\n",
      "step: 235 loss: 0.049\n",
      "step: 236 loss: 0.068\n",
      "step: 237 loss: 0.081\n",
      "step: 238 loss: 0.068\n",
      "step: 239 loss: 0.069\n",
      "step: 240 loss: 0.093\n",
      "step: 241 loss: 0.067\n",
      "step: 242 loss: 0.074\n",
      "step: 243 loss: 0.087\n",
      "step: 244 loss: 0.072\n",
      "step: 245 loss: 0.063\n",
      "step: 246 loss: 0.061\n",
      "step: 247 loss: 0.082\n",
      "step: 248 loss: 0.064\n",
      "step: 249 loss: 0.069\n",
      "step: 250 loss: 0.065\n",
      "step: 251 loss: 0.082\n",
      "step: 252 loss: 0.089\n",
      "step: 253 loss: 0.069\n",
      "step: 254 loss: 0.084\n",
      "step: 255 loss: 0.073\n",
      "step: 256 loss: 0.058\n",
      "step: 257 loss: 0.069\n",
      "step: 258 loss: 0.047\n",
      "step: 259 loss: 0.053\n",
      "step: 260 loss: 0.045\n",
      "step: 261 loss: 0.060\n",
      "step: 262 loss: 0.056\n",
      "step: 263 loss: 0.068\n",
      "step: 264 loss: 0.071\n",
      "step: 265 loss: 0.068\n",
      "step: 266 loss: 0.058\n",
      "step: 267 loss: 0.068\n",
      "step: 268 loss: 0.073\n",
      "step: 269 loss: 0.081\n",
      "step: 270 loss: 0.071\n",
      "step: 271 loss: 0.068\n",
      "step: 272 loss: 0.048\n",
      "step: 273 loss: 0.072\n",
      "step: 274 loss: 0.085\n",
      "step: 275 loss: 0.067\n",
      "step: 276 loss: 0.058\n",
      "step: 277 loss: 0.079\n",
      "step: 278 loss: 0.045\n",
      "step: 279 loss: 0.059\n",
      "step: 280 loss: 0.051\n",
      "step: 281 loss: 0.069\n",
      "step: 282 loss: 0.117\n",
      "step: 283 loss: 0.066\n",
      "step: 284 loss: 0.047\n",
      "step: 285 loss: 0.064\n",
      "step: 286 loss: 0.068\n",
      "step: 287 loss: 0.063\n",
      "step: 288 loss: 0.086\n",
      "step: 289 loss: 0.043\n",
      "step: 290 loss: 0.061\n",
      "step: 291 loss: 0.081\n",
      "step: 292 loss: 0.095\n",
      "step: 293 loss: 0.062\n",
      "step: 294 loss: 0.060\n",
      "step: 295 loss: 0.085\n",
      "step: 296 loss: 0.073\n",
      "step: 297 loss: 0.068\n",
      "step: 298 loss: 0.078\n",
      "step: 299 loss: 0.067\n",
      "step: 300 loss: 0.060\n",
      "step: 301 loss: 0.075\n",
      "step: 302 loss: 0.067\n",
      "step: 303 loss: 0.061\n",
      "step: 304 loss: 0.052\n",
      "step: 305 loss: 0.061\n",
      "step: 306 loss: 0.077\n",
      "step: 307 loss: 0.050\n",
      "step: 308 loss: 0.079\n",
      "step: 309 loss: 0.054\n",
      "step: 310 loss: 0.073\n",
      "step: 311 loss: 0.055\n",
      "step: 312 loss: 0.077\n",
      "step: 313 loss: 0.090\n",
      "step: 314 loss: 0.091\n",
      "step: 315 loss: 0.066\n",
      "step: 316 loss: 0.069\n",
      "step: 317 loss: 0.051\n",
      "step: 318 loss: 0.061\n",
      "step: 319 loss: 0.060\n",
      "step: 320 loss: 0.076\n",
      "step: 321 loss: 0.061\n",
      "step: 322 loss: 0.103\n",
      "step: 323 loss: 0.049\n",
      "step: 324 loss: 0.076\n",
      "step: 325 loss: 0.065\n",
      "step: 326 loss: 0.058\n",
      "step: 327 loss: 0.062\n",
      "step: 328 loss: 0.066\n",
      "step: 329 loss: 0.070\n",
      "step: 330 loss: 0.045\n",
      "step: 331 loss: 0.076\n",
      "step: 332 loss: 0.070\n",
      "step: 333 loss: 0.045\n",
      "step: 334 loss: 0.077\n",
      "step: 335 loss: 0.061\n",
      "step: 336 loss: 0.059\n",
      "step: 337 loss: 0.050\n",
      "step: 338 loss: 0.056\n",
      "step: 339 loss: 0.068\n",
      "step: 340 loss: 0.055\n",
      "step: 341 loss: 0.055\n",
      "step: 342 loss: 0.062\n",
      "step: 343 loss: 0.039\n",
      "step: 344 loss: 0.069\n",
      "step: 345 loss: 0.051\n",
      "step: 346 loss: 0.084\n",
      "step: 347 loss: 0.067\n",
      "step: 348 loss: 0.060\n",
      "step: 349 loss: 0.117\n",
      "step: 350 loss: 0.067\n",
      "step: 351 loss: 0.065\n",
      "step: 352 loss: 0.062\n",
      "step: 353 loss: 0.062\n",
      "step: 354 loss: 0.052\n",
      "step: 355 loss: 0.076\n",
      "step: 356 loss: 0.073\n",
      "step: 357 loss: 0.062\n",
      "step: 358 loss: 0.057\n",
      "step: 359 loss: 0.055\n",
      "step: 360 loss: 0.065\n",
      "step: 361 loss: 0.065\n",
      "step: 362 loss: 0.050\n",
      "step: 363 loss: 0.074\n",
      "step: 364 loss: 0.061\n",
      "step: 365 loss: 0.064\n",
      "step: 366 loss: 0.062\n",
      "step: 367 loss: 0.054\n",
      "step: 368 loss: 0.041\n",
      "step: 369 loss: 0.045\n",
      "step: 370 loss: 0.076\n",
      "step: 371 loss: 0.046\n",
      "step: 372 loss: 0.056\n",
      "step: 373 loss: 0.075\n",
      "step: 374 loss: 0.042\n",
      "step: 375 loss: 0.076\n",
      "step: 376 loss: 0.050\n",
      "step: 377 loss: 0.071\n",
      "step: 378 loss: 0.049\n",
      "step: 379 loss: 0.065\n",
      "step: 380 loss: 0.055\n",
      "step: 381 loss: 0.067\n",
      "step: 382 loss: 0.068\n",
      "step: 383 loss: 0.049\n",
      "step: 384 loss: 0.064\n",
      "step: 385 loss: 0.077\n",
      "step: 386 loss: 0.051\n",
      "step: 387 loss: 0.068\n",
      "step: 388 loss: 0.088\n",
      "step: 389 loss: 0.057\n",
      "step: 390 loss: 0.071\n",
      "step: 391 loss: 0.063\n",
      "step: 392 loss: 0.053\n",
      "step: 393 loss: 0.075\n",
      "step: 394 loss: 0.067\n",
      "step: 395 loss: 0.057\n",
      "step: 396 loss: 0.056\n",
      "step: 397 loss: 0.043\n",
      "step: 398 loss: 0.078\n",
      "step: 399 loss: 0.049\n",
      "step: 400 loss: 0.047\n",
      "step: 401 loss: 0.044\n",
      "step: 402 loss: 0.089\n",
      "step: 403 loss: 0.062\n",
      "step: 404 loss: 0.060\n",
      "step: 405 loss: 0.055\n",
      "step: 406 loss: 0.049\n",
      "step: 407 loss: 0.048\n",
      "step: 408 loss: 0.063\n",
      "step: 409 loss: 0.054\n",
      "step: 410 loss: 0.049\n",
      "step: 411 loss: 0.067\n",
      "step: 412 loss: 0.058\n",
      "step: 413 loss: 0.062\n",
      "step: 414 loss: 0.054\n",
      "step: 415 loss: 0.056\n",
      "step: 416 loss: 0.053\n",
      "step: 417 loss: 0.061\n",
      "step: 418 loss: 0.071\n",
      "step: 419 loss: 0.072\n",
      "step: 420 loss: 0.058\n",
      "step: 421 loss: 0.057\n",
      "step: 422 loss: 0.075\n",
      "step: 423 loss: 0.067\n",
      "step: 424 loss: 0.067\n",
      "step: 425 loss: 0.071\n",
      "step: 426 loss: 0.051\n",
      "step: 427 loss: 0.045\n",
      "step: 428 loss: 0.051\n",
      "step: 429 loss: 0.058\n",
      "step: 430 loss: 0.072\n",
      "step: 431 loss: 0.053\n",
      "step: 432 loss: 0.055\n",
      "step: 433 loss: 0.064\n",
      "step: 434 loss: 0.062\n",
      "step: 435 loss: 0.056\n",
      "step: 436 loss: 0.058\n",
      "step: 437 loss: 0.055\n",
      "step: 438 loss: 0.056\n",
      "step: 439 loss: 0.052\n",
      "step: 440 loss: 0.060\n",
      "step: 441 loss: 0.036\n",
      "step: 442 loss: 0.068\n",
      "step: 443 loss: 0.051\n",
      "step: 444 loss: 0.067\n",
      "step: 445 loss: 0.049\n",
      "step: 446 loss: 0.056\n",
      "step: 447 loss: 0.061\n",
      "step: 448 loss: 0.056\n",
      "step: 449 loss: 0.069\n",
      "step: 450 loss: 0.075\n",
      "step: 451 loss: 0.073\n",
      "step: 452 loss: 0.055\n",
      "step: 453 loss: 0.080\n",
      "step: 454 loss: 0.048\n",
      "step: 455 loss: 0.058\n",
      "step: 456 loss: 0.063\n",
      "step: 457 loss: 0.052\n",
      "step: 458 loss: 0.063\n",
      "step: 459 loss: 0.055\n",
      "step: 460 loss: 0.063\n",
      "step: 461 loss: 0.075\n",
      "step: 462 loss: 0.040\n",
      "step: 463 loss: 0.079\n",
      "step: 464 loss: 0.054\n",
      "step: 465 loss: 0.051\n",
      "step: 466 loss: 0.048\n",
      "step: 467 loss: 0.055\n",
      "step: 468 loss: 0.048\n",
      "step: 469 loss: 0.058\n",
      "step: 470 loss: 0.056\n",
      "step: 471 loss: 0.069\n",
      "step: 472 loss: 0.050\n",
      "step: 473 loss: 0.066\n",
      "step: 474 loss: 0.094\n",
      "step: 475 loss: 0.041\n",
      "step: 476 loss: 0.048\n",
      "step: 477 loss: 0.059\n",
      "step: 478 loss: 0.036\n",
      "step: 479 loss: 0.050\n",
      "step: 480 loss: 0.066\n",
      "step: 481 loss: 0.057\n",
      "step: 482 loss: 0.052\n",
      "step: 483 loss: 0.083\n",
      "step: 484 loss: 0.064\n",
      "step: 485 loss: 0.071\n",
      "step: 486 loss: 0.050\n",
      "step: 487 loss: 0.069\n",
      "step: 488 loss: 0.043\n",
      "step: 489 loss: 0.044\n",
      "step: 490 loss: 0.061\n",
      "step: 491 loss: 0.036\n",
      "step: 492 loss: 0.063\n",
      "step: 493 loss: 0.056\n",
      "step: 494 loss: 0.071\n",
      "step: 495 loss: 0.070\n",
      "step: 496 loss: 0.048\n",
      "step: 497 loss: 0.094\n",
      "step: 498 loss: 0.075\n",
      "step: 499 loss: 0.036\n",
      "step: 500 loss: 0.055\n",
      "step: 501 loss: 0.059\n",
      "step: 502 loss: 0.052\n",
      "step: 503 loss: 0.070\n",
      "step: 504 loss: 0.054\n",
      "step: 505 loss: 0.048\n",
      "step: 506 loss: 0.063\n",
      "step: 507 loss: 0.061\n",
      "step: 508 loss: 0.055\n",
      "step: 509 loss: 0.070\n",
      "step: 510 loss: 0.079\n",
      "step: 511 loss: 0.048\n",
      "step: 512 loss: 0.041\n",
      "step: 513 loss: 0.068\n",
      "step: 514 loss: 0.075\n",
      "step: 515 loss: 0.052\n",
      "step: 516 loss: 0.057\n",
      "step: 517 loss: 0.068\n",
      "step: 518 loss: 0.063\n",
      "step: 519 loss: 0.061\n",
      "step: 520 loss: 0.073\n",
      "step: 521 loss: 0.060\n",
      "step: 522 loss: 0.062\n",
      "step: 523 loss: 0.075\n",
      "step: 524 loss: 0.073\n",
      "step: 525 loss: 0.057\n",
      "step: 526 loss: 0.093\n",
      "step: 527 loss: 0.072\n",
      "step: 528 loss: 0.060\n",
      "step: 529 loss: 0.091\n",
      "step: 530 loss: 0.070\n",
      "step: 531 loss: 0.075\n",
      "step: 532 loss: 0.058\n",
      "step: 533 loss: 0.064\n",
      "step: 534 loss: 0.073\n",
      "step: 535 loss: 0.064\n",
      "step: 536 loss: 0.053\n",
      "step: 537 loss: 0.068\n",
      "step: 538 loss: 0.046\n",
      "step: 539 loss: 0.067\n",
      "step: 540 loss: 0.053\n",
      "step: 541 loss: 0.061\n",
      "step: 542 loss: 0.046\n",
      "step: 543 loss: 0.090\n",
      "step: 544 loss: 0.072\n",
      "step: 545 loss: 0.051\n",
      "step: 546 loss: 0.070\n",
      "step: 547 loss: 0.062\n",
      "step: 548 loss: 0.070\n",
      "step: 549 loss: 0.043\n",
      "step: 550 loss: 0.045\n",
      "step: 551 loss: 0.056\n",
      "step: 552 loss: 0.059\n",
      "step: 553 loss: 0.057\n",
      "step: 554 loss: 0.060\n",
      "step: 555 loss: 0.072\n",
      "step: 556 loss: 0.042\n",
      "step: 557 loss: 0.066\n",
      "step: 558 loss: 0.057\n",
      "step: 559 loss: 0.068\n",
      "step: 560 loss: 0.078\n",
      "step: 561 loss: 0.047\n",
      "step: 562 loss: 0.048\n",
      "step: 563 loss: 0.064\n",
      "step: 564 loss: 0.062\n",
      "step: 565 loss: 0.073\n",
      "step: 566 loss: 0.052\n",
      "step: 567 loss: 0.054\n",
      "step: 568 loss: 0.059\n",
      "step: 569 loss: 0.051\n",
      "step: 570 loss: 0.071\n",
      "step: 571 loss: 0.056\n",
      "step: 572 loss: 0.043\n",
      "step: 573 loss: 0.062\n",
      "step: 574 loss: 0.052\n",
      "step: 575 loss: 0.058\n",
      "step: 576 loss: 0.039\n",
      "step: 577 loss: 0.055\n",
      "step: 578 loss: 0.057\n",
      "step: 579 loss: 0.065\n",
      "step: 580 loss: 0.046\n",
      "step: 581 loss: 0.053\n",
      "step: 582 loss: 0.036\n",
      "step: 583 loss: 0.049\n",
      "step: 584 loss: 0.066\n",
      "step: 585 loss: 0.062\n",
      "step: 586 loss: 0.084\n",
      "step: 587 loss: 0.049\n",
      "step: 588 loss: 0.046\n",
      "step: 589 loss: 0.052\n",
      "step: 590 loss: 0.052\n",
      "step: 591 loss: 0.056\n",
      "step: 592 loss: 0.045\n",
      "step: 593 loss: 0.037\n",
      "step: 594 loss: 0.040\n",
      "step: 595 loss: 0.066\n",
      "step: 596 loss: 0.053\n",
      "step: 597 loss: 0.052\n",
      "step: 598 loss: 0.066\n",
      "step: 599 loss: 0.059\n",
      "step: 600 loss: 0.072\n",
      "step: 601 loss: 0.051\n",
      "step: 602 loss: 0.075\n",
      "step: 603 loss: 0.064\n",
      "step: 604 loss: 0.061\n",
      "step: 605 loss: 0.064\n",
      "step: 606 loss: 0.051\n",
      "step: 607 loss: 0.072\n",
      "step: 608 loss: 0.053\n",
      "step: 609 loss: 0.033\n",
      "step: 610 loss: 0.061\n",
      "step: 611 loss: 0.055\n",
      "step: 612 loss: 0.048\n",
      "step: 613 loss: 0.058\n",
      "step: 614 loss: 0.069\n",
      "step: 615 loss: 0.059\n",
      "step: 616 loss: 0.064\n",
      "step: 617 loss: 0.049\n",
      "step: 618 loss: 0.041\n",
      "step: 619 loss: 0.055\n",
      "step: 620 loss: 0.050\n",
      "step: 621 loss: 0.057\n",
      "step: 622 loss: 0.029\n",
      "step: 623 loss: 0.066\n",
      "step: 624 loss: 0.048\n",
      "step: 625 loss: 0.046\n",
      "step: 626 loss: 0.054\n",
      "step: 627 loss: 0.061\n",
      "step: 628 loss: 0.045\n",
      "step: 629 loss: 0.054\n",
      "step: 630 loss: 0.049\n",
      "step: 631 loss: 0.066\n",
      "step: 632 loss: 0.067\n",
      "step: 633 loss: 0.052\n",
      "step: 634 loss: 0.058\n",
      "step: 635 loss: 0.047\n",
      "step: 636 loss: 0.027\n",
      "step: 637 loss: 0.039\n",
      "step: 638 loss: 0.040\n",
      "step: 639 loss: 0.065\n",
      "step: 640 loss: 0.051\n",
      "step: 641 loss: 0.044\n",
      "step: 642 loss: 0.056\n",
      "step: 643 loss: 0.056\n",
      "step: 644 loss: 0.054\n",
      "step: 645 loss: 0.043\n",
      "step: 646 loss: 0.058\n",
      "step: 647 loss: 0.049\n",
      "step: 648 loss: 0.053\n",
      "step: 649 loss: 0.062\n",
      "step: 650 loss: 0.035\n",
      "step: 651 loss: 0.044\n",
      "step: 652 loss: 0.060\n",
      "step: 653 loss: 0.093\n",
      "step: 654 loss: 0.052\n",
      "step: 655 loss: 0.060\n",
      "step: 656 loss: 0.044\n",
      "step: 657 loss: 0.047\n",
      "step: 658 loss: 0.044\n",
      "step: 659 loss: 0.053\n",
      "step: 660 loss: 0.059\n",
      "step: 661 loss: 0.050\n",
      "step: 662 loss: 0.048\n",
      "step: 663 loss: 0.080\n",
      "step: 664 loss: 0.060\n",
      "step: 665 loss: 0.066\n",
      "step: 666 loss: 0.060\n",
      "step: 667 loss: 0.050\n",
      "step: 668 loss: 0.080\n",
      "step: 669 loss: 0.072\n",
      "step: 670 loss: 0.050\n",
      "step: 671 loss: 0.052\n",
      "step: 672 loss: 0.054\n",
      "step: 673 loss: 0.056\n",
      "step: 674 loss: 0.061\n",
      "step: 675 loss: 0.074\n",
      "step: 676 loss: 0.047\n",
      "step: 677 loss: 0.077\n",
      "step: 678 loss: 0.063\n",
      "step: 679 loss: 0.065\n",
      "step: 680 loss: 0.070\n",
      "step: 681 loss: 0.070\n",
      "step: 682 loss: 0.061\n",
      "step: 683 loss: 0.053\n",
      "step: 684 loss: 0.055\n",
      "step: 685 loss: 0.045\n",
      "step: 686 loss: 0.050\n",
      "step: 687 loss: 0.058\n",
      "step: 688 loss: 0.070\n",
      "step: 689 loss: 0.058\n",
      "step: 690 loss: 0.046\n",
      "step: 691 loss: 0.056\n",
      "step: 692 loss: 0.059\n",
      "step: 693 loss: 0.046\n",
      "step: 694 loss: 0.052\n",
      "step: 695 loss: 0.083\n",
      "step: 696 loss: 0.053\n",
      "step: 697 loss: 0.032\n",
      "step: 698 loss: 0.023\n",
      "step: 699 loss: 0.049\n",
      "step: 700 loss: 0.062\n",
      "step: 701 loss: 0.047\n",
      "step: 702 loss: 0.054\n",
      "step: 703 loss: 0.066\n",
      "step: 704 loss: 0.065\n",
      "step: 705 loss: 0.055\n",
      "step: 706 loss: 0.054\n",
      "step: 707 loss: 0.056\n",
      "step: 708 loss: 0.046\n",
      "step: 709 loss: 0.049\n",
      "step: 710 loss: 0.063\n",
      "step: 711 loss: 0.050\n",
      "step: 712 loss: 0.053\n",
      "step: 713 loss: 0.045\n",
      "step: 714 loss: 0.050\n",
      "step: 715 loss: 0.063\n",
      "step: 716 loss: 0.058\n",
      "step: 717 loss: 0.065\n",
      "step: 718 loss: 0.044\n",
      "step: 719 loss: 0.052\n",
      "step: 720 loss: 0.074\n",
      "step: 721 loss: 0.063\n",
      "step: 722 loss: 0.051\n",
      "step: 723 loss: 0.055\n",
      "step: 724 loss: 0.064\n",
      "step: 725 loss: 0.049\n",
      "step: 726 loss: 0.041\n",
      "step: 727 loss: 0.054\n",
      "step: 728 loss: 0.041\n",
      "step: 729 loss: 0.055\n",
      "step: 730 loss: 0.061\n",
      "step: 731 loss: 0.047\n",
      "step: 732 loss: 0.066\n",
      "step: 733 loss: 0.041\n",
      "step: 734 loss: 0.064\n",
      "step: 735 loss: 0.068\n",
      "step: 736 loss: 0.041\n",
      "step: 737 loss: 0.051\n",
      "step: 738 loss: 0.040\n",
      "step: 739 loss: 0.060\n",
      "step: 740 loss: 0.036\n",
      "step: 741 loss: 0.046\n",
      "step: 742 loss: 0.046\n",
      "step: 743 loss: 0.047\n",
      "step: 744 loss: 0.060\n",
      "step: 745 loss: 0.064\n",
      "step: 746 loss: 0.071\n",
      "step: 747 loss: 0.048\n",
      "step: 748 loss: 0.052\n",
      "step: 749 loss: 0.079\n",
      "step: 750 loss: 0.039\n",
      "step: 751 loss: 0.058\n",
      "step: 752 loss: 0.053\n",
      "step: 753 loss: 0.050\n",
      "step: 754 loss: 0.059\n",
      "step: 755 loss: 0.045\n",
      "step: 756 loss: 0.060\n",
      "step: 757 loss: 0.069\n",
      "step: 758 loss: 0.050\n",
      "step: 759 loss: 0.066\n",
      "step: 760 loss: 0.081\n",
      "step: 761 loss: 0.056\n",
      "step: 762 loss: 0.056\n",
      "step: 763 loss: 0.078\n",
      "step: 764 loss: 0.042\n",
      "step: 765 loss: 0.051\n",
      "step: 766 loss: 0.047\n",
      "step: 767 loss: 0.071\n",
      "step: 768 loss: 0.058\n",
      "step: 769 loss: 0.065\n",
      "step: 770 loss: 0.063\n",
      "step: 771 loss: 0.057\n",
      "step: 772 loss: 0.047\n",
      "step: 773 loss: 0.058\n",
      "step: 774 loss: 0.059\n",
      "step: 775 loss: 0.070\n",
      "step: 776 loss: 0.044\n",
      "step: 777 loss: 0.056\n",
      "step: 778 loss: 0.049\n",
      "step: 779 loss: 0.058\n",
      "step: 780 loss: 0.062\n",
      "step: 781 loss: 0.057\n",
      "step: 782 loss: 0.039\n",
      "step: 783 loss: 0.058\n",
      "step: 784 loss: 0.052\n",
      "step: 785 loss: 0.041\n",
      "step: 786 loss: 0.038\n",
      "step: 787 loss: 0.051\n",
      "step: 788 loss: 0.043\n",
      "step: 789 loss: 0.051\n",
      "step: 790 loss: 0.056\n",
      "step: 791 loss: 0.070\n",
      "step: 792 loss: 0.047\n",
      "step: 793 loss: 0.069\n",
      "step: 794 loss: 0.035\n",
      "step: 795 loss: 0.051\n",
      "step: 796 loss: 0.049\n",
      "step: 797 loss: 0.035\n",
      "step: 798 loss: 0.061\n",
      "step: 799 loss: 0.074\n",
      "step: 800 loss: 0.055\n",
      "step: 801 loss: 0.061\n",
      "step: 802 loss: 0.052\n",
      "step: 803 loss: 0.063\n",
      "step: 804 loss: 0.030\n",
      "step: 805 loss: 0.053\n",
      "step: 806 loss: 0.049\n",
      "step: 807 loss: 0.064\n",
      "step: 808 loss: 0.074\n",
      "step: 809 loss: 0.028\n",
      "step: 810 loss: 0.048\n",
      "step: 811 loss: 0.055\n",
      "step: 812 loss: 0.057\n",
      "step: 813 loss: 0.054\n",
      "step: 814 loss: 0.036\n",
      "step: 815 loss: 0.072\n",
      "step: 816 loss: 0.045\n",
      "step: 817 loss: 0.036\n",
      "step: 818 loss: 0.055\n",
      "step: 819 loss: 0.073\n",
      "step: 820 loss: 0.049\n",
      "step: 821 loss: 0.050\n",
      "step: 822 loss: 0.074\n",
      "step: 823 loss: 0.066\n",
      "step: 824 loss: 0.041\n",
      "step: 825 loss: 0.067\n",
      "step: 826 loss: 0.041\n",
      "step: 827 loss: 0.072\n",
      "step: 828 loss: 0.036\n",
      "step: 829 loss: 0.062\n",
      "step: 830 loss: 0.069\n",
      "step: 831 loss: 0.049\n",
      "step: 832 loss: 0.050\n",
      "step: 833 loss: 0.032\n",
      "step: 834 loss: 0.060\n",
      "step: 835 loss: 0.066\n",
      "step: 836 loss: 0.043\n",
      "step: 837 loss: 0.040\n",
      "step: 838 loss: 0.057\n",
      "step: 839 loss: 0.037\n",
      "step: 840 loss: 0.038\n",
      "step: 841 loss: 0.050\n",
      "step: 842 loss: 0.051\n",
      "step: 843 loss: 0.055\n",
      "step: 844 loss: 0.056\n",
      "step: 845 loss: 0.044\n",
      "step: 846 loss: 0.043\n",
      "step: 847 loss: 0.053\n",
      "step: 848 loss: 0.054\n",
      "step: 849 loss: 0.062\n",
      "step: 850 loss: 0.072\n",
      "step: 851 loss: 0.035\n",
      "step: 852 loss: 0.060\n",
      "step: 853 loss: 0.067\n",
      "step: 854 loss: 0.041\n",
      "step: 855 loss: 0.057\n",
      "step: 856 loss: 0.054\n",
      "step: 857 loss: 0.059\n",
      "step: 858 loss: 0.042\n",
      "step: 859 loss: 0.054\n",
      "step: 860 loss: 0.056\n",
      "step: 861 loss: 0.044\n",
      "step: 862 loss: 0.065\n",
      "step: 863 loss: 0.075\n",
      "step: 864 loss: 0.051\n",
      "step: 865 loss: 0.044\n",
      "step: 866 loss: 0.042\n",
      "step: 867 loss: 0.043\n",
      "step: 868 loss: 0.062\n",
      "step: 869 loss: 0.045\n",
      "step: 870 loss: 0.035\n",
      "step: 871 loss: 0.054\n",
      "step: 872 loss: 0.064\n",
      "step: 873 loss: 0.062\n",
      "step: 874 loss: 0.058\n",
      "step: 875 loss: 0.049\n",
      "step: 876 loss: 0.075\n",
      "step: 877 loss: 0.057\n",
      "step: 878 loss: 0.070\n",
      "step: 879 loss: 0.061\n",
      "step: 880 loss: 0.077\n",
      "step: 881 loss: 0.049\n",
      "step: 882 loss: 0.036\n",
      "step: 883 loss: 0.065\n",
      "step: 884 loss: 0.045\n",
      "step: 885 loss: 0.065\n",
      "step: 886 loss: 0.056\n",
      "step: 887 loss: 0.072\n",
      "step: 888 loss: 0.044\n",
      "step: 889 loss: 0.053\n",
      "step: 890 loss: 0.067\n",
      "step: 891 loss: 0.051\n",
      "step: 892 loss: 0.053\n",
      "step: 893 loss: 0.056\n",
      "step: 894 loss: 0.069\n",
      "step: 895 loss: 0.046\n",
      "step: 896 loss: 0.038\n",
      "step: 897 loss: 0.052\n",
      "step: 898 loss: 0.069\n",
      "step: 899 loss: 0.055\n",
      "step: 900 loss: 0.056\n",
      "step: 901 loss: 0.062\n",
      "step: 902 loss: 0.043\n",
      "step: 903 loss: 0.050\n",
      "step: 904 loss: 0.055\n",
      "step: 905 loss: 0.042\n",
      "step: 906 loss: 0.068\n",
      "step: 907 loss: 0.054\n",
      "step: 908 loss: 0.055\n",
      "step: 909 loss: 0.052\n",
      "step: 910 loss: 0.041\n",
      "step: 911 loss: 0.056\n",
      "step: 912 loss: 0.035\n",
      "step: 913 loss: 0.051\n",
      "step: 914 loss: 0.052\n",
      "step: 915 loss: 0.065\n",
      "step: 916 loss: 0.046\n",
      "step: 917 loss: 0.056\n",
      "step: 918 loss: 0.031\n",
      "step: 919 loss: 0.054\n",
      "step: 920 loss: 0.058\n",
      "step: 921 loss: 0.048\n",
      "step: 922 loss: 0.048\n",
      "step: 923 loss: 0.062\n",
      "step: 924 loss: 0.047\n",
      "step: 925 loss: 0.046\n",
      "step: 926 loss: 0.056\n",
      "step: 927 loss: 0.065\n",
      "step: 928 loss: 0.053\n",
      "step: 929 loss: 0.046\n",
      "step: 930 loss: 0.054\n",
      "step: 931 loss: 0.037\n",
      "step: 932 loss: 0.056\n",
      "step: 933 loss: 0.068\n",
      "step: 934 loss: 0.064\n",
      "step: 935 loss: 0.050\n",
      "step: 936 loss: 0.052\n",
      "step: 937 loss: 0.063\n",
      "step: 938 loss: 0.031\n",
      "step: 939 loss: 0.048\n",
      "step: 940 loss: 0.059\n",
      "step: 941 loss: 0.034\n",
      "step: 942 loss: 0.052\n",
      "step: 943 loss: 0.047\n",
      "step: 944 loss: 0.047\n",
      "step: 945 loss: 0.055\n",
      "step: 946 loss: 0.067\n",
      "step: 947 loss: 0.054\n",
      "step: 948 loss: 0.049\n",
      "step: 949 loss: 0.051\n",
      "step: 950 loss: 0.049\n",
      "step: 951 loss: 0.068\n",
      "step: 952 loss: 0.053\n",
      "step: 953 loss: 0.063\n",
      "step: 954 loss: 0.050\n",
      "step: 955 loss: 0.067\n",
      "step: 956 loss: 0.052\n",
      "step: 957 loss: 0.058\n",
      "step: 958 loss: 0.044\n",
      "step: 959 loss: 0.054\n",
      "step: 960 loss: 0.063\n",
      "step: 961 loss: 0.048\n",
      "step: 962 loss: 0.041\n",
      "step: 963 loss: 0.054\n",
      "step: 964 loss: 0.038\n",
      "step: 965 loss: 0.057\n",
      "step: 966 loss: 0.052\n",
      "step: 967 loss: 0.046\n",
      "step: 968 loss: 0.066\n",
      "step: 969 loss: 0.061\n",
      "step: 970 loss: 0.059\n",
      "step: 971 loss: 0.052\n",
      "step: 972 loss: 0.051\n",
      "step: 973 loss: 0.062\n",
      "step: 974 loss: 0.061\n",
      "step: 975 loss: 0.048\n",
      "step: 976 loss: 0.063\n",
      "step: 977 loss: 0.063\n",
      "step: 978 loss: 0.061\n",
      "step: 979 loss: 0.040\n",
      "step: 980 loss: 0.088\n",
      "step: 981 loss: 0.055\n",
      "step: 982 loss: 0.048\n",
      "step: 983 loss: 0.051\n",
      "step: 984 loss: 0.062\n",
      "step: 985 loss: 0.052\n",
      "step: 986 loss: 0.048\n",
      "step: 987 loss: 0.047\n",
      "step: 988 loss: 0.053\n",
      "step: 989 loss: 0.065\n",
      "step: 990 loss: 0.063\n",
      "step: 991 loss: 0.048\n",
      "step: 992 loss: 0.058\n",
      "step: 993 loss: 0.051\n",
      "step: 994 loss: 0.057\n",
      "step: 995 loss: 0.052\n",
      "step: 996 loss: 0.055\n",
      "step: 997 loss: 0.058\n",
      "step: 998 loss: 0.041\n",
      "step: 999 loss: 0.075\n",
      "step: 1000 loss: 0.058\n",
      "step: 1001 loss: 0.040\n",
      "step: 1002 loss: 0.067\n",
      "step: 1003 loss: 0.066\n",
      "step: 1004 loss: 0.049\n",
      "step: 1005 loss: 0.042\n",
      "step: 1006 loss: 0.056\n",
      "step: 1007 loss: 0.069\n",
      "step: 1008 loss: 0.037\n",
      "step: 1009 loss: 0.060\n",
      "step: 1010 loss: 0.071\n",
      "step: 1011 loss: 0.048\n",
      "step: 1012 loss: 0.060\n",
      "step: 1013 loss: 0.058\n",
      "step: 1014 loss: 0.036\n",
      "step: 1015 loss: 0.043\n",
      "step: 1016 loss: 0.057\n",
      "step: 1017 loss: 0.089\n",
      "step: 1018 loss: 0.040\n",
      "step: 1019 loss: 0.045\n",
      "step: 1020 loss: 0.038\n",
      "step: 1021 loss: 0.041\n",
      "step: 1022 loss: 0.056\n",
      "step: 1023 loss: 0.062\n",
      "step: 1024 loss: 0.053\n",
      "step: 1025 loss: 0.054\n",
      "step: 1026 loss: 0.057\n",
      "step: 1027 loss: 0.044\n",
      "step: 1028 loss: 0.039\n",
      "step: 1029 loss: 0.038\n",
      "step: 1030 loss: 0.064\n",
      "step: 1031 loss: 0.064\n",
      "step: 1032 loss: 0.065\n",
      "step: 1033 loss: 0.047\n",
      "step: 1034 loss: 0.046\n",
      "step: 1035 loss: 0.061\n",
      "step: 1036 loss: 0.055\n",
      "step: 1037 loss: 0.051\n",
      "step: 1038 loss: 0.042\n",
      "step: 1039 loss: 0.048\n",
      "step: 1040 loss: 0.078\n",
      "step: 1041 loss: 0.055\n",
      "step: 1042 loss: 0.064\n",
      "step: 1043 loss: 0.061\n",
      "step: 1044 loss: 0.050\n",
      "step: 1045 loss: 0.049\n",
      "step: 1046 loss: 0.039\n",
      "step: 1047 loss: 0.048\n",
      "step: 1048 loss: 0.051\n",
      "step: 1049 loss: 0.045\n",
      "step: 1050 loss: 0.081\n",
      "step: 1051 loss: 0.048\n",
      "step: 1052 loss: 0.046\n",
      "step: 1053 loss: 0.039\n",
      "step: 1054 loss: 0.056\n",
      "step: 1055 loss: 0.051\n",
      "step: 1056 loss: 0.030\n",
      "step: 1057 loss: 0.061\n",
      "step: 1058 loss: 0.042\n",
      "step: 1059 loss: 0.051\n",
      "step: 1060 loss: 0.069\n",
      "step: 1061 loss: 0.041\n",
      "step: 1062 loss: 0.068\n",
      "step: 1063 loss: 0.047\n",
      "step: 1064 loss: 0.046\n",
      "step: 1065 loss: 0.057\n",
      "step: 1066 loss: 0.057\n",
      "step: 1067 loss: 0.075\n",
      "step: 1068 loss: 0.061\n",
      "step: 1069 loss: 0.047\n",
      "step: 1070 loss: 0.057\n",
      "step: 1071 loss: 0.043\n",
      "step: 1072 loss: 0.052\n",
      "step: 1073 loss: 0.043\n",
      "step: 1074 loss: 0.053\n",
      "step: 1075 loss: 0.071\n",
      "step: 1076 loss: 0.034\n",
      "step: 1077 loss: 0.055\n",
      "step: 1078 loss: 0.060\n",
      "step: 1079 loss: 0.041\n",
      "step: 1080 loss: 0.046\n",
      "step: 1081 loss: 0.057\n",
      "step: 1082 loss: 0.046\n",
      "step: 1083 loss: 0.046\n",
      "step: 1084 loss: 0.056\n",
      "step: 1085 loss: 0.050\n",
      "step: 1086 loss: 0.094\n",
      "step: 1087 loss: 0.061\n",
      "step: 1088 loss: 0.062\n",
      "step: 1089 loss: 0.052\n",
      "step: 1090 loss: 0.046\n",
      "step: 1091 loss: 0.064\n",
      "step: 1092 loss: 0.090\n",
      "step: 1093 loss: 0.053\n",
      "step: 1094 loss: 0.065\n",
      "step: 1095 loss: 0.051\n",
      "step: 1096 loss: 0.033\n",
      "step: 1097 loss: 0.062\n",
      "step: 1098 loss: 0.057\n",
      "step: 1099 loss: 0.046\n",
      "step: 1100 loss: 0.043\n",
      "step: 1101 loss: 0.043\n",
      "step: 1102 loss: 0.052\n",
      "step: 1103 loss: 0.050\n",
      "step: 1104 loss: 0.057\n",
      "step: 1105 loss: 0.041\n",
      "step: 1106 loss: 0.042\n",
      "step: 1107 loss: 0.061\n",
      "step: 1108 loss: 0.039\n",
      "step: 1109 loss: 0.054\n",
      "step: 1110 loss: 0.046\n",
      "step: 1111 loss: 0.037\n",
      "step: 1112 loss: 0.059\n",
      "step: 1113 loss: 0.051\n",
      "step: 1114 loss: 0.042\n",
      "step: 1115 loss: 0.067\n",
      "step: 1116 loss: 0.035\n",
      "step: 1117 loss: 0.026\n",
      "step: 1118 loss: 0.033\n",
      "step: 1119 loss: 0.043\n",
      "step: 1120 loss: 0.083\n",
      "step: 1121 loss: 0.067\n",
      "step: 1122 loss: 0.054\n",
      "step: 1123 loss: 0.054\n",
      "step: 1124 loss: 0.039\n",
      "step: 1125 loss: 0.041\n",
      "step: 1126 loss: 0.038\n",
      "step: 1127 loss: 0.051\n",
      "step: 1128 loss: 0.069\n",
      "step: 1129 loss: 0.062\n",
      "step: 1130 loss: 0.055\n",
      "step: 1131 loss: 0.078\n",
      "step: 1132 loss: 0.066\n",
      "step: 1133 loss: 0.064\n",
      "step: 1134 loss: 0.043\n",
      "step: 1135 loss: 0.052\n",
      "step: 1136 loss: 0.047\n",
      "step: 1137 loss: 0.055\n",
      "step: 1138 loss: 0.048\n",
      "step: 1139 loss: 0.054\n",
      "step: 1140 loss: 0.046\n",
      "step: 1141 loss: 0.058\n",
      "step: 1142 loss: 0.052\n",
      "step: 1143 loss: 0.061\n",
      "step: 1144 loss: 0.051\n",
      "step: 1145 loss: 0.055\n",
      "step: 1146 loss: 0.041\n",
      "step: 1147 loss: 0.041\n",
      "step: 1148 loss: 0.057\n",
      "step: 1149 loss: 0.047\n",
      "step: 1150 loss: 0.054\n",
      "step: 1151 loss: 0.048\n",
      "step: 1152 loss: 0.065\n",
      "step: 1153 loss: 0.049\n",
      "step: 1154 loss: 0.061\n",
      "step: 1155 loss: 0.057\n",
      "step: 1156 loss: 0.051\n",
      "step: 1157 loss: 0.074\n",
      "step: 1158 loss: 0.065\n",
      "step: 1159 loss: 0.051\n",
      "step: 1160 loss: 0.053\n",
      "step: 1161 loss: 0.050\n",
      "step: 1162 loss: 0.061\n",
      "step: 1163 loss: 0.048\n",
      "step: 1164 loss: 0.047\n",
      "step: 1165 loss: 0.046\n",
      "step: 1166 loss: 0.052\n",
      "step: 1167 loss: 0.039\n",
      "step: 1168 loss: 0.039\n",
      "step: 1169 loss: 0.042\n",
      "step: 1170 loss: 0.058\n",
      "step: 1171 loss: 0.056\n",
      "step: 1172 loss: 0.060\n",
      "step: 1173 loss: 0.045\n",
      "step: 1174 loss: 0.058\n",
      "step: 1175 loss: 0.062\n",
      "step: 1176 loss: 0.035\n",
      "step: 1177 loss: 0.080\n",
      "step: 1178 loss: 0.046\n",
      "step: 1179 loss: 0.068\n",
      "step: 1180 loss: 0.052\n",
      "step: 1181 loss: 0.058\n",
      "step: 1182 loss: 0.039\n",
      "step: 1183 loss: 0.034\n",
      "step: 1184 loss: 0.060\n",
      "step: 1185 loss: 0.041\n",
      "step: 1186 loss: 0.067\n",
      "step: 1187 loss: 0.060\n",
      "step: 1188 loss: 0.062\n",
      "step: 1189 loss: 0.048\n",
      "step: 1190 loss: 0.050\n",
      "step: 1191 loss: 0.054\n",
      "step: 1192 loss: 0.062\n",
      "step: 1193 loss: 0.054\n",
      "step: 1194 loss: 0.032\n",
      "step: 1195 loss: 0.063\n",
      "step: 1196 loss: 0.058\n",
      "step: 1197 loss: 0.057\n",
      "step: 1198 loss: 0.053\n",
      "step: 1199 loss: 0.034\n",
      "step: 1200 loss: 0.058\n",
      "step: 1201 loss: 0.041\n",
      "step: 1202 loss: 0.049\n",
      "step: 1203 loss: 0.055\n",
      "step: 1204 loss: 0.040\n",
      "step: 1205 loss: 0.056\n",
      "step: 1206 loss: 0.030\n",
      "step: 1207 loss: 0.071\n",
      "step: 1208 loss: 0.055\n",
      "step: 1209 loss: 0.052\n",
      "step: 1210 loss: 0.059\n",
      "step: 1211 loss: 0.066\n",
      "step: 1212 loss: 0.051\n",
      "step: 1213 loss: 0.059\n",
      "step: 1214 loss: 0.069\n",
      "step: 1215 loss: 0.052\n",
      "step: 1216 loss: 0.042\n",
      "step: 1217 loss: 0.042\n",
      "step: 1218 loss: 0.045\n",
      "step: 1219 loss: 0.067\n",
      "step: 1220 loss: 0.061\n",
      "step: 1221 loss: 0.033\n",
      "step: 1222 loss: 0.053\n",
      "step: 1223 loss: 0.043\n",
      "step: 1224 loss: 0.057\n",
      "step: 1225 loss: 0.039\n",
      "step: 1226 loss: 0.048\n",
      "step: 1227 loss: 0.042\n",
      "step: 1228 loss: 0.051\n",
      "step: 1229 loss: 0.043\n",
      "step: 1230 loss: 0.042\n",
      "step: 1231 loss: 0.029\n",
      "step: 1232 loss: 0.057\n",
      "step: 1233 loss: 0.056\n",
      "step: 1234 loss: 0.051\n",
      "step: 1235 loss: 0.042\n",
      "step: 1236 loss: 0.042\n",
      "step: 1237 loss: 0.039\n",
      "step: 1238 loss: 0.066\n",
      "step: 1239 loss: 0.059\n",
      "step: 1240 loss: 0.036\n",
      "step: 1241 loss: 0.058\n",
      "step: 1242 loss: 0.050\n",
      "step: 1243 loss: 0.046\n",
      "step: 1244 loss: 0.051\n",
      "step: 1245 loss: 0.040\n",
      "step: 1246 loss: 0.038\n",
      "step: 1247 loss: 0.066\n",
      "step: 1248 loss: 0.043\n",
      "step: 1249 loss: 0.042\n",
      "step: 1250 loss: 0.076\n",
      "step: 1251 loss: 0.048\n",
      "step: 1252 loss: 0.056\n",
      "step: 1253 loss: 0.053\n",
      "step: 1254 loss: 0.046\n",
      "step: 1255 loss: 0.045\n",
      "step: 1256 loss: 0.067\n",
      "step: 1257 loss: 0.069\n",
      "step: 1258 loss: 0.055\n",
      "step: 1259 loss: 0.066\n",
      "step: 1260 loss: 0.044\n",
      "step: 1261 loss: 0.061\n",
      "step: 1262 loss: 0.042\n",
      "step: 1263 loss: 0.050\n",
      "step: 1264 loss: 0.056\n",
      "step: 1265 loss: 0.053\n",
      "step: 1266 loss: 0.051\n",
      "step: 1267 loss: 0.039\n",
      "step: 1268 loss: 0.043\n",
      "step: 1269 loss: 0.052\n",
      "step: 1270 loss: 0.051\n",
      "step: 1271 loss: 0.078\n",
      "step: 1272 loss: 0.036\n",
      "step: 1273 loss: 0.048\n",
      "step: 1274 loss: 0.062\n",
      "step: 1275 loss: 0.059\n",
      "step: 1276 loss: 0.057\n",
      "step: 1277 loss: 0.054\n",
      "step: 1278 loss: 0.039\n",
      "step: 1279 loss: 0.065\n",
      "step: 1280 loss: 0.073\n",
      "step: 1281 loss: 0.064\n",
      "step: 1282 loss: 0.043\n",
      "step: 1283 loss: 0.049\n",
      "step: 1284 loss: 0.056\n",
      "step: 1285 loss: 0.057\n",
      "step: 1286 loss: 0.041\n",
      "step: 1287 loss: 0.042\n",
      "step: 1288 loss: 0.064\n",
      "step: 1289 loss: 0.040\n",
      "step: 1290 loss: 0.060\n",
      "step: 1291 loss: 0.046\n",
      "step: 1292 loss: 0.061\n",
      "step: 1293 loss: 0.053\n",
      "step: 1294 loss: 0.034\n",
      "step: 1295 loss: 0.053\n",
      "step: 1296 loss: 0.040\n",
      "step: 1297 loss: 0.040\n",
      "step: 1298 loss: 0.036\n",
      "step: 1299 loss: 0.043\n",
      "step: 1300 loss: 0.043\n",
      "step: 1301 loss: 0.039\n",
      "step: 1302 loss: 0.045\n",
      "step: 1303 loss: 0.038\n",
      "step: 1304 loss: 0.042\n",
      "step: 1305 loss: 0.065\n",
      "step: 1306 loss: 0.047\n",
      "step: 1307 loss: 0.059\n",
      "step: 1308 loss: 0.079\n",
      "step: 1309 loss: 0.055\n",
      "step: 1310 loss: 0.053\n",
      "step: 1311 loss: 0.056\n",
      "step: 1312 loss: 0.044\n",
      "step: 1313 loss: 0.060\n",
      "step: 1314 loss: 0.045\n",
      "step: 1315 loss: 0.058\n",
      "step: 1316 loss: 0.046\n",
      "step: 1317 loss: 0.073\n",
      "step: 1318 loss: 0.064\n",
      "step: 1319 loss: 0.044\n",
      "step: 1320 loss: 0.035\n",
      "step: 1321 loss: 0.062\n",
      "step: 1322 loss: 0.054\n",
      "step: 1323 loss: 0.060\n",
      "step: 1324 loss: 0.036\n",
      "step: 1325 loss: 0.049\n",
      "step: 1326 loss: 0.056\n",
      "step: 1327 loss: 0.048\n",
      "step: 1328 loss: 0.052\n",
      "step: 1329 loss: 0.044\n",
      "step: 1330 loss: 0.059\n",
      "step: 1331 loss: 0.045\n",
      "step: 1332 loss: 0.047\n",
      "step: 1333 loss: 0.041\n",
      "step: 1334 loss: 0.063\n",
      "step: 1335 loss: 0.062\n",
      "step: 1336 loss: 0.032\n",
      "step: 1337 loss: 0.058\n",
      "step: 1338 loss: 0.065\n",
      "step: 1339 loss: 0.041\n",
      "step: 1340 loss: 0.040\n",
      "step: 1341 loss: 0.060\n",
      "step: 1342 loss: 0.042\n",
      "step: 1343 loss: 0.057\n",
      "step: 1344 loss: 0.053\n",
      "step: 1345 loss: 0.069\n",
      "step: 1346 loss: 0.052\n",
      "step: 1347 loss: 0.042\n",
      "step: 1348 loss: 0.067\n",
      "step: 1349 loss: 0.057\n",
      "step: 1350 loss: 0.037\n",
      "step: 1351 loss: 0.054\n",
      "step: 1352 loss: 0.041\n",
      "step: 1353 loss: 0.048\n",
      "step: 1354 loss: 0.063\n",
      "step: 1355 loss: 0.029\n",
      "step: 1356 loss: 0.045\n",
      "step: 1357 loss: 0.045\n",
      "step: 1358 loss: 0.053\n",
      "step: 1359 loss: 0.065\n",
      "step: 1360 loss: 0.054\n",
      "step: 1361 loss: 0.044\n",
      "step: 1362 loss: 0.044\n",
      "step: 1363 loss: 0.045\n",
      "step: 1364 loss: 0.042\n",
      "step: 1365 loss: 0.058\n",
      "step: 1366 loss: 0.047\n",
      "step: 1367 loss: 0.068\n",
      "step: 1368 loss: 0.058\n",
      "step: 1369 loss: 0.049\n",
      "step: 1370 loss: 0.044\n",
      "step: 1371 loss: 0.047\n",
      "step: 1372 loss: 0.035\n",
      "step: 1373 loss: 0.039\n",
      "step: 1374 loss: 0.036\n",
      "step: 1375 loss: 0.054\n",
      "step: 1376 loss: 0.054\n",
      "step: 1377 loss: 0.040\n",
      "step: 1378 loss: 0.043\n",
      "step: 1379 loss: 0.071\n",
      "step: 1380 loss: 0.050\n",
      "step: 1381 loss: 0.032\n",
      "step: 1382 loss: 0.037\n",
      "step: 1383 loss: 0.046\n",
      "step: 1384 loss: 0.051\n",
      "step: 1385 loss: 0.068\n",
      "step: 1386 loss: 0.048\n",
      "step: 1387 loss: 0.054\n",
      "step: 1388 loss: 0.060\n",
      "step: 1389 loss: 0.054\n",
      "step: 1390 loss: 0.036\n",
      "step: 1391 loss: 0.070\n",
      "step: 1392 loss: 0.052\n",
      "step: 1393 loss: 0.062\n",
      "step: 1394 loss: 0.047\n",
      "step: 1395 loss: 0.066\n",
      "step: 1396 loss: 0.049\n",
      "step: 1397 loss: 0.054\n",
      "step: 1398 loss: 0.061\n",
      "step: 1399 loss: 0.054\n",
      "step: 1400 loss: 0.040\n",
      "step: 1401 loss: 0.030\n",
      "step: 1402 loss: 0.045\n",
      "step: 1403 loss: 0.039\n",
      "step: 1404 loss: 0.041\n",
      "step: 1405 loss: 0.042\n",
      "step: 1406 loss: 0.044\n",
      "step: 1407 loss: 0.068\n",
      "step: 1408 loss: 0.047\n",
      "step: 1409 loss: 0.052\n",
      "step: 1410 loss: 0.048\n",
      "step: 1411 loss: 0.035\n",
      "step: 1412 loss: 0.063\n",
      "step: 1413 loss: 0.055\n",
      "step: 1414 loss: 0.041\n",
      "step: 1415 loss: 0.039\n",
      "step: 1416 loss: 0.061\n",
      "step: 1417 loss: 0.041\n",
      "step: 1418 loss: 0.067\n",
      "step: 1419 loss: 0.049\n",
      "step: 1420 loss: 0.046\n",
      "step: 1421 loss: 0.053\n",
      "step: 1422 loss: 0.034\n",
      "step: 1423 loss: 0.046\n",
      "step: 1424 loss: 0.039\n",
      "step: 1425 loss: 0.044\n",
      "step: 1426 loss: 0.029\n",
      "step: 1427 loss: 0.057\n",
      "step: 1428 loss: 0.048\n",
      "step: 1429 loss: 0.042\n",
      "step: 1430 loss: 0.049\n",
      "step: 1431 loss: 0.071\n",
      "step: 1432 loss: 0.031\n",
      "step: 1433 loss: 0.045\n",
      "step: 1434 loss: 0.056\n",
      "step: 1435 loss: 0.049\n",
      "step: 1436 loss: 0.040\n",
      "step: 1437 loss: 0.045\n",
      "step: 1438 loss: 0.051\n",
      "step: 1439 loss: 0.049\n",
      "step: 1440 loss: 0.054\n",
      "step: 1441 loss: 0.058\n",
      "step: 1442 loss: 0.032\n",
      "step: 1443 loss: 0.029\n",
      "step: 1444 loss: 0.043\n",
      "step: 1445 loss: 0.048\n",
      "step: 1446 loss: 0.033\n",
      "step: 1447 loss: 0.034\n",
      "step: 1448 loss: 0.039\n",
      "step: 1449 loss: 0.056\n",
      "step: 1450 loss: 0.069\n",
      "step: 1451 loss: 0.052\n",
      "step: 1452 loss: 0.048\n",
      "step: 1453 loss: 0.041\n",
      "step: 1454 loss: 0.042\n",
      "step: 1455 loss: 0.050\n",
      "step: 1456 loss: 0.071\n",
      "step: 1457 loss: 0.065\n",
      "step: 1458 loss: 0.042\n",
      "step: 1459 loss: 0.053\n",
      "step: 1460 loss: 0.039\n",
      "step: 1461 loss: 0.050\n",
      "step: 1462 loss: 0.043\n",
      "step: 1463 loss: 0.043\n",
      "step: 1464 loss: 0.053\n",
      "step: 1465 loss: 0.060\n",
      "step: 1466 loss: 0.046\n",
      "step: 1467 loss: 0.048\n",
      "step: 1468 loss: 0.049\n",
      "step: 1469 loss: 0.049\n",
      "step: 1470 loss: 0.042\n",
      "step: 1471 loss: 0.050\n",
      "step: 1472 loss: 0.058\n",
      "step: 1473 loss: 0.050\n",
      "step: 1474 loss: 0.053\n",
      "step: 1475 loss: 0.049\n",
      "step: 1476 loss: 0.043\n",
      "step: 1477 loss: 0.040\n",
      "step: 1478 loss: 0.047\n",
      "step: 1479 loss: 0.032\n",
      "step: 1480 loss: 0.064\n",
      "step: 1481 loss: 0.067\n",
      "step: 1482 loss: 0.051\n",
      "step: 1483 loss: 0.044\n",
      "step: 1484 loss: 0.049\n",
      "step: 1485 loss: 0.048\n",
      "step: 1486 loss: 0.055\n",
      "step: 1487 loss: 0.047\n",
      "step: 1488 loss: 0.050\n",
      "step: 1489 loss: 0.026\n",
      "step: 1490 loss: 0.053\n",
      "step: 1491 loss: 0.047\n",
      "step: 1492 loss: 0.050\n",
      "step: 1493 loss: 0.049\n",
      "step: 1494 loss: 0.055\n",
      "step: 1495 loss: 0.053\n",
      "step: 1496 loss: 0.047\n",
      "step: 1497 loss: 0.043\n",
      "step: 1498 loss: 0.034\n",
      "step: 1499 loss: 0.040\n",
      "step: 1500 loss: 0.044\n",
      "step: 1501 loss: 0.052\n",
      "step: 1502 loss: 0.038\n",
      "step: 1503 loss: 0.064\n",
      "step: 1504 loss: 0.049\n",
      "step: 1505 loss: 0.051\n",
      "step: 1506 loss: 0.064\n",
      "step: 1507 loss: 0.052\n",
      "step: 1508 loss: 0.066\n",
      "step: 1509 loss: 0.041\n",
      "step: 1510 loss: 0.045\n",
      "step: 1511 loss: 0.043\n",
      "step: 1512 loss: 0.043\n",
      "step: 1513 loss: 0.052\n",
      "step: 1514 loss: 0.042\n",
      "step: 1515 loss: 0.052\n",
      "step: 1516 loss: 0.063\n",
      "step: 1517 loss: 0.051\n",
      "step: 1518 loss: 0.041\n",
      "step: 1519 loss: 0.048\n",
      "step: 1520 loss: 0.041\n",
      "step: 1521 loss: 0.054\n",
      "step: 1522 loss: 0.055\n",
      "step: 1523 loss: 0.047\n",
      "step: 1524 loss: 0.043\n",
      "step: 1525 loss: 0.043\n",
      "step: 1526 loss: 0.049\n",
      "step: 1527 loss: 0.066\n",
      "step: 1528 loss: 0.038\n",
      "step: 1529 loss: 0.072\n",
      "step: 1530 loss: 0.054\n",
      "step: 1531 loss: 0.049\n",
      "step: 1532 loss: 0.051\n",
      "step: 1533 loss: 0.039\n",
      "step: 1534 loss: 0.053\n",
      "step: 1535 loss: 0.051\n",
      "step: 1536 loss: 0.036\n",
      "step: 1537 loss: 0.053\n",
      "step: 1538 loss: 0.062\n",
      "step: 1539 loss: 0.070\n",
      "step: 1540 loss: 0.037\n",
      "step: 1541 loss: 0.069\n",
      "step: 1542 loss: 0.045\n",
      "step: 1543 loss: 0.045\n",
      "step: 1544 loss: 0.039\n",
      "step: 1545 loss: 0.048\n",
      "step: 1546 loss: 0.050\n",
      "step: 1547 loss: 0.055\n",
      "step: 1548 loss: 0.053\n",
      "step: 1549 loss: 0.041\n",
      "step: 1550 loss: 0.047\n",
      "step: 1551 loss: 0.039\n",
      "step: 1552 loss: 0.046\n",
      "step: 1553 loss: 0.042\n",
      "step: 1554 loss: 0.046\n",
      "step: 1555 loss: 0.049\n",
      "step: 1556 loss: 0.048\n",
      "step: 1557 loss: 0.071\n",
      "step: 1558 loss: 0.045\n",
      "step: 1559 loss: 0.058\n",
      "step: 1560 loss: 0.047\n",
      "step: 1561 loss: 0.043\n",
      "step: 1562 loss: 0.039\n",
      "step: 1563 loss: 0.041\n",
      "step: 1564 loss: 0.076\n",
      "step: 1565 loss: 0.040\n",
      "step: 1566 loss: 0.045\n",
      "step: 1567 loss: 0.049\n",
      "step: 1568 loss: 0.050\n",
      "step: 1569 loss: 0.045\n",
      "step: 1570 loss: 0.050\n",
      "step: 1571 loss: 0.039\n",
      "step: 1572 loss: 0.030\n",
      "step: 1573 loss: 0.035\n",
      "step: 1574 loss: 0.048\n",
      "step: 1575 loss: 0.042\n",
      "step: 1576 loss: 0.036\n",
      "step: 1577 loss: 0.047\n",
      "step: 1578 loss: 0.066\n",
      "step: 1579 loss: 0.052\n",
      "step: 1580 loss: 0.037\n",
      "step: 1581 loss: 0.045\n",
      "step: 1582 loss: 0.041\n",
      "step: 1583 loss: 0.028\n",
      "step: 1584 loss: 0.062\n",
      "step: 1585 loss: 0.049\n",
      "step: 1586 loss: 0.059\n",
      "step: 1587 loss: 0.039\n",
      "step: 1588 loss: 0.037\n",
      "step: 1589 loss: 0.039\n",
      "step: 1590 loss: 0.055\n",
      "step: 1591 loss: 0.046\n",
      "step: 1592 loss: 0.043\n",
      "step: 1593 loss: 0.037\n",
      "step: 1594 loss: 0.038\n",
      "step: 1595 loss: 0.045\n",
      "step: 1596 loss: 0.044\n",
      "step: 1597 loss: 0.039\n",
      "step: 1598 loss: 0.059\n",
      "step: 1599 loss: 0.041\n",
      "step: 1600 loss: 0.055\n",
      "step: 1601 loss: 0.045\n",
      "step: 1602 loss: 0.057\n",
      "step: 1603 loss: 0.042\n",
      "step: 1604 loss: 0.047\n",
      "step: 1605 loss: 0.058\n",
      "step: 1606 loss: 0.051\n",
      "step: 1607 loss: 0.054\n",
      "step: 1608 loss: 0.041\n",
      "step: 1609 loss: 0.050\n",
      "step: 1610 loss: 0.034\n",
      "step: 1611 loss: 0.055\n",
      "step: 1612 loss: 0.051\n",
      "step: 1613 loss: 0.045\n",
      "step: 1614 loss: 0.039\n",
      "step: 1615 loss: 0.051\n",
      "step: 1616 loss: 0.038\n",
      "step: 1617 loss: 0.052\n",
      "step: 1618 loss: 0.045\n",
      "step: 1619 loss: 0.061\n",
      "step: 1620 loss: 0.034\n",
      "step: 1621 loss: 0.042\n",
      "step: 1622 loss: 0.037\n",
      "step: 1623 loss: 0.048\n",
      "step: 1624 loss: 0.051\n",
      "step: 1625 loss: 0.055\n",
      "step: 1626 loss: 0.050\n",
      "step: 1627 loss: 0.055\n",
      "step: 1628 loss: 0.042\n",
      "step: 1629 loss: 0.043\n",
      "step: 1630 loss: 0.037\n",
      "step: 1631 loss: 0.049\n",
      "step: 1632 loss: 0.042\n",
      "step: 1633 loss: 0.048\n",
      "step: 1634 loss: 0.029\n",
      "step: 1635 loss: 0.033\n",
      "step: 1636 loss: 0.025\n",
      "step: 1637 loss: 0.043\n",
      "step: 1638 loss: 0.060\n",
      "step: 1639 loss: 0.052\n",
      "step: 1640 loss: 0.045\n",
      "step: 1641 loss: 0.056\n",
      "step: 1642 loss: 0.070\n",
      "step: 1643 loss: 0.052\n",
      "step: 1644 loss: 0.042\n",
      "step: 1645 loss: 0.040\n",
      "step: 1646 loss: 0.054\n",
      "step: 1647 loss: 0.047\n",
      "step: 1648 loss: 0.036\n",
      "step: 1649 loss: 0.060\n",
      "step: 1650 loss: 0.047\n",
      "step: 1651 loss: 0.048\n",
      "step: 1652 loss: 0.045\n",
      "step: 1653 loss: 0.046\n",
      "step: 1654 loss: 0.044\n",
      "step: 1655 loss: 0.041\n",
      "step: 1656 loss: 0.046\n",
      "step: 1657 loss: 0.051\n",
      "step: 1658 loss: 0.043\n",
      "step: 1659 loss: 0.039\n",
      "step: 1660 loss: 0.037\n",
      "step: 1661 loss: 0.043\n",
      "step: 1662 loss: 0.044\n",
      "step: 1663 loss: 0.044\n",
      "step: 1664 loss: 0.047\n",
      "step: 1665 loss: 0.056\n",
      "step: 1666 loss: 0.063\n",
      "step: 1667 loss: 0.041\n",
      "step: 1668 loss: 0.040\n",
      "step: 1669 loss: 0.045\n",
      "step: 1670 loss: 0.036\n",
      "step: 1671 loss: 0.036\n",
      "step: 1672 loss: 0.055\n",
      "step: 1673 loss: 0.044\n",
      "step: 1674 loss: 0.060\n",
      "step: 1675 loss: 0.046\n",
      "step: 1676 loss: 0.044\n",
      "step: 1677 loss: 0.046\n",
      "step: 1678 loss: 0.043\n",
      "step: 1679 loss: 0.038\n",
      "step: 1680 loss: 0.057\n",
      "step: 1681 loss: 0.062\n",
      "step: 1682 loss: 0.048\n",
      "step: 1683 loss: 0.045\n",
      "step: 1684 loss: 0.046\n",
      "step: 1685 loss: 0.038\n",
      "step: 1686 loss: 0.048\n",
      "step: 1687 loss: 0.056\n",
      "step: 1688 loss: 0.051\n",
      "step: 1689 loss: 0.046\n",
      "step: 1690 loss: 0.056\n",
      "step: 1691 loss: 0.050\n",
      "step: 1692 loss: 0.053\n",
      "step: 1693 loss: 0.045\n",
      "step: 1694 loss: 0.048\n",
      "step: 1695 loss: 0.038\n",
      "step: 1696 loss: 0.049\n",
      "step: 1697 loss: 0.057\n",
      "step: 1698 loss: 0.040\n",
      "step: 1699 loss: 0.053\n",
      "step: 1700 loss: 0.052\n",
      "step: 1701 loss: 0.038\n",
      "step: 1702 loss: 0.048\n",
      "step: 1703 loss: 0.061\n",
      "step: 1704 loss: 0.029\n",
      "step: 1705 loss: 0.058\n",
      "step: 1706 loss: 0.043\n",
      "step: 1707 loss: 0.049\n",
      "step: 1708 loss: 0.051\n",
      "step: 1709 loss: 0.060\n",
      "step: 1710 loss: 0.044\n",
      "step: 1711 loss: 0.045\n",
      "step: 1712 loss: 0.051\n",
      "step: 1713 loss: 0.040\n",
      "step: 1714 loss: 0.051\n",
      "step: 1715 loss: 0.047\n",
      "step: 1716 loss: 0.036\n",
      "step: 1717 loss: 0.043\n",
      "step: 1718 loss: 0.046\n",
      "step: 1719 loss: 0.049\n",
      "step: 1720 loss: 0.059\n",
      "step: 1721 loss: 0.051\n",
      "step: 1722 loss: 0.049\n",
      "step: 1723 loss: 0.035\n",
      "step: 1724 loss: 0.056\n",
      "step: 1725 loss: 0.037\n",
      "step: 1726 loss: 0.036\n",
      "step: 1727 loss: 0.048\n",
      "step: 1728 loss: 0.041\n",
      "step: 1729 loss: 0.040\n",
      "step: 1730 loss: 0.052\n",
      "step: 1731 loss: 0.058\n",
      "step: 1732 loss: 0.044\n",
      "step: 1733 loss: 0.055\n",
      "step: 1734 loss: 0.035\n",
      "step: 1735 loss: 0.047\n",
      "step: 1736 loss: 0.058\n",
      "step: 1737 loss: 0.048\n",
      "step: 1738 loss: 0.058\n",
      "step: 1739 loss: 0.036\n",
      "step: 1740 loss: 0.044\n",
      "step: 1741 loss: 0.037\n",
      "step: 1742 loss: 0.041\n",
      "step: 1743 loss: 0.049\n",
      "step: 1744 loss: 0.076\n",
      "step: 1745 loss: 0.032\n",
      "step: 1746 loss: 0.054\n",
      "step: 1747 loss: 0.048\n",
      "step: 1748 loss: 0.028\n",
      "step: 1749 loss: 0.038\n",
      "step: 1750 loss: 0.081\n",
      "step: 1751 loss: 0.045\n",
      "step: 1752 loss: 0.047\n",
      "step: 1753 loss: 0.041\n",
      "step: 1754 loss: 0.043\n",
      "step: 1755 loss: 0.043\n",
      "step: 1756 loss: 0.054\n",
      "step: 1757 loss: 0.043\n",
      "step: 1758 loss: 0.047\n",
      "step: 1759 loss: 0.039\n",
      "step: 1760 loss: 0.048\n",
      "step: 1761 loss: 0.048\n",
      "step: 1762 loss: 0.047\n",
      "step: 1763 loss: 0.049\n",
      "step: 1764 loss: 0.044\n",
      "step: 1765 loss: 0.057\n",
      "step: 1766 loss: 0.060\n",
      "step: 1767 loss: 0.057\n",
      "step: 1768 loss: 0.046\n",
      "step: 1769 loss: 0.054\n",
      "step: 1770 loss: 0.043\n",
      "step: 1771 loss: 0.041\n",
      "step: 1772 loss: 0.059\n",
      "step: 1773 loss: 0.050\n",
      "step: 1774 loss: 0.041\n",
      "step: 1775 loss: 0.053\n",
      "step: 1776 loss: 0.045\n",
      "step: 1777 loss: 0.038\n",
      "step: 1778 loss: 0.062\n",
      "step: 1779 loss: 0.032\n",
      "step: 1780 loss: 0.054\n",
      "step: 1781 loss: 0.042\n",
      "step: 1782 loss: 0.053\n",
      "step: 1783 loss: 0.032\n",
      "step: 1784 loss: 0.053\n",
      "step: 1785 loss: 0.038\n",
      "step: 1786 loss: 0.054\n",
      "step: 1787 loss: 0.038\n",
      "step: 1788 loss: 0.055\n",
      "step: 1789 loss: 0.044\n",
      "step: 1790 loss: 0.048\n",
      "step: 1791 loss: 0.048\n",
      "step: 1792 loss: 0.029\n",
      "step: 1793 loss: 0.072\n",
      "step: 1794 loss: 0.054\n",
      "step: 1795 loss: 0.043\n",
      "step: 1796 loss: 0.044\n",
      "step: 1797 loss: 0.053\n",
      "step: 1798 loss: 0.036\n",
      "step: 1799 loss: 0.035\n",
      "step: 1800 loss: 0.051\n",
      "step: 1801 loss: 0.061\n",
      "step: 1802 loss: 0.057\n",
      "step: 1803 loss: 0.044\n",
      "step: 1804 loss: 0.044\n",
      "step: 1805 loss: 0.062\n",
      "step: 1806 loss: 0.044\n",
      "step: 1807 loss: 0.067\n",
      "step: 1808 loss: 0.046\n",
      "step: 1809 loss: 0.051\n",
      "step: 1810 loss: 0.045\n",
      "step: 1811 loss: 0.064\n",
      "step: 1812 loss: 0.060\n",
      "step: 1813 loss: 0.063\n",
      "step: 1814 loss: 0.047\n",
      "step: 1815 loss: 0.042\n",
      "step: 1816 loss: 0.033\n",
      "step: 1817 loss: 0.043\n",
      "step: 1818 loss: 0.044\n",
      "step: 1819 loss: 0.038\n",
      "step: 1820 loss: 0.066\n",
      "step: 1821 loss: 0.050\n",
      "step: 1822 loss: 0.063\n",
      "step: 1823 loss: 0.042\n",
      "step: 1824 loss: 0.049\n",
      "step: 1825 loss: 0.045\n",
      "step: 1826 loss: 0.035\n",
      "step: 1827 loss: 0.056\n",
      "step: 1828 loss: 0.075\n",
      "step: 1829 loss: 0.047\n",
      "step: 1830 loss: 0.043\n",
      "step: 1831 loss: 0.041\n",
      "step: 1832 loss: 0.065\n",
      "step: 1833 loss: 0.050\n",
      "step: 1834 loss: 0.038\n",
      "step: 1835 loss: 0.059\n",
      "step: 1836 loss: 0.042\n",
      "step: 1837 loss: 0.073\n",
      "step: 1838 loss: 0.055\n",
      "step: 1839 loss: 0.069\n",
      "step: 1840 loss: 0.057\n",
      "step: 1841 loss: 0.040\n",
      "step: 1842 loss: 0.043\n",
      "step: 1843 loss: 0.061\n",
      "step: 1844 loss: 0.055\n",
      "step: 1845 loss: 0.045\n",
      "step: 1846 loss: 0.059\n",
      "step: 1847 loss: 0.049\n",
      "step: 1848 loss: 0.056\n",
      "step: 1849 loss: 0.052\n",
      "step: 1850 loss: 0.061\n",
      "step: 1851 loss: 0.046\n",
      "step: 1852 loss: 0.039\n",
      "step: 1853 loss: 0.035\n",
      "step: 1854 loss: 0.064\n",
      "step: 1855 loss: 0.042\n",
      "step: 1856 loss: 0.065\n",
      "step: 1857 loss: 0.036\n",
      "step: 1858 loss: 0.051\n",
      "step: 1859 loss: 0.057\n",
      "step: 1860 loss: 0.041\n",
      "step: 1861 loss: 0.044\n",
      "step: 1862 loss: 0.035\n",
      "step: 1863 loss: 0.050\n",
      "step: 1864 loss: 0.046\n",
      "step: 1865 loss: 0.049\n",
      "step: 1866 loss: 0.043\n",
      "step: 1867 loss: 0.059\n",
      "step: 1868 loss: 0.034\n",
      "step: 1869 loss: 0.052\n",
      "step: 1870 loss: 0.052\n",
      "step: 1871 loss: 0.041\n",
      "step: 1872 loss: 0.032\n",
      "step: 1873 loss: 0.060\n",
      "step: 1874 loss: 0.036\n",
      "step: 1875 loss: 0.031\n",
      "step: 1876 loss: 0.047\n",
      "step: 1877 loss: 0.036\n",
      "step: 1878 loss: 0.055\n",
      "step: 1879 loss: 0.066\n",
      "step: 1880 loss: 0.045\n",
      "step: 1881 loss: 0.035\n",
      "step: 1882 loss: 0.052\n",
      "step: 1883 loss: 0.059\n",
      "step: 1884 loss: 0.055\n",
      "step: 1885 loss: 0.077\n",
      "step: 1886 loss: 0.042\n",
      "step: 1887 loss: 0.029\n",
      "step: 1888 loss: 0.042\n",
      "step: 1889 loss: 0.060\n",
      "step: 1890 loss: 0.043\n",
      "step: 1891 loss: 0.068\n",
      "step: 1892 loss: 0.036\n",
      "step: 1893 loss: 0.044\n",
      "step: 1894 loss: 0.053\n",
      "step: 1895 loss: 0.068\n",
      "step: 1896 loss: 0.066\n",
      "step: 1897 loss: 0.062\n",
      "step: 1898 loss: 0.065\n",
      "step: 1899 loss: 0.048\n",
      "step: 1900 loss: 0.040\n",
      "step: 1901 loss: 0.048\n",
      "step: 1902 loss: 0.046\n",
      "step: 1903 loss: 0.040\n",
      "step: 1904 loss: 0.056\n",
      "step: 1905 loss: 0.052\n",
      "step: 1906 loss: 0.040\n",
      "step: 1907 loss: 0.061\n",
      "step: 1908 loss: 0.053\n",
      "step: 1909 loss: 0.041\n",
      "step: 1910 loss: 0.045\n",
      "step: 1911 loss: 0.041\n",
      "step: 1912 loss: 0.049\n",
      "step: 1913 loss: 0.042\n",
      "step: 1914 loss: 0.066\n",
      "step: 1915 loss: 0.045\n",
      "step: 1916 loss: 0.042\n",
      "step: 1917 loss: 0.048\n",
      "step: 1918 loss: 0.050\n",
      "step: 1919 loss: 0.049\n",
      "step: 1920 loss: 0.051\n",
      "step: 1921 loss: 0.077\n",
      "step: 1922 loss: 0.048\n",
      "step: 1923 loss: 0.069\n",
      "step: 1924 loss: 0.053\n",
      "step: 1925 loss: 0.049\n",
      "step: 1926 loss: 0.061\n",
      "step: 1927 loss: 0.051\n",
      "step: 1928 loss: 0.072\n",
      "step: 1929 loss: 0.056\n",
      "step: 1930 loss: 0.049\n",
      "step: 1931 loss: 0.038\n",
      "step: 1932 loss: 0.062\n",
      "step: 1933 loss: 0.040\n",
      "step: 1934 loss: 0.063\n",
      "step: 1935 loss: 0.039\n",
      "step: 1936 loss: 0.057\n",
      "step: 1937 loss: 0.043\n",
      "step: 1938 loss: 0.029\n",
      "step: 1939 loss: 0.041\n",
      "step: 1940 loss: 0.050\n",
      "step: 1941 loss: 0.032\n",
      "step: 1942 loss: 0.049\n",
      "step: 1943 loss: 0.060\n",
      "step: 1944 loss: 0.044\n",
      "step: 1945 loss: 0.042\n",
      "step: 1946 loss: 0.061\n",
      "step: 1947 loss: 0.044\n",
      "step: 1948 loss: 0.061\n",
      "step: 1949 loss: 0.062\n",
      "step: 1950 loss: 0.045\n",
      "step: 1951 loss: 0.056\n",
      "step: 1952 loss: 0.052\n",
      "step: 1953 loss: 0.051\n",
      "step: 1954 loss: 0.042\n",
      "step: 1955 loss: 0.052\n",
      "step: 1956 loss: 0.035\n",
      "step: 1957 loss: 0.038\n",
      "step: 1958 loss: 0.047\n",
      "step: 1959 loss: 0.040\n",
      "step: 1960 loss: 0.041\n",
      "step: 1961 loss: 0.049\n",
      "step: 1962 loss: 0.054\n",
      "step: 1963 loss: 0.034\n",
      "step: 1964 loss: 0.053\n",
      "step: 1965 loss: 0.039\n",
      "step: 1966 loss: 0.048\n",
      "step: 1967 loss: 0.047\n",
      "step: 1968 loss: 0.047\n",
      "step: 1969 loss: 0.044\n",
      "step: 1970 loss: 0.041\n",
      "step: 1971 loss: 0.040\n",
      "step: 1972 loss: 0.059\n",
      "step: 1973 loss: 0.051\n",
      "step: 1974 loss: 0.047\n",
      "step: 1975 loss: 0.047\n",
      "step: 1976 loss: 0.038\n",
      "step: 1977 loss: 0.044\n",
      "step: 1978 loss: 0.061\n",
      "step: 1979 loss: 0.057\n",
      "step: 1980 loss: 0.046\n",
      "step: 1981 loss: 0.035\n",
      "step: 1982 loss: 0.057\n",
      "step: 1983 loss: 0.042\n",
      "step: 1984 loss: 0.043\n",
      "step: 1985 loss: 0.046\n",
      "step: 1986 loss: 0.045\n",
      "step: 1987 loss: 0.038\n",
      "step: 1988 loss: 0.042\n",
      "step: 1989 loss: 0.051\n",
      "step: 1990 loss: 0.046\n",
      "step: 1991 loss: 0.042\n",
      "step: 1992 loss: 0.048\n",
      "step: 1993 loss: 0.036\n",
      "step: 1994 loss: 0.085\n",
      "step: 1995 loss: 0.036\n",
      "step: 1996 loss: 0.035\n",
      "step: 1997 loss: 0.045\n",
      "step: 1998 loss: 0.040\n",
      "step: 1999 loss: 0.045\n",
      "step: 2000 loss: 0.043\n",
      "step: 2001 loss: 0.055\n",
      "step: 2002 loss: 0.046\n",
      "step: 2003 loss: 0.035\n",
      "step: 2004 loss: 0.037\n",
      "step: 2005 loss: 0.050\n",
      "step: 2006 loss: 0.043\n",
      "step: 2007 loss: 0.054\n",
      "step: 2008 loss: 0.040\n",
      "step: 2009 loss: 0.037\n",
      "step: 2010 loss: 0.036\n",
      "step: 2011 loss: 0.039\n",
      "step: 2012 loss: 0.041\n",
      "step: 2013 loss: 0.053\n",
      "step: 2014 loss: 0.038\n",
      "step: 2015 loss: 0.058\n",
      "step: 2016 loss: 0.044\n",
      "step: 2017 loss: 0.041\n",
      "step: 2018 loss: 0.037\n",
      "step: 2019 loss: 0.039\n",
      "step: 2020 loss: 0.032\n",
      "step: 2021 loss: 0.040\n",
      "step: 2022 loss: 0.037\n",
      "step: 2023 loss: 0.037\n",
      "step: 2024 loss: 0.052\n",
      "step: 2025 loss: 0.036\n",
      "step: 2026 loss: 0.049\n",
      "step: 2027 loss: 0.043\n",
      "step: 2028 loss: 0.044\n",
      "step: 2029 loss: 0.042\n",
      "step: 2030 loss: 0.050\n",
      "step: 2031 loss: 0.047\n",
      "step: 2032 loss: 0.037\n",
      "step: 2033 loss: 0.036\n",
      "step: 2034 loss: 0.050\n",
      "step: 2035 loss: 0.044\n",
      "step: 2036 loss: 0.039\n",
      "step: 2037 loss: 0.037\n",
      "step: 2038 loss: 0.026\n",
      "step: 2039 loss: 0.050\n",
      "step: 2040 loss: 0.043\n",
      "step: 2041 loss: 0.029\n",
      "step: 2042 loss: 0.035\n",
      "step: 2043 loss: 0.055\n",
      "step: 2044 loss: 0.042\n",
      "step: 2045 loss: 0.044\n",
      "step: 2046 loss: 0.063\n",
      "step: 2047 loss: 0.038\n",
      "step: 2048 loss: 0.060\n",
      "step: 2049 loss: 0.054\n",
      "step: 2050 loss: 0.038\n",
      "step: 2051 loss: 0.043\n",
      "step: 2052 loss: 0.039\n",
      "step: 2053 loss: 0.065\n",
      "step: 2054 loss: 0.052\n",
      "step: 2055 loss: 0.041\n",
      "step: 2056 loss: 0.056\n",
      "step: 2057 loss: 0.049\n",
      "step: 2058 loss: 0.053\n",
      "step: 2059 loss: 0.044\n",
      "step: 2060 loss: 0.050\n",
      "step: 2061 loss: 0.050\n",
      "step: 2062 loss: 0.041\n",
      "step: 2063 loss: 0.041\n",
      "step: 2064 loss: 0.035\n",
      "step: 2065 loss: 0.032\n",
      "step: 2066 loss: 0.060\n",
      "step: 2067 loss: 0.039\n",
      "step: 2068 loss: 0.048\n",
      "step: 2069 loss: 0.029\n",
      "step: 2070 loss: 0.048\n",
      "step: 2071 loss: 0.029\n",
      "step: 2072 loss: 0.039\n",
      "step: 2073 loss: 0.035\n",
      "step: 2074 loss: 0.040\n",
      "step: 2075 loss: 0.039\n",
      "step: 2076 loss: 0.047\n",
      "step: 2077 loss: 0.039\n",
      "step: 2078 loss: 0.053\n",
      "step: 2079 loss: 0.037\n",
      "step: 2080 loss: 0.053\n",
      "step: 2081 loss: 0.041\n",
      "step: 2082 loss: 0.038\n",
      "step: 2083 loss: 0.068\n",
      "step: 2084 loss: 0.038\n",
      "step: 2085 loss: 0.025\n",
      "step: 2086 loss: 0.053\n",
      "step: 2087 loss: 0.033\n",
      "step: 2088 loss: 0.024\n",
      "step: 2089 loss: 0.061\n",
      "step: 2090 loss: 0.042\n",
      "step: 2091 loss: 0.046\n",
      "step: 2092 loss: 0.059\n",
      "step: 2093 loss: 0.038\n",
      "step: 2094 loss: 0.036\n",
      "step: 2095 loss: 0.029\n",
      "step: 2096 loss: 0.054\n",
      "step: 2097 loss: 0.057\n",
      "step: 2098 loss: 0.065\n",
      "step: 2099 loss: 0.041\n",
      "step: 2100 loss: 0.058\n",
      "step: 2101 loss: 0.025\n",
      "step: 2102 loss: 0.056\n",
      "step: 2103 loss: 0.059\n",
      "step: 2104 loss: 0.028\n",
      "step: 2105 loss: 0.027\n",
      "step: 2106 loss: 0.045\n",
      "step: 2107 loss: 0.059\n",
      "step: 2108 loss: 0.046\n",
      "step: 2109 loss: 0.057\n",
      "step: 2110 loss: 0.037\n",
      "step: 2111 loss: 0.041\n",
      "step: 2112 loss: 0.042\n",
      "step: 2113 loss: 0.062\n",
      "step: 2114 loss: 0.046\n",
      "step: 2115 loss: 0.036\n",
      "step: 2116 loss: 0.065\n",
      "step: 2117 loss: 0.029\n",
      "step: 2118 loss: 0.046\n",
      "step: 2119 loss: 0.046\n",
      "step: 2120 loss: 0.057\n",
      "step: 2121 loss: 0.046\n",
      "step: 2122 loss: 0.049\n",
      "step: 2123 loss: 0.065\n",
      "step: 2124 loss: 0.033\n",
      "step: 2125 loss: 0.036\n",
      "step: 2126 loss: 0.030\n",
      "step: 2127 loss: 0.026\n",
      "step: 2128 loss: 0.048\n",
      "step: 2129 loss: 0.049\n",
      "step: 2130 loss: 0.036\n",
      "step: 2131 loss: 0.046\n",
      "step: 2132 loss: 0.039\n",
      "step: 2133 loss: 0.043\n",
      "step: 2134 loss: 0.035\n",
      "step: 2135 loss: 0.036\n",
      "step: 2136 loss: 0.038\n",
      "step: 2137 loss: 0.049\n",
      "step: 2138 loss: 0.046\n",
      "step: 2139 loss: 0.051\n",
      "step: 2140 loss: 0.035\n",
      "step: 2141 loss: 0.051\n",
      "step: 2142 loss: 0.029\n",
      "step: 2143 loss: 0.064\n",
      "step: 2144 loss: 0.057\n",
      "step: 2145 loss: 0.043\n",
      "step: 2146 loss: 0.074\n",
      "step: 2147 loss: 0.042\n",
      "step: 2148 loss: 0.041\n",
      "step: 2149 loss: 0.058\n",
      "step: 2150 loss: 0.040\n",
      "step: 2151 loss: 0.033\n",
      "step: 2152 loss: 0.047\n",
      "step: 2153 loss: 0.055\n",
      "step: 2154 loss: 0.036\n",
      "step: 2155 loss: 0.053\n",
      "step: 2156 loss: 0.055\n",
      "step: 2157 loss: 0.028\n",
      "step: 2158 loss: 0.034\n",
      "step: 2159 loss: 0.024\n",
      "step: 2160 loss: 0.043\n",
      "step: 2161 loss: 0.044\n",
      "step: 2162 loss: 0.049\n",
      "step: 2163 loss: 0.037\n",
      "step: 2164 loss: 0.039\n",
      "step: 2165 loss: 0.044\n",
      "step: 2166 loss: 0.037\n",
      "step: 2167 loss: 0.039\n",
      "step: 2168 loss: 0.039\n",
      "step: 2169 loss: 0.042\n",
      "step: 2170 loss: 0.051\n",
      "step: 2171 loss: 0.040\n",
      "step: 2172 loss: 0.066\n",
      "step: 2173 loss: 0.036\n",
      "step: 2174 loss: 0.047\n",
      "step: 2175 loss: 0.040\n",
      "step: 2176 loss: 0.029\n",
      "step: 2177 loss: 0.049\n",
      "step: 2178 loss: 0.035\n",
      "step: 2179 loss: 0.049\n",
      "step: 2180 loss: 0.042\n",
      "step: 2181 loss: 0.039\n",
      "step: 2182 loss: 0.037\n",
      "step: 2183 loss: 0.050\n",
      "step: 2184 loss: 0.037\n",
      "step: 2185 loss: 0.046\n",
      "step: 2186 loss: 0.052\n",
      "step: 2187 loss: 0.042\n",
      "step: 2188 loss: 0.050\n",
      "step: 2189 loss: 0.035\n",
      "step: 2190 loss: 0.054\n",
      "step: 2191 loss: 0.042\n",
      "step: 2192 loss: 0.057\n",
      "step: 2193 loss: 0.047\n",
      "step: 2194 loss: 0.040\n",
      "step: 2195 loss: 0.046\n",
      "step: 2196 loss: 0.048\n",
      "step: 2197 loss: 0.043\n",
      "step: 2198 loss: 0.048\n",
      "step: 2199 loss: 0.058\n",
      "step: 2200 loss: 0.044\n",
      "step: 2201 loss: 0.037\n",
      "step: 2202 loss: 0.041\n",
      "step: 2203 loss: 0.049\n",
      "step: 2204 loss: 0.042\n",
      "step: 2205 loss: 0.085\n",
      "step: 2206 loss: 0.041\n",
      "step: 2207 loss: 0.043\n",
      "step: 2208 loss: 0.035\n",
      "step: 2209 loss: 0.055\n",
      "step: 2210 loss: 0.051\n",
      "step: 2211 loss: 0.039\n",
      "step: 2212 loss: 0.036\n",
      "step: 2213 loss: 0.057\n",
      "step: 2214 loss: 0.044\n",
      "step: 2215 loss: 0.051\n",
      "step: 2216 loss: 0.041\n",
      "step: 2217 loss: 0.060\n",
      "step: 2218 loss: 0.042\n",
      "step: 2219 loss: 0.047\n",
      "step: 2220 loss: 0.040\n",
      "step: 2221 loss: 0.042\n",
      "step: 2222 loss: 0.038\n",
      "step: 2223 loss: 0.052\n",
      "step: 2224 loss: 0.044\n",
      "step: 2225 loss: 0.065\n",
      "step: 2226 loss: 0.052\n",
      "step: 2227 loss: 0.047\n",
      "step: 2228 loss: 0.052\n",
      "step: 2229 loss: 0.051\n",
      "step: 2230 loss: 0.043\n",
      "step: 2231 loss: 0.039\n",
      "step: 2232 loss: 0.061\n",
      "step: 2233 loss: 0.048\n",
      "step: 2234 loss: 0.039\n",
      "step: 2235 loss: 0.046\n",
      "step: 2236 loss: 0.046\n",
      "step: 2237 loss: 0.061\n",
      "step: 2238 loss: 0.042\n",
      "step: 2239 loss: 0.035\n",
      "step: 2240 loss: 0.038\n",
      "step: 2241 loss: 0.058\n",
      "step: 2242 loss: 0.065\n",
      "step: 2243 loss: 0.058\n",
      "step: 2244 loss: 0.037\n",
      "step: 2245 loss: 0.042\n",
      "step: 2246 loss: 0.046\n",
      "step: 2247 loss: 0.040\n",
      "step: 2248 loss: 0.061\n",
      "step: 2249 loss: 0.041\n",
      "step: 2250 loss: 0.050\n",
      "step: 2251 loss: 0.041\n",
      "step: 2252 loss: 0.053\n",
      "step: 2253 loss: 0.043\n",
      "step: 2254 loss: 0.044\n",
      "step: 2255 loss: 0.050\n",
      "step: 2256 loss: 0.033\n",
      "step: 2257 loss: 0.042\n",
      "step: 2258 loss: 0.059\n",
      "step: 2259 loss: 0.058\n",
      "step: 2260 loss: 0.039\n",
      "step: 2261 loss: 0.034\n",
      "step: 2262 loss: 0.044\n",
      "step: 2263 loss: 0.044\n",
      "step: 2264 loss: 0.054\n",
      "step: 2265 loss: 0.049\n",
      "step: 2266 loss: 0.043\n",
      "step: 2267 loss: 0.052\n",
      "step: 2268 loss: 0.053\n",
      "step: 2269 loss: 0.045\n",
      "step: 2270 loss: 0.046\n",
      "step: 2271 loss: 0.036\n",
      "step: 2272 loss: 0.046\n",
      "step: 2273 loss: 0.039\n",
      "step: 2274 loss: 0.042\n",
      "step: 2275 loss: 0.046\n",
      "step: 2276 loss: 0.049\n",
      "step: 2277 loss: 0.035\n",
      "step: 2278 loss: 0.046\n",
      "step: 2279 loss: 0.033\n",
      "step: 2280 loss: 0.045\n",
      "step: 2281 loss: 0.033\n",
      "step: 2282 loss: 0.043\n",
      "step: 2283 loss: 0.050\n",
      "step: 2284 loss: 0.056\n",
      "step: 2285 loss: 0.052\n",
      "step: 2286 loss: 0.052\n",
      "step: 2287 loss: 0.060\n",
      "step: 2288 loss: 0.030\n",
      "step: 2289 loss: 0.039\n",
      "step: 2290 loss: 0.050\n",
      "step: 2291 loss: 0.020\n",
      "step: 2292 loss: 0.054\n",
      "step: 2293 loss: 0.032\n",
      "step: 2294 loss: 0.053\n",
      "step: 2295 loss: 0.059\n",
      "step: 2296 loss: 0.049\n",
      "step: 2297 loss: 0.037\n",
      "step: 2298 loss: 0.048\n",
      "step: 2299 loss: 0.035\n",
      "step: 2300 loss: 0.051\n",
      "step: 2301 loss: 0.038\n",
      "step: 2302 loss: 0.041\n",
      "step: 2303 loss: 0.038\n",
      "step: 2304 loss: 0.062\n",
      "step: 2305 loss: 0.042\n",
      "step: 2306 loss: 0.057\n",
      "step: 2307 loss: 0.034\n",
      "step: 2308 loss: 0.037\n",
      "step: 2309 loss: 0.045\n",
      "step: 2310 loss: 0.036\n",
      "step: 2311 loss: 0.046\n",
      "step: 2312 loss: 0.051\n",
      "step: 2313 loss: 0.049\n",
      "step: 2314 loss: 0.045\n",
      "step: 2315 loss: 0.036\n",
      "step: 2316 loss: 0.026\n",
      "step: 2317 loss: 0.038\n",
      "step: 2318 loss: 0.049\n",
      "step: 2319 loss: 0.052\n",
      "step: 2320 loss: 0.043\n",
      "step: 2321 loss: 0.046\n",
      "step: 2322 loss: 0.045\n",
      "step: 2323 loss: 0.052\n",
      "step: 2324 loss: 0.041\n",
      "step: 2325 loss: 0.056\n",
      "step: 2326 loss: 0.038\n",
      "step: 2327 loss: 0.052\n",
      "step: 2328 loss: 0.058\n",
      "step: 2329 loss: 0.055\n",
      "step: 2330 loss: 0.050\n",
      "step: 2331 loss: 0.043\n",
      "step: 2332 loss: 0.046\n",
      "step: 2333 loss: 0.046\n",
      "step: 2334 loss: 0.041\n",
      "step: 2335 loss: 0.044\n",
      "step: 2336 loss: 0.041\n",
      "step: 2337 loss: 0.026\n",
      "step: 2338 loss: 0.027\n",
      "step: 2339 loss: 0.030\n",
      "step: 2340 loss: 0.034\n",
      "step: 2341 loss: 0.056\n",
      "step: 2342 loss: 0.044\n",
      "step: 2343 loss: 0.033\n",
      "step: 2344 loss: 0.039\n",
      "step: 2345 loss: 0.051\n",
      "step: 2346 loss: 0.058\n",
      "step: 2347 loss: 0.046\n",
      "step: 2348 loss: 0.041\n",
      "step: 2349 loss: 0.026\n",
      "step: 2350 loss: 0.029\n",
      "step: 2351 loss: 0.043\n",
      "step: 2352 loss: 0.030\n",
      "step: 2353 loss: 0.052\n",
      "step: 2354 loss: 0.051\n",
      "step: 2355 loss: 0.034\n",
      "step: 2356 loss: 0.040\n",
      "step: 2357 loss: 0.057\n",
      "step: 2358 loss: 0.042\n",
      "step: 2359 loss: 0.065\n",
      "step: 2360 loss: 0.044\n",
      "step: 2361 loss: 0.043\n",
      "step: 2362 loss: 0.038\n",
      "step: 2363 loss: 0.036\n",
      "step: 2364 loss: 0.039\n",
      "step: 2365 loss: 0.031\n",
      "step: 2366 loss: 0.044\n",
      "step: 2367 loss: 0.043\n",
      "step: 2368 loss: 0.028\n",
      "step: 2369 loss: 0.040\n",
      "step: 2370 loss: 0.040\n",
      "step: 2371 loss: 0.044\n",
      "step: 2372 loss: 0.055\n",
      "step: 2373 loss: 0.033\n",
      "step: 2374 loss: 0.042\n",
      "step: 2375 loss: 0.045\n",
      "step: 2376 loss: 0.052\n",
      "step: 2377 loss: 0.052\n",
      "step: 2378 loss: 0.037\n",
      "step: 2379 loss: 0.070\n",
      "step: 2380 loss: 0.041\n",
      "step: 2381 loss: 0.046\n",
      "step: 2382 loss: 0.043\n",
      "step: 2383 loss: 0.055\n",
      "step: 2384 loss: 0.037\n",
      "step: 2385 loss: 0.053\n",
      "step: 2386 loss: 0.050\n",
      "step: 2387 loss: 0.036\n",
      "step: 2388 loss: 0.033\n",
      "step: 2389 loss: 0.025\n",
      "step: 2390 loss: 0.033\n",
      "step: 2391 loss: 0.056\n",
      "step: 2392 loss: 0.046\n",
      "step: 2393 loss: 0.041\n",
      "step: 2394 loss: 0.034\n",
      "step: 2395 loss: 0.042\n",
      "step: 2396 loss: 0.038\n",
      "step: 2397 loss: 0.052\n",
      "step: 2398 loss: 0.045\n",
      "step: 2399 loss: 0.035\n",
      "step: 2400 loss: 0.022\n",
      "step: 2401 loss: 0.043\n",
      "step: 2402 loss: 0.036\n",
      "step: 2403 loss: 0.042\n",
      "step: 2404 loss: 0.032\n",
      "step: 2405 loss: 0.055\n",
      "step: 2406 loss: 0.031\n",
      "step: 2407 loss: 0.023\n",
      "step: 2408 loss: 0.036\n",
      "step: 2409 loss: 0.028\n",
      "step: 2410 loss: 0.033\n",
      "step: 2411 loss: 0.059\n",
      "step: 2412 loss: 0.040\n",
      "step: 2413 loss: 0.056\n",
      "step: 2414 loss: 0.042\n",
      "step: 2415 loss: 0.048\n",
      "step: 2416 loss: 0.063\n",
      "step: 2417 loss: 0.059\n",
      "step: 2418 loss: 0.051\n",
      "step: 2419 loss: 0.053\n",
      "step: 2420 loss: 0.037\n",
      "step: 2421 loss: 0.042\n",
      "step: 2422 loss: 0.039\n",
      "step: 2423 loss: 0.043\n",
      "step: 2424 loss: 0.031\n",
      "step: 2425 loss: 0.052\n",
      "step: 2426 loss: 0.053\n",
      "step: 2427 loss: 0.058\n",
      "step: 2428 loss: 0.040\n",
      "step: 2429 loss: 0.030\n",
      "step: 2430 loss: 0.039\n",
      "step: 2431 loss: 0.053\n",
      "step: 2432 loss: 0.056\n",
      "step: 2433 loss: 0.035\n",
      "step: 2434 loss: 0.037\n",
      "step: 2435 loss: 0.043\n",
      "step: 2436 loss: 0.032\n",
      "step: 2437 loss: 0.037\n",
      "step: 2438 loss: 0.042\n",
      "step: 2439 loss: 0.043\n",
      "step: 2440 loss: 0.045\n",
      "step: 2441 loss: 0.044\n",
      "step: 2442 loss: 0.034\n",
      "step: 2443 loss: 0.024\n",
      "step: 2444 loss: 0.031\n",
      "step: 2445 loss: 0.033\n",
      "step: 2446 loss: 0.050\n",
      "step: 2447 loss: 0.056\n",
      "step: 2448 loss: 0.039\n",
      "step: 2449 loss: 0.041\n",
      "step: 2450 loss: 0.040\n",
      "step: 2451 loss: 0.039\n",
      "step: 2452 loss: 0.052\n",
      "step: 2453 loss: 0.047\n",
      "step: 2454 loss: 0.055\n",
      "step: 2455 loss: 0.050\n",
      "step: 2456 loss: 0.040\n",
      "step: 2457 loss: 0.042\n",
      "step: 2458 loss: 0.035\n",
      "step: 2459 loss: 0.047\n",
      "step: 2460 loss: 0.030\n",
      "step: 2461 loss: 0.053\n",
      "step: 2462 loss: 0.042\n",
      "step: 2463 loss: 0.041\n",
      "step: 2464 loss: 0.032\n",
      "step: 2465 loss: 0.035\n",
      "step: 2466 loss: 0.054\n",
      "step: 2467 loss: 0.050\n",
      "step: 2468 loss: 0.042\n",
      "step: 2469 loss: 0.046\n",
      "step: 2470 loss: 0.052\n",
      "step: 2471 loss: 0.044\n",
      "step: 2472 loss: 0.052\n",
      "step: 2473 loss: 0.033\n",
      "step: 2474 loss: 0.035\n",
      "step: 2475 loss: 0.046\n",
      "step: 2476 loss: 0.043\n",
      "step: 2477 loss: 0.048\n",
      "step: 2478 loss: 0.044\n",
      "step: 2479 loss: 0.048\n",
      "step: 2480 loss: 0.062\n",
      "step: 2481 loss: 0.050\n",
      "step: 2482 loss: 0.025\n",
      "step: 2483 loss: 0.053\n",
      "step: 2484 loss: 0.059\n",
      "step: 2485 loss: 0.062\n",
      "step: 2486 loss: 0.048\n",
      "step: 2487 loss: 0.056\n",
      "step: 2488 loss: 0.039\n",
      "step: 2489 loss: 0.054\n",
      "step: 2490 loss: 0.040\n",
      "step: 2491 loss: 0.041\n",
      "step: 2492 loss: 0.054\n",
      "step: 2493 loss: 0.041\n",
      "step: 2494 loss: 0.034\n",
      "step: 2495 loss: 0.043\n",
      "step: 2496 loss: 0.052\n",
      "step: 2497 loss: 0.049\n",
      "step: 2498 loss: 0.051\n",
      "step: 2499 loss: 0.039\n",
      "step: 2500 loss: 0.037\n",
      "step: 2501 loss: 0.062\n",
      "step: 2502 loss: 0.033\n",
      "step: 2503 loss: 0.036\n",
      "step: 2504 loss: 0.046\n",
      "step: 2505 loss: 0.045\n",
      "step: 2506 loss: 0.031\n",
      "step: 2507 loss: 0.053\n",
      "step: 2508 loss: 0.048\n",
      "step: 2509 loss: 0.044\n",
      "step: 2510 loss: 0.050\n",
      "step: 2511 loss: 0.052\n",
      "step: 2512 loss: 0.068\n",
      "step: 2513 loss: 0.033\n",
      "step: 2514 loss: 0.057\n",
      "step: 2515 loss: 0.042\n",
      "step: 2516 loss: 0.058\n",
      "step: 2517 loss: 0.048\n",
      "step: 2518 loss: 0.036\n",
      "step: 2519 loss: 0.050\n",
      "step: 2520 loss: 0.047\n",
      "step: 2521 loss: 0.046\n",
      "step: 2522 loss: 0.038\n",
      "step: 2523 loss: 0.046\n",
      "step: 2524 loss: 0.032\n",
      "step: 2525 loss: 0.038\n",
      "step: 2526 loss: 0.045\n",
      "step: 2527 loss: 0.052\n",
      "step: 2528 loss: 0.051\n",
      "step: 2529 loss: 0.037\n",
      "step: 2530 loss: 0.043\n",
      "step: 2531 loss: 0.040\n",
      "step: 2532 loss: 0.041\n",
      "step: 2533 loss: 0.035\n",
      "step: 2534 loss: 0.049\n",
      "step: 2535 loss: 0.036\n",
      "step: 2536 loss: 0.031\n",
      "step: 2537 loss: 0.045\n",
      "step: 2538 loss: 0.041\n",
      "step: 2539 loss: 0.064\n",
      "step: 2540 loss: 0.047\n",
      "step: 2541 loss: 0.038\n",
      "step: 2542 loss: 0.056\n",
      "step: 2543 loss: 0.042\n",
      "step: 2544 loss: 0.042\n",
      "step: 2545 loss: 0.037\n",
      "step: 2546 loss: 0.059\n",
      "step: 2547 loss: 0.038\n",
      "step: 2548 loss: 0.065\n",
      "step: 2549 loss: 0.039\n",
      "step: 2550 loss: 0.029\n",
      "step: 2551 loss: 0.056\n",
      "step: 2552 loss: 0.035\n",
      "step: 2553 loss: 0.057\n",
      "step: 2554 loss: 0.029\n",
      "step: 2555 loss: 0.057\n",
      "step: 2556 loss: 0.042\n",
      "step: 2557 loss: 0.040\n",
      "step: 2558 loss: 0.058\n",
      "step: 2559 loss: 0.034\n",
      "step: 2560 loss: 0.024\n",
      "step: 2561 loss: 0.051\n",
      "step: 2562 loss: 0.035\n",
      "step: 2563 loss: 0.050\n",
      "step: 2564 loss: 0.042\n",
      "step: 2565 loss: 0.040\n",
      "step: 2566 loss: 0.069\n",
      "step: 2567 loss: 0.040\n",
      "step: 2568 loss: 0.041\n",
      "step: 2569 loss: 0.040\n",
      "step: 2570 loss: 0.066\n",
      "step: 2571 loss: 0.032\n",
      "step: 2572 loss: 0.038\n",
      "step: 2573 loss: 0.035\n",
      "step: 2574 loss: 0.052\n",
      "step: 2575 loss: 0.034\n",
      "step: 2576 loss: 0.041\n",
      "step: 2577 loss: 0.032\n",
      "step: 2578 loss: 0.048\n",
      "step: 2579 loss: 0.044\n",
      "step: 2580 loss: 0.046\n",
      "step: 2581 loss: 0.046\n",
      "step: 2582 loss: 0.039\n",
      "step: 2583 loss: 0.050\n",
      "step: 2584 loss: 0.031\n",
      "step: 2585 loss: 0.042\n",
      "step: 2586 loss: 0.037\n",
      "step: 2587 loss: 0.029\n",
      "step: 2588 loss: 0.041\n",
      "step: 2589 loss: 0.029\n",
      "step: 2590 loss: 0.042\n",
      "step: 2591 loss: 0.044\n",
      "step: 2592 loss: 0.033\n",
      "step: 2593 loss: 0.042\n",
      "step: 2594 loss: 0.043\n",
      "step: 2595 loss: 0.039\n",
      "step: 2596 loss: 0.047\n",
      "step: 2597 loss: 0.054\n",
      "step: 2598 loss: 0.050\n",
      "step: 2599 loss: 0.039\n",
      "step: 2600 loss: 0.023\n",
      "step: 2601 loss: 0.035\n",
      "step: 2602 loss: 0.048\n",
      "step: 2603 loss: 0.042\n",
      "step: 2604 loss: 0.028\n",
      "step: 2605 loss: 0.029\n",
      "step: 2606 loss: 0.040\n",
      "step: 2607 loss: 0.017\n",
      "step: 2608 loss: 0.045\n",
      "step: 2609 loss: 0.060\n",
      "step: 2610 loss: 0.060\n",
      "step: 2611 loss: 0.042\n",
      "step: 2612 loss: 0.059\n",
      "step: 2613 loss: 0.043\n",
      "step: 2614 loss: 0.045\n",
      "step: 2615 loss: 0.033\n",
      "step: 2616 loss: 0.040\n",
      "step: 2617 loss: 0.052\n",
      "step: 2618 loss: 0.029\n",
      "step: 2619 loss: 0.044\n",
      "step: 2620 loss: 0.038\n",
      "step: 2621 loss: 0.058\n",
      "step: 2622 loss: 0.030\n",
      "step: 2623 loss: 0.047\n",
      "step: 2624 loss: 0.052\n",
      "step: 2625 loss: 0.046\n",
      "step: 2626 loss: 0.042\n",
      "step: 2627 loss: 0.052\n",
      "step: 2628 loss: 0.051\n",
      "step: 2629 loss: 0.040\n",
      "step: 2630 loss: 0.037\n",
      "step: 2631 loss: 0.035\n",
      "step: 2632 loss: 0.037\n",
      "step: 2633 loss: 0.036\n",
      "step: 2634 loss: 0.020\n",
      "step: 2635 loss: 0.032\n",
      "step: 2636 loss: 0.040\n",
      "step: 2637 loss: 0.033\n",
      "step: 2638 loss: 0.060\n",
      "step: 2639 loss: 0.025\n",
      "step: 2640 loss: 0.052\n",
      "step: 2641 loss: 0.041\n",
      "step: 2642 loss: 0.048\n",
      "step: 2643 loss: 0.039\n",
      "step: 2644 loss: 0.052\n",
      "step: 2645 loss: 0.040\n",
      "step: 2646 loss: 0.056\n",
      "step: 2647 loss: 0.034\n",
      "step: 2648 loss: 0.043\n",
      "step: 2649 loss: 0.034\n",
      "step: 2650 loss: 0.038\n",
      "step: 2651 loss: 0.050\n",
      "step: 2652 loss: 0.043\n",
      "step: 2653 loss: 0.049\n",
      "step: 2654 loss: 0.041\n",
      "step: 2655 loss: 0.062\n",
      "step: 2656 loss: 0.043\n",
      "step: 2657 loss: 0.032\n",
      "step: 2658 loss: 0.060\n",
      "step: 2659 loss: 0.048\n",
      "step: 2660 loss: 0.049\n",
      "step: 2661 loss: 0.035\n",
      "step: 2662 loss: 0.049\n",
      "step: 2663 loss: 0.051\n",
      "step: 2664 loss: 0.033\n",
      "step: 2665 loss: 0.036\n",
      "step: 2666 loss: 0.076\n",
      "step: 2667 loss: 0.050\n",
      "step: 2668 loss: 0.049\n",
      "step: 2669 loss: 0.066\n",
      "step: 2670 loss: 0.070\n",
      "step: 2671 loss: 0.042\n",
      "step: 2672 loss: 0.040\n",
      "step: 2673 loss: 0.057\n",
      "step: 2674 loss: 0.046\n",
      "step: 2675 loss: 0.055\n",
      "step: 2676 loss: 0.054\n",
      "step: 2677 loss: 0.057\n",
      "step: 2678 loss: 0.054\n",
      "step: 2679 loss: 0.053\n",
      "step: 2680 loss: 0.071\n",
      "step: 2681 loss: 0.044\n",
      "step: 2682 loss: 0.034\n",
      "step: 2683 loss: 0.060\n",
      "step: 2684 loss: 0.033\n",
      "step: 2685 loss: 0.033\n",
      "step: 2686 loss: 0.072\n",
      "step: 2687 loss: 0.034\n",
      "step: 2688 loss: 0.030\n",
      "step: 2689 loss: 0.064\n",
      "step: 2690 loss: 0.035\n",
      "step: 2691 loss: 0.031\n",
      "step: 2692 loss: 0.046\n",
      "step: 2693 loss: 0.047\n",
      "step: 2694 loss: 0.040\n",
      "step: 2695 loss: 0.047\n",
      "step: 2696 loss: 0.058\n",
      "step: 2697 loss: 0.041\n",
      "step: 2698 loss: 0.051\n",
      "step: 2699 loss: 0.041\n",
      "step: 2700 loss: 0.048\n",
      "step: 2701 loss: 0.060\n",
      "step: 2702 loss: 0.035\n",
      "step: 2703 loss: 0.042\n",
      "step: 2704 loss: 0.044\n",
      "step: 2705 loss: 0.032\n",
      "step: 2706 loss: 0.038\n",
      "step: 2707 loss: 0.039\n",
      "step: 2708 loss: 0.043\n",
      "step: 2709 loss: 0.040\n",
      "step: 2710 loss: 0.045\n",
      "step: 2711 loss: 0.045\n",
      "step: 2712 loss: 0.035\n",
      "step: 2713 loss: 0.048\n",
      "step: 2714 loss: 0.048\n",
      "step: 2715 loss: 0.049\n",
      "step: 2716 loss: 0.039\n",
      "step: 2717 loss: 0.034\n",
      "step: 2718 loss: 0.038\n",
      "step: 2719 loss: 0.027\n",
      "step: 2720 loss: 0.048\n",
      "step: 2721 loss: 0.038\n",
      "step: 2722 loss: 0.044\n",
      "step: 2723 loss: 0.043\n",
      "step: 2724 loss: 0.031\n",
      "step: 2725 loss: 0.041\n",
      "step: 2726 loss: 0.045\n",
      "step: 2727 loss: 0.049\n",
      "step: 2728 loss: 0.056\n",
      "step: 2729 loss: 0.033\n",
      "step: 2730 loss: 0.032\n",
      "step: 2731 loss: 0.036\n",
      "step: 2732 loss: 0.053\n",
      "step: 2733 loss: 0.035\n",
      "step: 2734 loss: 0.049\n",
      "step: 2735 loss: 0.034\n",
      "step: 2736 loss: 0.039\n",
      "step: 2737 loss: 0.047\n",
      "step: 2738 loss: 0.045\n",
      "step: 2739 loss: 0.046\n",
      "step: 2740 loss: 0.031\n",
      "step: 2741 loss: 0.074\n",
      "step: 2742 loss: 0.039\n",
      "step: 2743 loss: 0.033\n",
      "step: 2744 loss: 0.041\n",
      "step: 2745 loss: 0.057\n",
      "step: 2746 loss: 0.024\n",
      "step: 2747 loss: 0.042\n",
      "step: 2748 loss: 0.040\n",
      "step: 2749 loss: 0.034\n",
      "step: 2750 loss: 0.033\n",
      "step: 2751 loss: 0.046\n",
      "step: 2752 loss: 0.054\n",
      "step: 2753 loss: 0.031\n",
      "step: 2754 loss: 0.050\n",
      "step: 2755 loss: 0.033\n",
      "step: 2756 loss: 0.034\n",
      "step: 2757 loss: 0.042\n",
      "step: 2758 loss: 0.035\n",
      "step: 2759 loss: 0.061\n",
      "step: 2760 loss: 0.043\n",
      "step: 2761 loss: 0.033\n",
      "step: 2762 loss: 0.044\n",
      "step: 2763 loss: 0.031\n",
      "step: 2764 loss: 0.043\n",
      "step: 2765 loss: 0.039\n",
      "step: 2766 loss: 0.050\n",
      "step: 2767 loss: 0.041\n",
      "step: 2768 loss: 0.042\n",
      "step: 2769 loss: 0.031\n",
      "step: 2770 loss: 0.059\n",
      "step: 2771 loss: 0.048\n",
      "step: 2772 loss: 0.038\n",
      "step: 2773 loss: 0.038\n",
      "step: 2774 loss: 0.032\n",
      "step: 2775 loss: 0.052\n",
      "step: 2776 loss: 0.040\n",
      "step: 2777 loss: 0.033\n",
      "step: 2778 loss: 0.042\n",
      "step: 2779 loss: 0.042\n",
      "step: 2780 loss: 0.046\n",
      "step: 2781 loss: 0.061\n",
      "step: 2782 loss: 0.044\n",
      "step: 2783 loss: 0.022\n",
      "step: 2784 loss: 0.048\n",
      "step: 2785 loss: 0.046\n",
      "step: 2786 loss: 0.033\n",
      "step: 2787 loss: 0.045\n",
      "step: 2788 loss: 0.048\n",
      "step: 2789 loss: 0.028\n",
      "step: 2790 loss: 0.029\n",
      "step: 2791 loss: 0.039\n",
      "step: 2792 loss: 0.043\n",
      "step: 2793 loss: 0.036\n",
      "step: 2794 loss: 0.040\n",
      "step: 2795 loss: 0.036\n",
      "step: 2796 loss: 0.045\n",
      "step: 2797 loss: 0.046\n",
      "step: 2798 loss: 0.046\n",
      "step: 2799 loss: 0.057\n",
      "step: 2800 loss: 0.030\n",
      "step: 2801 loss: 0.050\n",
      "step: 2802 loss: 0.037\n",
      "step: 2803 loss: 0.034\n",
      "step: 2804 loss: 0.041\n",
      "step: 2805 loss: 0.056\n",
      "step: 2806 loss: 0.021\n",
      "step: 2807 loss: 0.038\n",
      "step: 2808 loss: 0.059\n",
      "step: 2809 loss: 0.023\n",
      "step: 2810 loss: 0.039\n",
      "step: 2811 loss: 0.052\n",
      "step: 2812 loss: 0.061\n",
      "step: 2813 loss: 0.031\n",
      "step: 2814 loss: 0.026\n",
      "step: 2815 loss: 0.025\n",
      "step: 2816 loss: 0.056\n",
      "step: 2817 loss: 0.041\n",
      "step: 2818 loss: 0.039\n",
      "step: 2819 loss: 0.046\n",
      "step: 2820 loss: 0.042\n",
      "step: 2821 loss: 0.024\n",
      "step: 2822 loss: 0.043\n",
      "step: 2823 loss: 0.048\n",
      "step: 2824 loss: 0.053\n",
      "step: 2825 loss: 0.051\n",
      "step: 2826 loss: 0.038\n",
      "step: 2827 loss: 0.061\n",
      "step: 2828 loss: 0.043\n",
      "step: 2829 loss: 0.031\n",
      "step: 2830 loss: 0.043\n",
      "step: 2831 loss: 0.045\n",
      "step: 2832 loss: 0.042\n",
      "step: 2833 loss: 0.045\n",
      "step: 2834 loss: 0.040\n",
      "step: 2835 loss: 0.036\n",
      "step: 2836 loss: 0.026\n",
      "step: 2837 loss: 0.038\n",
      "step: 2838 loss: 0.034\n",
      "step: 2839 loss: 0.033\n",
      "step: 2840 loss: 0.051\n",
      "step: 2841 loss: 0.044\n",
      "step: 2842 loss: 0.042\n",
      "step: 2843 loss: 0.053\n",
      "step: 2844 loss: 0.052\n",
      "step: 2845 loss: 0.045\n",
      "step: 2846 loss: 0.039\n",
      "step: 2847 loss: 0.041\n",
      "step: 2848 loss: 0.041\n",
      "step: 2849 loss: 0.050\n",
      "step: 2850 loss: 0.037\n",
      "step: 2851 loss: 0.034\n",
      "step: 2852 loss: 0.038\n",
      "step: 2853 loss: 0.052\n",
      "step: 2854 loss: 0.039\n",
      "step: 2855 loss: 0.047\n",
      "step: 2856 loss: 0.028\n",
      "step: 2857 loss: 0.039\n",
      "step: 2858 loss: 0.042\n",
      "step: 2859 loss: 0.045\n",
      "step: 2860 loss: 0.035\n",
      "step: 2861 loss: 0.041\n",
      "step: 2862 loss: 0.037\n",
      "step: 2863 loss: 0.026\n",
      "step: 2864 loss: 0.041\n",
      "step: 2865 loss: 0.031\n",
      "step: 2866 loss: 0.035\n",
      "step: 2867 loss: 0.046\n",
      "step: 2868 loss: 0.033\n",
      "step: 2869 loss: 0.028\n",
      "step: 2870 loss: 0.048\n",
      "step: 2871 loss: 0.036\n",
      "step: 2872 loss: 0.033\n",
      "step: 2873 loss: 0.028\n",
      "step: 2874 loss: 0.056\n",
      "step: 2875 loss: 0.025\n",
      "step: 2876 loss: 0.046\n",
      "step: 2877 loss: 0.035\n",
      "step: 2878 loss: 0.039\n",
      "step: 2879 loss: 0.043\n",
      "step: 2880 loss: 0.054\n",
      "step: 2881 loss: 0.028\n",
      "step: 2882 loss: 0.036\n",
      "step: 2883 loss: 0.033\n",
      "step: 2884 loss: 0.048\n",
      "step: 2885 loss: 0.040\n",
      "step: 2886 loss: 0.038\n",
      "step: 2887 loss: 0.038\n",
      "step: 2888 loss: 0.051\n",
      "step: 2889 loss: 0.029\n",
      "step: 2890 loss: 0.034\n",
      "step: 2891 loss: 0.042\n",
      "step: 2892 loss: 0.035\n",
      "step: 2893 loss: 0.052\n",
      "step: 2894 loss: 0.034\n",
      "step: 2895 loss: 0.033\n",
      "step: 2896 loss: 0.063\n",
      "step: 2897 loss: 0.043\n",
      "step: 2898 loss: 0.029\n",
      "step: 2899 loss: 0.048\n",
      "step: 2900 loss: 0.032\n",
      "step: 2901 loss: 0.028\n",
      "step: 2902 loss: 0.047\n",
      "step: 2903 loss: 0.048\n",
      "step: 2904 loss: 0.033\n",
      "step: 2905 loss: 0.043\n",
      "step: 2906 loss: 0.028\n",
      "step: 2907 loss: 0.052\n",
      "step: 2908 loss: 0.038\n",
      "step: 2909 loss: 0.045\n",
      "step: 2910 loss: 0.050\n",
      "step: 2911 loss: 0.054\n",
      "step: 2912 loss: 0.055\n",
      "step: 2913 loss: 0.044\n",
      "step: 2914 loss: 0.040\n",
      "step: 2915 loss: 0.035\n",
      "step: 2916 loss: 0.044\n",
      "step: 2917 loss: 0.045\n",
      "step: 2918 loss: 0.031\n",
      "step: 2919 loss: 0.030\n",
      "step: 2920 loss: 0.055\n",
      "step: 2921 loss: 0.050\n",
      "step: 2922 loss: 0.038\n",
      "step: 2923 loss: 0.027\n",
      "step: 2924 loss: 0.057\n",
      "step: 2925 loss: 0.030\n",
      "step: 2926 loss: 0.050\n",
      "step: 2927 loss: 0.051\n",
      "step: 2928 loss: 0.043\n",
      "step: 2929 loss: 0.039\n",
      "step: 2930 loss: 0.036\n",
      "step: 2931 loss: 0.056\n",
      "step: 2932 loss: 0.030\n",
      "step: 2933 loss: 0.039\n",
      "step: 2934 loss: 0.044\n",
      "step: 2935 loss: 0.046\n",
      "step: 2936 loss: 0.032\n",
      "step: 2937 loss: 0.052\n",
      "step: 2938 loss: 0.036\n",
      "step: 2939 loss: 0.060\n",
      "step: 2940 loss: 0.035\n",
      "step: 2941 loss: 0.038\n",
      "step: 2942 loss: 0.048\n",
      "step: 2943 loss: 0.053\n",
      "step: 2944 loss: 0.046\n",
      "step: 2945 loss: 0.028\n",
      "step: 2946 loss: 0.048\n",
      "step: 2947 loss: 0.036\n",
      "step: 2948 loss: 0.054\n",
      "step: 2949 loss: 0.042\n",
      "step: 2950 loss: 0.042\n",
      "step: 2951 loss: 0.039\n",
      "step: 2952 loss: 0.062\n",
      "step: 2953 loss: 0.049\n",
      "step: 2954 loss: 0.042\n",
      "step: 2955 loss: 0.029\n",
      "step: 2956 loss: 0.037\n",
      "step: 2957 loss: 0.045\n",
      "step: 2958 loss: 0.058\n",
      "step: 2959 loss: 0.056\n",
      "step: 2960 loss: 0.037\n",
      "step: 2961 loss: 0.033\n",
      "step: 2962 loss: 0.028\n",
      "step: 2963 loss: 0.030\n",
      "step: 2964 loss: 0.046\n",
      "step: 2965 loss: 0.051\n",
      "step: 2966 loss: 0.034\n",
      "step: 2967 loss: 0.044\n",
      "step: 2968 loss: 0.044\n",
      "step: 2969 loss: 0.054\n",
      "step: 2970 loss: 0.054\n",
      "step: 2971 loss: 0.046\n",
      "step: 2972 loss: 0.060\n",
      "step: 2973 loss: 0.045\n",
      "step: 2974 loss: 0.046\n",
      "step: 2975 loss: 0.059\n",
      "step: 2976 loss: 0.030\n",
      "step: 2977 loss: 0.047\n",
      "step: 2978 loss: 0.030\n",
      "step: 2979 loss: 0.060\n",
      "step: 2980 loss: 0.039\n",
      "step: 2981 loss: 0.043\n",
      "step: 2982 loss: 0.036\n",
      "step: 2983 loss: 0.035\n",
      "step: 2984 loss: 0.041\n",
      "step: 2985 loss: 0.047\n",
      "step: 2986 loss: 0.031\n",
      "step: 2987 loss: 0.039\n",
      "step: 2988 loss: 0.043\n",
      "step: 2989 loss: 0.060\n",
      "step: 2990 loss: 0.058\n",
      "step: 2991 loss: 0.039\n",
      "step: 2992 loss: 0.030\n",
      "step: 2993 loss: 0.051\n",
      "step: 2994 loss: 0.064\n",
      "step: 2995 loss: 0.047\n",
      "step: 2996 loss: 0.040\n",
      "step: 2997 loss: 0.053\n",
      "step: 2998 loss: 0.051\n",
      "step: 2999 loss: 0.034\n",
      "step: 3000 loss: 0.030\n",
      "step: 3001 loss: 0.056\n",
      "step: 3002 loss: 0.048\n",
      "step: 3003 loss: 0.047\n",
      "step: 3004 loss: 0.074\n",
      "step: 3005 loss: 0.047\n",
      "step: 3006 loss: 0.025\n",
      "step: 3007 loss: 0.049\n",
      "step: 3008 loss: 0.052\n",
      "step: 3009 loss: 0.043\n",
      "step: 3010 loss: 0.025\n",
      "step: 3011 loss: 0.044\n",
      "step: 3012 loss: 0.047\n",
      "step: 3013 loss: 0.038\n",
      "step: 3014 loss: 0.044\n",
      "step: 3015 loss: 0.040\n",
      "step: 3016 loss: 0.045\n",
      "step: 3017 loss: 0.040\n",
      "step: 3018 loss: 0.051\n",
      "step: 3019 loss: 0.049\n",
      "step: 3020 loss: 0.046\n",
      "step: 3021 loss: 0.054\n",
      "step: 3022 loss: 0.044\n",
      "step: 3023 loss: 0.047\n",
      "step: 3024 loss: 0.034\n",
      "step: 3025 loss: 0.039\n",
      "step: 3026 loss: 0.045\n",
      "step: 3027 loss: 0.052\n",
      "step: 3028 loss: 0.034\n",
      "step: 3029 loss: 0.038\n",
      "step: 3030 loss: 0.045\n",
      "step: 3031 loss: 0.039\n",
      "step: 3032 loss: 0.036\n",
      "step: 3033 loss: 0.042\n",
      "step: 3034 loss: 0.055\n",
      "step: 3035 loss: 0.053\n",
      "step: 3036 loss: 0.034\n",
      "step: 3037 loss: 0.026\n",
      "step: 3038 loss: 0.047\n",
      "step: 3039 loss: 0.036\n",
      "step: 3040 loss: 0.045\n",
      "step: 3041 loss: 0.066\n",
      "step: 3042 loss: 0.028\n",
      "step: 3043 loss: 0.066\n",
      "step: 3044 loss: 0.043\n",
      "step: 3045 loss: 0.041\n",
      "step: 3046 loss: 0.044\n",
      "step: 3047 loss: 0.031\n",
      "step: 3048 loss: 0.044\n",
      "step: 3049 loss: 0.077\n",
      "step: 3050 loss: 0.038\n",
      "step: 3051 loss: 0.048\n",
      "step: 3052 loss: 0.042\n",
      "step: 3053 loss: 0.047\n",
      "step: 3054 loss: 0.041\n",
      "step: 3055 loss: 0.031\n",
      "step: 3056 loss: 0.037\n",
      "step: 3057 loss: 0.037\n",
      "step: 3058 loss: 0.031\n",
      "step: 3059 loss: 0.054\n",
      "step: 3060 loss: 0.046\n",
      "step: 3061 loss: 0.034\n",
      "step: 3062 loss: 0.053\n",
      "step: 3063 loss: 0.025\n",
      "step: 3064 loss: 0.038\n",
      "step: 3065 loss: 0.040\n",
      "step: 3066 loss: 0.036\n",
      "step: 3067 loss: 0.041\n",
      "step: 3068 loss: 0.037\n",
      "step: 3069 loss: 0.044\n",
      "step: 3070 loss: 0.036\n",
      "step: 3071 loss: 0.027\n",
      "step: 3072 loss: 0.030\n",
      "step: 3073 loss: 0.024\n",
      "step: 3074 loss: 0.044\n",
      "step: 3075 loss: 0.040\n",
      "step: 3076 loss: 0.057\n",
      "step: 3077 loss: 0.044\n",
      "step: 3078 loss: 0.042\n",
      "step: 3079 loss: 0.042\n",
      "step: 3080 loss: 0.039\n",
      "step: 3081 loss: 0.050\n",
      "step: 3082 loss: 0.043\n",
      "step: 3083 loss: 0.040\n",
      "step: 3084 loss: 0.032\n",
      "step: 3085 loss: 0.054\n",
      "step: 3086 loss: 0.029\n",
      "step: 3087 loss: 0.042\n",
      "step: 3088 loss: 0.047\n",
      "step: 3089 loss: 0.041\n",
      "step: 3090 loss: 0.042\n",
      "step: 3091 loss: 0.051\n",
      "step: 3092 loss: 0.052\n",
      "step: 3093 loss: 0.051\n",
      "step: 3094 loss: 0.053\n",
      "step: 3095 loss: 0.041\n",
      "step: 3096 loss: 0.032\n",
      "step: 3097 loss: 0.039\n",
      "step: 3098 loss: 0.050\n",
      "step: 3099 loss: 0.038\n",
      "step: 3100 loss: 0.028\n",
      "step: 3101 loss: 0.025\n",
      "step: 3102 loss: 0.043\n",
      "step: 3103 loss: 0.032\n",
      "step: 3104 loss: 0.040\n",
      "step: 3105 loss: 0.035\n",
      "step: 3106 loss: 0.045\n",
      "step: 3107 loss: 0.049\n",
      "step: 3108 loss: 0.029\n",
      "step: 3109 loss: 0.023\n",
      "step: 3110 loss: 0.056\n",
      "step: 3111 loss: 0.030\n",
      "step: 3112 loss: 0.049\n",
      "step: 3113 loss: 0.059\n",
      "step: 3114 loss: 0.037\n",
      "step: 3115 loss: 0.045\n",
      "step: 3116 loss: 0.042\n",
      "step: 3117 loss: 0.041\n",
      "step: 3118 loss: 0.038\n",
      "step: 3119 loss: 0.042\n",
      "step: 3120 loss: 0.045\n",
      "step: 3121 loss: 0.052\n",
      "step: 3122 loss: 0.049\n",
      "step: 3123 loss: 0.047\n",
      "step: 3124 loss: 0.028\n",
      "step: 3125 loss: 0.047\n",
      "step: 3126 loss: 0.030\n",
      "step: 3127 loss: 0.046\n",
      "step: 3128 loss: 0.037\n",
      "step: 3129 loss: 0.039\n",
      "step: 3130 loss: 0.023\n",
      "step: 3131 loss: 0.026\n",
      "step: 3132 loss: 0.048\n",
      "step: 3133 loss: 0.042\n",
      "step: 3134 loss: 0.045\n",
      "step: 3135 loss: 0.034\n",
      "step: 3136 loss: 0.040\n",
      "step: 3137 loss: 0.039\n",
      "step: 3138 loss: 0.039\n",
      "step: 3139 loss: 0.034\n",
      "step: 3140 loss: 0.029\n",
      "step: 3141 loss: 0.042\n",
      "step: 3142 loss: 0.027\n",
      "step: 3143 loss: 0.029\n",
      "step: 3144 loss: 0.030\n",
      "step: 3145 loss: 0.057\n",
      "step: 3146 loss: 0.032\n",
      "step: 3147 loss: 0.058\n",
      "step: 3148 loss: 0.051\n",
      "step: 3149 loss: 0.030\n",
      "step: 3150 loss: 0.041\n",
      "step: 3151 loss: 0.045\n",
      "step: 3152 loss: 0.055\n",
      "step: 3153 loss: 0.033\n",
      "step: 3154 loss: 0.043\n",
      "step: 3155 loss: 0.039\n",
      "step: 3156 loss: 0.043\n",
      "step: 3157 loss: 0.046\n",
      "step: 3158 loss: 0.038\n",
      "step: 3159 loss: 0.043\n",
      "step: 3160 loss: 0.051\n",
      "step: 3161 loss: 0.049\n",
      "step: 3162 loss: 0.040\n",
      "step: 3163 loss: 0.041\n",
      "step: 3164 loss: 0.046\n",
      "step: 3165 loss: 0.031\n",
      "step: 3166 loss: 0.029\n",
      "step: 3167 loss: 0.038\n",
      "step: 3168 loss: 0.029\n",
      "step: 3169 loss: 0.027\n",
      "step: 3170 loss: 0.058\n",
      "step: 3171 loss: 0.055\n",
      "step: 3172 loss: 0.046\n",
      "step: 3173 loss: 0.058\n",
      "step: 3174 loss: 0.046\n",
      "step: 3175 loss: 0.036\n",
      "step: 3176 loss: 0.038\n",
      "step: 3177 loss: 0.054\n",
      "step: 3178 loss: 0.029\n",
      "step: 3179 loss: 0.050\n",
      "step: 3180 loss: 0.037\n",
      "step: 3181 loss: 0.045\n",
      "step: 3182 loss: 0.037\n",
      "step: 3183 loss: 0.045\n",
      "step: 3184 loss: 0.044\n",
      "step: 3185 loss: 0.034\n",
      "step: 3186 loss: 0.039\n",
      "step: 3187 loss: 0.044\n",
      "step: 3188 loss: 0.055\n",
      "step: 3189 loss: 0.041\n",
      "step: 3190 loss: 0.043\n",
      "step: 3191 loss: 0.049\n",
      "step: 3192 loss: 0.054\n",
      "step: 3193 loss: 0.042\n",
      "step: 3194 loss: 0.040\n",
      "step: 3195 loss: 0.049\n",
      "step: 3196 loss: 0.031\n",
      "step: 3197 loss: 0.030\n",
      "step: 3198 loss: 0.033\n",
      "step: 3199 loss: 0.035\n",
      "step: 3200 loss: 0.029\n",
      "step: 3201 loss: 0.066\n",
      "step: 3202 loss: 0.072\n",
      "step: 3203 loss: 0.042\n",
      "step: 3204 loss: 0.032\n",
      "step: 3205 loss: 0.035\n",
      "step: 3206 loss: 0.035\n",
      "step: 3207 loss: 0.036\n",
      "step: 3208 loss: 0.040\n",
      "step: 3209 loss: 0.060\n",
      "step: 3210 loss: 0.034\n",
      "step: 3211 loss: 0.034\n",
      "step: 3212 loss: 0.037\n",
      "step: 3213 loss: 0.047\n",
      "step: 3214 loss: 0.041\n",
      "step: 3215 loss: 0.035\n",
      "step: 3216 loss: 0.059\n",
      "step: 3217 loss: 0.054\n",
      "step: 3218 loss: 0.039\n",
      "step: 3219 loss: 0.030\n",
      "step: 3220 loss: 0.031\n",
      "step: 3221 loss: 0.028\n",
      "step: 3222 loss: 0.055\n",
      "step: 3223 loss: 0.034\n",
      "step: 3224 loss: 0.040\n",
      "step: 3225 loss: 0.037\n",
      "step: 3226 loss: 0.045\n",
      "step: 3227 loss: 0.041\n",
      "step: 3228 loss: 0.039\n",
      "step: 3229 loss: 0.060\n",
      "step: 3230 loss: 0.032\n",
      "step: 3231 loss: 0.046\n",
      "step: 3232 loss: 0.042\n",
      "step: 3233 loss: 0.039\n",
      "step: 3234 loss: 0.044\n",
      "step: 3235 loss: 0.026\n",
      "step: 3236 loss: 0.028\n",
      "step: 3237 loss: 0.030\n",
      "step: 3238 loss: 0.051\n",
      "step: 3239 loss: 0.038\n",
      "step: 3240 loss: 0.055\n",
      "step: 3241 loss: 0.032\n",
      "step: 3242 loss: 0.052\n",
      "step: 3243 loss: 0.052\n",
      "step: 3244 loss: 0.031\n",
      "step: 3245 loss: 0.028\n",
      "step: 3246 loss: 0.060\n",
      "step: 3247 loss: 0.045\n",
      "step: 3248 loss: 0.028\n",
      "step: 3249 loss: 0.045\n",
      "step: 3250 loss: 0.040\n",
      "step: 3251 loss: 0.046\n",
      "step: 3252 loss: 0.038\n",
      "step: 3253 loss: 0.039\n",
      "step: 3254 loss: 0.050\n",
      "step: 3255 loss: 0.042\n",
      "step: 3256 loss: 0.034\n",
      "step: 3257 loss: 0.033\n",
      "step: 3258 loss: 0.048\n",
      "step: 3259 loss: 0.024\n",
      "step: 3260 loss: 0.030\n",
      "step: 3261 loss: 0.045\n",
      "step: 3262 loss: 0.041\n",
      "step: 3263 loss: 0.034\n",
      "step: 3264 loss: 0.037\n",
      "step: 3265 loss: 0.032\n",
      "step: 3266 loss: 0.060\n",
      "step: 3267 loss: 0.022\n",
      "step: 3268 loss: 0.041\n",
      "step: 3269 loss: 0.023\n",
      "step: 3270 loss: 0.049\n",
      "step: 3271 loss: 0.041\n",
      "step: 3272 loss: 0.038\n",
      "step: 3273 loss: 0.036\n",
      "step: 3274 loss: 0.038\n",
      "step: 3275 loss: 0.049\n",
      "step: 3276 loss: 0.030\n",
      "step: 3277 loss: 0.049\n",
      "step: 3278 loss: 0.029\n",
      "step: 3279 loss: 0.038\n",
      "step: 3280 loss: 0.041\n",
      "step: 3281 loss: 0.034\n",
      "step: 3282 loss: 0.044\n",
      "step: 3283 loss: 0.038\n",
      "step: 3284 loss: 0.052\n",
      "step: 3285 loss: 0.047\n",
      "step: 3286 loss: 0.060\n",
      "step: 3287 loss: 0.035\n",
      "step: 3288 loss: 0.061\n",
      "step: 3289 loss: 0.031\n",
      "step: 3290 loss: 0.035\n",
      "step: 3291 loss: 0.045\n",
      "step: 3292 loss: 0.043\n",
      "step: 3293 loss: 0.039\n",
      "step: 3294 loss: 0.033\n",
      "step: 3295 loss: 0.031\n",
      "step: 3296 loss: 0.035\n",
      "step: 3297 loss: 0.026\n",
      "step: 3298 loss: 0.026\n",
      "step: 3299 loss: 0.051\n",
      "step: 3300 loss: 0.031\n",
      "step: 3301 loss: 0.026\n",
      "step: 3302 loss: 0.046\n",
      "step: 3303 loss: 0.061\n",
      "step: 3304 loss: 0.035\n",
      "step: 3305 loss: 0.048\n",
      "step: 3306 loss: 0.031\n",
      "step: 3307 loss: 0.040\n",
      "step: 3308 loss: 0.040\n",
      "step: 3309 loss: 0.057\n",
      "step: 3310 loss: 0.054\n",
      "step: 3311 loss: 0.046\n",
      "step: 3312 loss: 0.031\n",
      "step: 3313 loss: 0.034\n",
      "step: 3314 loss: 0.032\n",
      "step: 3315 loss: 0.049\n",
      "step: 3316 loss: 0.044\n",
      "step: 3317 loss: 0.034\n",
      "step: 3318 loss: 0.033\n",
      "step: 3319 loss: 0.044\n",
      "step: 3320 loss: 0.038\n",
      "step: 3321 loss: 0.036\n",
      "step: 3322 loss: 0.036\n",
      "step: 3323 loss: 0.043\n",
      "step: 3324 loss: 0.036\n",
      "step: 3325 loss: 0.053\n",
      "step: 3326 loss: 0.037\n",
      "step: 3327 loss: 0.043\n",
      "step: 3328 loss: 0.015\n",
      "step: 3329 loss: 0.039\n",
      "step: 3330 loss: 0.046\n",
      "step: 3331 loss: 0.045\n",
      "step: 3332 loss: 0.047\n",
      "step: 3333 loss: 0.038\n",
      "step: 3334 loss: 0.031\n",
      "step: 3335 loss: 0.042\n",
      "step: 3336 loss: 0.030\n",
      "step: 3337 loss: 0.041\n",
      "step: 3338 loss: 0.040\n",
      "step: 3339 loss: 0.036\n",
      "step: 3340 loss: 0.059\n",
      "step: 3341 loss: 0.035\n",
      "step: 3342 loss: 0.050\n",
      "step: 3343 loss: 0.038\n",
      "step: 3344 loss: 0.052\n",
      "step: 3345 loss: 0.043\n",
      "step: 3346 loss: 0.027\n",
      "step: 3347 loss: 0.027\n",
      "step: 3348 loss: 0.028\n",
      "step: 3349 loss: 0.041\n",
      "step: 3350 loss: 0.045\n",
      "step: 3351 loss: 0.049\n",
      "step: 3352 loss: 0.036\n",
      "step: 3353 loss: 0.028\n",
      "step: 3354 loss: 0.052\n",
      "step: 3355 loss: 0.028\n",
      "step: 3356 loss: 0.037\n",
      "step: 3357 loss: 0.032\n",
      "step: 3358 loss: 0.044\n",
      "step: 3359 loss: 0.033\n",
      "step: 3360 loss: 0.030\n",
      "step: 3361 loss: 0.020\n",
      "step: 3362 loss: 0.038\n",
      "step: 3363 loss: 0.063\n",
      "step: 3364 loss: 0.042\n",
      "step: 3365 loss: 0.023\n",
      "step: 3366 loss: 0.044\n",
      "step: 3367 loss: 0.042\n",
      "step: 3368 loss: 0.056\n",
      "step: 3369 loss: 0.046\n",
      "step: 3370 loss: 0.049\n",
      "step: 3371 loss: 0.034\n",
      "step: 3372 loss: 0.044\n",
      "step: 3373 loss: 0.028\n",
      "step: 3374 loss: 0.027\n",
      "step: 3375 loss: 0.044\n",
      "step: 3376 loss: 0.035\n",
      "step: 3377 loss: 0.051\n",
      "step: 3378 loss: 0.047\n",
      "step: 3379 loss: 0.038\n",
      "step: 3380 loss: 0.040\n",
      "step: 3381 loss: 0.028\n",
      "step: 3382 loss: 0.035\n",
      "step: 3383 loss: 0.041\n",
      "step: 3384 loss: 0.048\n",
      "step: 3385 loss: 0.034\n",
      "step: 3386 loss: 0.031\n",
      "step: 3387 loss: 0.030\n",
      "step: 3388 loss: 0.029\n",
      "step: 3389 loss: 0.038\n",
      "step: 3390 loss: 0.027\n",
      "step: 3391 loss: 0.037\n",
      "step: 3392 loss: 0.053\n",
      "step: 3393 loss: 0.052\n",
      "step: 3394 loss: 0.031\n",
      "step: 3395 loss: 0.041\n",
      "step: 3396 loss: 0.037\n",
      "step: 3397 loss: 0.051\n",
      "step: 3398 loss: 0.063\n",
      "step: 3399 loss: 0.054\n",
      "step: 3400 loss: 0.041\n",
      "step: 3401 loss: 0.046\n",
      "step: 3402 loss: 0.032\n",
      "step: 3403 loss: 0.040\n",
      "step: 3404 loss: 0.037\n",
      "step: 3405 loss: 0.041\n",
      "step: 3406 loss: 0.036\n",
      "step: 3407 loss: 0.034\n",
      "step: 3408 loss: 0.044\n",
      "step: 3409 loss: 0.045\n",
      "step: 3410 loss: 0.037\n",
      "step: 3411 loss: 0.045\n",
      "step: 3412 loss: 0.048\n",
      "step: 3413 loss: 0.038\n",
      "step: 3414 loss: 0.036\n",
      "step: 3415 loss: 0.040\n",
      "step: 3416 loss: 0.041\n",
      "step: 3417 loss: 0.046\n",
      "step: 3418 loss: 0.039\n",
      "step: 3419 loss: 0.036\n",
      "step: 3420 loss: 0.030\n",
      "step: 3421 loss: 0.038\n",
      "step: 3422 loss: 0.043\n",
      "step: 3423 loss: 0.031\n",
      "step: 3424 loss: 0.035\n",
      "step: 3425 loss: 0.035\n",
      "step: 3426 loss: 0.037\n",
      "step: 3427 loss: 0.043\n",
      "step: 3428 loss: 0.040\n",
      "step: 3429 loss: 0.041\n",
      "step: 3430 loss: 0.049\n",
      "step: 3431 loss: 0.033\n",
      "step: 3432 loss: 0.033\n",
      "step: 3433 loss: 0.036\n",
      "step: 3434 loss: 0.026\n",
      "step: 3435 loss: 0.043\n",
      "step: 3436 loss: 0.057\n",
      "step: 3437 loss: 0.054\n",
      "step: 3438 loss: 0.056\n",
      "step: 3439 loss: 0.046\n",
      "step: 3440 loss: 0.018\n",
      "step: 3441 loss: 0.045\n",
      "step: 3442 loss: 0.037\n",
      "step: 3443 loss: 0.038\n",
      "step: 3444 loss: 0.041\n",
      "step: 3445 loss: 0.043\n",
      "step: 3446 loss: 0.050\n",
      "step: 3447 loss: 0.032\n",
      "step: 3448 loss: 0.042\n",
      "step: 3449 loss: 0.029\n",
      "step: 3450 loss: 0.039\n",
      "step: 3451 loss: 0.047\n",
      "step: 3452 loss: 0.055\n",
      "step: 3453 loss: 0.033\n",
      "step: 3454 loss: 0.026\n",
      "step: 3455 loss: 0.034\n",
      "step: 3456 loss: 0.034\n",
      "step: 3457 loss: 0.042\n",
      "step: 3458 loss: 0.034\n",
      "step: 3459 loss: 0.047\n",
      "step: 3460 loss: 0.031\n",
      "step: 3461 loss: 0.044\n",
      "step: 3462 loss: 0.044\n",
      "step: 3463 loss: 0.061\n",
      "step: 3464 loss: 0.034\n",
      "step: 3465 loss: 0.055\n",
      "step: 3466 loss: 0.040\n",
      "step: 3467 loss: 0.043\n",
      "step: 3468 loss: 0.043\n",
      "step: 3469 loss: 0.045\n",
      "step: 3470 loss: 0.047\n",
      "step: 3471 loss: 0.060\n",
      "step: 3472 loss: 0.043\n",
      "step: 3473 loss: 0.029\n",
      "step: 3474 loss: 0.041\n",
      "step: 3475 loss: 0.062\n",
      "step: 3476 loss: 0.043\n",
      "step: 3477 loss: 0.026\n",
      "step: 3478 loss: 0.050\n",
      "step: 3479 loss: 0.061\n",
      "step: 3480 loss: 0.040\n",
      "step: 3481 loss: 0.049\n",
      "step: 3482 loss: 0.049\n",
      "step: 3483 loss: 0.040\n",
      "step: 3484 loss: 0.081\n",
      "step: 3485 loss: 0.042\n",
      "step: 3486 loss: 0.040\n",
      "step: 3487 loss: 0.050\n",
      "step: 3488 loss: 0.036\n",
      "step: 3489 loss: 0.033\n",
      "step: 3490 loss: 0.029\n",
      "step: 3491 loss: 0.055\n",
      "step: 3492 loss: 0.046\n",
      "step: 3493 loss: 0.036\n",
      "step: 3494 loss: 0.038\n",
      "step: 3495 loss: 0.036\n",
      "step: 3496 loss: 0.034\n",
      "step: 3497 loss: 0.034\n",
      "step: 3498 loss: 0.035\n",
      "step: 3499 loss: 0.029\n",
      "step: 3500 loss: 0.050\n",
      "step: 3501 loss: 0.046\n",
      "step: 3502 loss: 0.033\n",
      "step: 3503 loss: 0.051\n",
      "step: 3504 loss: 0.047\n",
      "step: 3505 loss: 0.035\n",
      "step: 3506 loss: 0.023\n",
      "step: 3507 loss: 0.027\n",
      "step: 3508 loss: 0.041\n",
      "step: 3509 loss: 0.027\n",
      "step: 3510 loss: 0.049\n",
      "step: 3511 loss: 0.022\n",
      "step: 3512 loss: 0.045\n",
      "step: 3513 loss: 0.045\n",
      "step: 3514 loss: 0.036\n",
      "step: 3515 loss: 0.048\n",
      "step: 3516 loss: 0.058\n",
      "step: 3517 loss: 0.035\n",
      "step: 3518 loss: 0.051\n",
      "step: 3519 loss: 0.026\n",
      "step: 3520 loss: 0.040\n",
      "step: 3521 loss: 0.030\n",
      "step: 3522 loss: 0.036\n",
      "step: 3523 loss: 0.027\n",
      "step: 3524 loss: 0.040\n",
      "step: 3525 loss: 0.049\n",
      "step: 3526 loss: 0.026\n",
      "step: 3527 loss: 0.047\n",
      "step: 3528 loss: 0.050\n",
      "step: 3529 loss: 0.059\n",
      "step: 3530 loss: 0.036\n",
      "step: 3531 loss: 0.034\n",
      "step: 3532 loss: 0.044\n",
      "step: 3533 loss: 0.044\n",
      "step: 3534 loss: 0.048\n",
      "step: 3535 loss: 0.053\n",
      "step: 3536 loss: 0.045\n",
      "step: 3537 loss: 0.059\n",
      "step: 3538 loss: 0.040\n",
      "step: 3539 loss: 0.032\n",
      "step: 3540 loss: 0.055\n",
      "step: 3541 loss: 0.036\n",
      "step: 3542 loss: 0.042\n",
      "step: 3543 loss: 0.059\n",
      "step: 3544 loss: 0.039\n",
      "step: 3545 loss: 0.044\n",
      "step: 3546 loss: 0.047\n",
      "step: 3547 loss: 0.056\n",
      "step: 3548 loss: 0.046\n",
      "step: 3549 loss: 0.045\n",
      "step: 3550 loss: 0.045\n",
      "step: 3551 loss: 0.045\n",
      "step: 3552 loss: 0.044\n",
      "step: 3553 loss: 0.028\n",
      "step: 3554 loss: 0.029\n",
      "step: 3555 loss: 0.049\n",
      "step: 3556 loss: 0.044\n",
      "step: 3557 loss: 0.056\n",
      "step: 3558 loss: 0.032\n",
      "step: 3559 loss: 0.037\n",
      "step: 3560 loss: 0.034\n",
      "step: 3561 loss: 0.035\n",
      "step: 3562 loss: 0.036\n",
      "step: 3563 loss: 0.019\n",
      "step: 3564 loss: 0.043\n",
      "step: 3565 loss: 0.030\n",
      "step: 3566 loss: 0.049\n",
      "step: 3567 loss: 0.037\n",
      "step: 3568 loss: 0.053\n",
      "step: 3569 loss: 0.048\n",
      "step: 3570 loss: 0.049\n",
      "step: 3571 loss: 0.044\n",
      "step: 3572 loss: 0.039\n",
      "step: 3573 loss: 0.033\n",
      "step: 3574 loss: 0.024\n",
      "step: 3575 loss: 0.041\n",
      "step: 3576 loss: 0.060\n",
      "step: 3577 loss: 0.038\n",
      "step: 3578 loss: 0.033\n",
      "step: 3579 loss: 0.039\n",
      "step: 3580 loss: 0.070\n",
      "step: 3581 loss: 0.037\n",
      "step: 3582 loss: 0.023\n",
      "step: 3583 loss: 0.046\n",
      "step: 3584 loss: 0.038\n",
      "step: 3585 loss: 0.032\n",
      "step: 3586 loss: 0.046\n",
      "step: 3587 loss: 0.032\n",
      "step: 3588 loss: 0.046\n",
      "step: 3589 loss: 0.045\n",
      "step: 3590 loss: 0.044\n",
      "step: 3591 loss: 0.029\n",
      "step: 3592 loss: 0.045\n",
      "step: 3593 loss: 0.040\n",
      "step: 3594 loss: 0.058\n",
      "step: 3595 loss: 0.040\n",
      "step: 3596 loss: 0.043\n",
      "step: 3597 loss: 0.054\n",
      "step: 3598 loss: 0.032\n",
      "step: 3599 loss: 0.045\n",
      "step: 3600 loss: 0.044\n",
      "step: 3601 loss: 0.043\n",
      "step: 3602 loss: 0.043\n",
      "step: 3603 loss: 0.053\n",
      "step: 3604 loss: 0.037\n",
      "step: 3605 loss: 0.035\n",
      "step: 3606 loss: 0.047\n",
      "step: 3607 loss: 0.036\n",
      "step: 3608 loss: 0.043\n",
      "step: 3609 loss: 0.033\n",
      "step: 3610 loss: 0.035\n",
      "step: 3611 loss: 0.055\n",
      "step: 3612 loss: 0.046\n",
      "step: 3613 loss: 0.024\n",
      "step: 3614 loss: 0.044\n",
      "step: 3615 loss: 0.046\n",
      "step: 3616 loss: 0.042\n",
      "step: 3617 loss: 0.042\n",
      "step: 3618 loss: 0.044\n",
      "step: 3619 loss: 0.052\n",
      "step: 3620 loss: 0.051\n",
      "step: 3621 loss: 0.032\n",
      "step: 3622 loss: 0.059\n",
      "step: 3623 loss: 0.040\n",
      "step: 3624 loss: 0.028\n",
      "step: 3625 loss: 0.035\n",
      "step: 3626 loss: 0.052\n",
      "step: 3627 loss: 0.038\n",
      "step: 3628 loss: 0.030\n",
      "step: 3629 loss: 0.043\n",
      "step: 3630 loss: 0.043\n",
      "step: 3631 loss: 0.052\n",
      "step: 3632 loss: 0.028\n",
      "step: 3633 loss: 0.046\n",
      "step: 3634 loss: 0.048\n",
      "step: 3635 loss: 0.026\n",
      "step: 3636 loss: 0.059\n",
      "step: 3637 loss: 0.047\n",
      "step: 3638 loss: 0.038\n",
      "step: 3639 loss: 0.057\n",
      "step: 3640 loss: 0.043\n",
      "step: 3641 loss: 0.036\n",
      "step: 3642 loss: 0.054\n",
      "step: 3643 loss: 0.044\n",
      "step: 3644 loss: 0.039\n",
      "step: 3645 loss: 0.050\n",
      "step: 3646 loss: 0.035\n",
      "step: 3647 loss: 0.040\n",
      "step: 3648 loss: 0.030\n",
      "step: 3649 loss: 0.045\n",
      "step: 3650 loss: 0.046\n",
      "step: 3651 loss: 0.032\n",
      "step: 3652 loss: 0.041\n",
      "step: 3653 loss: 0.058\n",
      "step: 3654 loss: 0.034\n",
      "step: 3655 loss: 0.038\n",
      "step: 3656 loss: 0.046\n",
      "step: 3657 loss: 0.028\n",
      "step: 3658 loss: 0.047\n",
      "step: 3659 loss: 0.031\n",
      "step: 3660 loss: 0.029\n",
      "step: 3661 loss: 0.043\n",
      "step: 3662 loss: 0.034\n",
      "step: 3663 loss: 0.030\n",
      "step: 3664 loss: 0.032\n",
      "step: 3665 loss: 0.036\n",
      "step: 3666 loss: 0.033\n",
      "step: 3667 loss: 0.035\n",
      "step: 3668 loss: 0.051\n",
      "step: 3669 loss: 0.026\n",
      "step: 3670 loss: 0.054\n",
      "step: 3671 loss: 0.035\n",
      "step: 3672 loss: 0.040\n",
      "step: 3673 loss: 0.042\n",
      "step: 3674 loss: 0.040\n",
      "step: 3675 loss: 0.038\n",
      "step: 3676 loss: 0.046\n",
      "step: 3677 loss: 0.040\n",
      "step: 3678 loss: 0.030\n",
      "step: 3679 loss: 0.042\n",
      "step: 3680 loss: 0.043\n",
      "step: 3681 loss: 0.022\n",
      "step: 3682 loss: 0.036\n",
      "step: 3683 loss: 0.043\n",
      "step: 3684 loss: 0.054\n",
      "step: 3685 loss: 0.057\n",
      "step: 3686 loss: 0.042\n",
      "step: 3687 loss: 0.029\n",
      "step: 3688 loss: 0.055\n",
      "step: 3689 loss: 0.037\n",
      "step: 3690 loss: 0.034\n",
      "step: 3691 loss: 0.054\n",
      "step: 3692 loss: 0.025\n",
      "step: 3693 loss: 0.040\n",
      "step: 3694 loss: 0.046\n",
      "step: 3695 loss: 0.042\n",
      "step: 3696 loss: 0.026\n",
      "step: 3697 loss: 0.028\n",
      "step: 3698 loss: 0.037\n",
      "step: 3699 loss: 0.034\n",
      "step: 3700 loss: 0.023\n",
      "step: 3701 loss: 0.047\n",
      "step: 3702 loss: 0.037\n",
      "step: 3703 loss: 0.030\n",
      "step: 3704 loss: 0.046\n",
      "step: 3705 loss: 0.050\n",
      "step: 3706 loss: 0.053\n",
      "step: 3707 loss: 0.037\n",
      "step: 3708 loss: 0.036\n",
      "step: 3709 loss: 0.038\n",
      "step: 3710 loss: 0.048\n",
      "step: 3711 loss: 0.035\n",
      "step: 3712 loss: 0.041\n",
      "step: 3713 loss: 0.035\n",
      "step: 3714 loss: 0.023\n",
      "step: 3715 loss: 0.037\n",
      "step: 3716 loss: 0.037\n",
      "step: 3717 loss: 0.032\n",
      "step: 3718 loss: 0.038\n",
      "step: 3719 loss: 0.056\n",
      "step: 3720 loss: 0.023\n",
      "step: 3721 loss: 0.027\n",
      "step: 3722 loss: 0.047\n",
      "step: 3723 loss: 0.055\n",
      "step: 3724 loss: 0.036\n",
      "step: 3725 loss: 0.045\n",
      "step: 3726 loss: 0.027\n",
      "step: 3727 loss: 0.047\n",
      "step: 3728 loss: 0.045\n",
      "step: 3729 loss: 0.050\n",
      "step: 3730 loss: 0.045\n",
      "step: 3731 loss: 0.056\n",
      "step: 3732 loss: 0.030\n",
      "step: 3733 loss: 0.055\n",
      "step: 3734 loss: 0.061\n",
      "step: 3735 loss: 0.041\n",
      "step: 3736 loss: 0.044\n",
      "step: 3737 loss: 0.039\n",
      "step: 3738 loss: 0.034\n",
      "step: 3739 loss: 0.047\n",
      "step: 3740 loss: 0.052\n",
      "step: 3741 loss: 0.042\n",
      "step: 3742 loss: 0.051\n",
      "step: 3743 loss: 0.054\n",
      "step: 3744 loss: 0.051\n",
      "step: 3745 loss: 0.055\n",
      "step: 3746 loss: 0.045\n",
      "step: 3747 loss: 0.043\n",
      "step: 3748 loss: 0.040\n",
      "step: 3749 loss: 0.032\n",
      "step: 3750 loss: 0.051\n",
      "step: 3751 loss: 0.035\n",
      "step: 3752 loss: 0.032\n",
      "step: 3753 loss: 0.040\n",
      "step: 3754 loss: 0.073\n",
      "step: 3755 loss: 0.052\n",
      "step: 3756 loss: 0.038\n",
      "step: 3757 loss: 0.026\n",
      "step: 3758 loss: 0.034\n",
      "step: 3759 loss: 0.036\n",
      "step: 3760 loss: 0.042\n",
      "step: 3761 loss: 0.058\n",
      "step: 3762 loss: 0.051\n",
      "step: 3763 loss: 0.057\n",
      "step: 3764 loss: 0.037\n",
      "step: 3765 loss: 0.048\n",
      "step: 3766 loss: 0.040\n",
      "step: 3767 loss: 0.032\n",
      "step: 3768 loss: 0.029\n",
      "step: 3769 loss: 0.038\n",
      "step: 3770 loss: 0.029\n",
      "step: 3771 loss: 0.050\n",
      "step: 3772 loss: 0.035\n",
      "step: 3773 loss: 0.051\n",
      "step: 3774 loss: 0.048\n",
      "step: 3775 loss: 0.049\n",
      "step: 3776 loss: 0.040\n",
      "step: 3777 loss: 0.034\n",
      "step: 3778 loss: 0.034\n",
      "step: 3779 loss: 0.024\n",
      "step: 3780 loss: 0.045\n",
      "step: 3781 loss: 0.038\n",
      "step: 3782 loss: 0.035\n",
      "step: 3783 loss: 0.038\n",
      "step: 3784 loss: 0.034\n",
      "step: 3785 loss: 0.040\n",
      "step: 3786 loss: 0.035\n",
      "step: 3787 loss: 0.032\n",
      "step: 3788 loss: 0.043\n",
      "step: 3789 loss: 0.026\n",
      "step: 3790 loss: 0.027\n",
      "step: 3791 loss: 0.045\n",
      "step: 3792 loss: 0.034\n",
      "step: 3793 loss: 0.049\n",
      "step: 3794 loss: 0.041\n",
      "step: 3795 loss: 0.037\n",
      "step: 3796 loss: 0.044\n",
      "step: 3797 loss: 0.050\n",
      "step: 3798 loss: 0.035\n",
      "step: 3799 loss: 0.046\n",
      "step: 3800 loss: 0.038\n",
      "step: 3801 loss: 0.042\n",
      "step: 3802 loss: 0.046\n",
      "step: 3803 loss: 0.049\n",
      "step: 3804 loss: 0.052\n",
      "step: 3805 loss: 0.030\n",
      "step: 3806 loss: 0.032\n",
      "step: 3807 loss: 0.055\n",
      "step: 3808 loss: 0.030\n",
      "step: 3809 loss: 0.035\n",
      "step: 3810 loss: 0.038\n",
      "step: 3811 loss: 0.042\n",
      "step: 3812 loss: 0.031\n",
      "step: 3813 loss: 0.065\n",
      "step: 3814 loss: 0.046\n",
      "step: 3815 loss: 0.060\n",
      "step: 3816 loss: 0.047\n",
      "step: 3817 loss: 0.037\n",
      "step: 3818 loss: 0.043\n",
      "step: 3819 loss: 0.039\n",
      "step: 3820 loss: 0.048\n",
      "step: 3821 loss: 0.040\n",
      "step: 3822 loss: 0.046\n",
      "step: 3823 loss: 0.031\n",
      "step: 3824 loss: 0.056\n",
      "step: 3825 loss: 0.033\n",
      "step: 3826 loss: 0.056\n",
      "step: 3827 loss: 0.037\n",
      "step: 3828 loss: 0.034\n",
      "step: 3829 loss: 0.037\n",
      "step: 3830 loss: 0.071\n",
      "step: 3831 loss: 0.052\n",
      "step: 3832 loss: 0.033\n",
      "step: 3833 loss: 0.033\n",
      "step: 3834 loss: 0.050\n",
      "step: 3835 loss: 0.051\n",
      "step: 3836 loss: 0.038\n",
      "step: 3837 loss: 0.051\n",
      "step: 3838 loss: 0.042\n",
      "step: 3839 loss: 0.062\n",
      "step: 3840 loss: 0.043\n",
      "step: 3841 loss: 0.025\n",
      "step: 3842 loss: 0.040\n",
      "step: 3843 loss: 0.035\n",
      "step: 3844 loss: 0.045\n",
      "step: 3845 loss: 0.037\n",
      "step: 3846 loss: 0.050\n",
      "step: 3847 loss: 0.033\n",
      "step: 3848 loss: 0.028\n",
      "step: 3849 loss: 0.040\n",
      "step: 3850 loss: 0.070\n",
      "step: 3851 loss: 0.037\n",
      "step: 3852 loss: 0.034\n",
      "step: 3853 loss: 0.042\n",
      "step: 3854 loss: 0.050\n",
      "step: 3855 loss: 0.035\n",
      "step: 3856 loss: 0.033\n",
      "step: 3857 loss: 0.042\n",
      "step: 3858 loss: 0.037\n",
      "step: 3859 loss: 0.069\n",
      "step: 3860 loss: 0.035\n",
      "step: 3861 loss: 0.030\n",
      "step: 3862 loss: 0.040\n",
      "step: 3863 loss: 0.036\n",
      "step: 3864 loss: 0.030\n",
      "step: 3865 loss: 0.038\n",
      "step: 3866 loss: 0.027\n",
      "step: 3867 loss: 0.049\n",
      "step: 3868 loss: 0.027\n",
      "step: 3869 loss: 0.028\n",
      "step: 3870 loss: 0.022\n",
      "step: 3871 loss: 0.018\n",
      "step: 3872 loss: 0.047\n",
      "step: 3873 loss: 0.034\n",
      "step: 3874 loss: 0.052\n",
      "step: 3875 loss: 0.049\n",
      "step: 3876 loss: 0.039\n",
      "step: 3877 loss: 0.059\n",
      "step: 3878 loss: 0.038\n",
      "step: 3879 loss: 0.024\n",
      "step: 3880 loss: 0.039\n",
      "step: 3881 loss: 0.044\n",
      "step: 3882 loss: 0.041\n",
      "step: 3883 loss: 0.038\n",
      "step: 3884 loss: 0.037\n",
      "step: 3885 loss: 0.036\n",
      "step: 3886 loss: 0.038\n",
      "step: 3887 loss: 0.050\n",
      "step: 3888 loss: 0.030\n",
      "step: 3889 loss: 0.051\n",
      "step: 3890 loss: 0.024\n",
      "step: 3891 loss: 0.042\n",
      "step: 3892 loss: 0.030\n",
      "step: 3893 loss: 0.025\n",
      "step: 3894 loss: 0.031\n",
      "step: 3895 loss: 0.040\n",
      "step: 3896 loss: 0.028\n",
      "step: 3897 loss: 0.039\n",
      "step: 3898 loss: 0.053\n",
      "step: 3899 loss: 0.049\n",
      "step: 3900 loss: 0.048\n",
      "step: 3901 loss: 0.030\n",
      "step: 3902 loss: 0.048\n",
      "step: 3903 loss: 0.033\n",
      "step: 3904 loss: 0.046\n",
      "step: 3905 loss: 0.050\n",
      "step: 3906 loss: 0.030\n",
      "step: 3907 loss: 0.027\n",
      "step: 3908 loss: 0.034\n",
      "step: 3909 loss: 0.051\n",
      "step: 3910 loss: 0.025\n",
      "step: 3911 loss: 0.039\n",
      "step: 3912 loss: 0.037\n",
      "step: 3913 loss: 0.042\n",
      "step: 3914 loss: 0.037\n",
      "step: 3915 loss: 0.057\n",
      "step: 3916 loss: 0.033\n",
      "step: 3917 loss: 0.036\n",
      "step: 3918 loss: 0.041\n",
      "step: 3919 loss: 0.039\n",
      "step: 3920 loss: 0.042\n",
      "step: 3921 loss: 0.042\n",
      "step: 3922 loss: 0.047\n",
      "step: 3923 loss: 0.025\n",
      "step: 3924 loss: 0.034\n",
      "step: 3925 loss: 0.040\n",
      "step: 3926 loss: 0.047\n",
      "step: 3927 loss: 0.037\n",
      "step: 3928 loss: 0.036\n",
      "step: 3929 loss: 0.052\n",
      "step: 3930 loss: 0.058\n",
      "step: 3931 loss: 0.036\n",
      "step: 3932 loss: 0.031\n",
      "step: 3933 loss: 0.046\n",
      "step: 3934 loss: 0.022\n",
      "step: 3935 loss: 0.043\n",
      "step: 3936 loss: 0.046\n",
      "step: 3937 loss: 0.043\n",
      "step: 3938 loss: 0.031\n",
      "step: 3939 loss: 0.038\n",
      "step: 3940 loss: 0.030\n",
      "step: 3941 loss: 0.044\n",
      "step: 3942 loss: 0.043\n",
      "step: 3943 loss: 0.049\n",
      "step: 3944 loss: 0.054\n",
      "step: 3945 loss: 0.042\n",
      "step: 3946 loss: 0.039\n",
      "step: 3947 loss: 0.039\n",
      "step: 3948 loss: 0.043\n",
      "step: 3949 loss: 0.058\n",
      "step: 3950 loss: 0.040\n",
      "step: 3951 loss: 0.052\n",
      "step: 3952 loss: 0.034\n",
      "step: 3953 loss: 0.034\n",
      "step: 3954 loss: 0.044\n",
      "step: 3955 loss: 0.031\n",
      "step: 3956 loss: 0.035\n",
      "step: 3957 loss: 0.049\n",
      "step: 3958 loss: 0.046\n",
      "step: 3959 loss: 0.031\n",
      "step: 3960 loss: 0.038\n",
      "step: 3961 loss: 0.032\n",
      "step: 3962 loss: 0.041\n",
      "step: 3963 loss: 0.030\n",
      "step: 3964 loss: 0.056\n",
      "step: 3965 loss: 0.031\n",
      "step: 3966 loss: 0.030\n",
      "step: 3967 loss: 0.037\n",
      "step: 3968 loss: 0.040\n",
      "step: 3969 loss: 0.052\n",
      "step: 3970 loss: 0.040\n",
      "step: 3971 loss: 0.040\n",
      "step: 3972 loss: 0.035\n",
      "step: 3973 loss: 0.042\n",
      "step: 3974 loss: 0.048\n",
      "step: 3975 loss: 0.025\n",
      "step: 3976 loss: 0.022\n",
      "step: 3977 loss: 0.036\n",
      "step: 3978 loss: 0.040\n",
      "step: 3979 loss: 0.045\n",
      "step: 3980 loss: 0.046\n",
      "step: 3981 loss: 0.043\n",
      "step: 3982 loss: 0.036\n",
      "step: 3983 loss: 0.051\n",
      "step: 3984 loss: 0.033\n",
      "step: 3985 loss: 0.050\n",
      "step: 3986 loss: 0.032\n",
      "step: 3987 loss: 0.040\n",
      "step: 3988 loss: 0.033\n",
      "step: 3989 loss: 0.046\n",
      "step: 3990 loss: 0.052\n",
      "step: 3991 loss: 0.049\n",
      "step: 3992 loss: 0.039\n",
      "step: 3993 loss: 0.045\n",
      "step: 3994 loss: 0.040\n",
      "step: 3995 loss: 0.047\n",
      "step: 3996 loss: 0.042\n",
      "step: 3997 loss: 0.030\n",
      "step: 3998 loss: 0.040\n",
      "step: 3999 loss: 0.032\n",
      "step: 4000 loss: 0.040\n",
      "step: 4001 loss: 0.033\n",
      "step: 4002 loss: 0.044\n",
      "step: 4003 loss: 0.051\n",
      "step: 4004 loss: 0.025\n",
      "step: 4005 loss: 0.032\n",
      "step: 4006 loss: 0.063\n",
      "step: 4007 loss: 0.039\n",
      "step: 4008 loss: 0.023\n",
      "step: 4009 loss: 0.040\n",
      "step: 4010 loss: 0.034\n",
      "step: 4011 loss: 0.026\n",
      "step: 4012 loss: 0.037\n",
      "step: 4013 loss: 0.035\n",
      "step: 4014 loss: 0.034\n",
      "step: 4015 loss: 0.043\n",
      "step: 4016 loss: 0.039\n",
      "step: 4017 loss: 0.044\n",
      "step: 4018 loss: 0.053\n",
      "step: 4019 loss: 0.034\n",
      "step: 4020 loss: 0.032\n",
      "step: 4021 loss: 0.054\n",
      "step: 4022 loss: 0.031\n",
      "step: 4023 loss: 0.035\n",
      "step: 4024 loss: 0.048\n",
      "step: 4025 loss: 0.039\n",
      "step: 4026 loss: 0.042\n",
      "step: 4027 loss: 0.030\n",
      "step: 4028 loss: 0.035\n",
      "step: 4029 loss: 0.058\n",
      "step: 4030 loss: 0.037\n",
      "step: 4031 loss: 0.055\n",
      "step: 4032 loss: 0.033\n",
      "step: 4033 loss: 0.049\n",
      "step: 4034 loss: 0.073\n",
      "step: 4035 loss: 0.032\n",
      "step: 4036 loss: 0.042\n",
      "step: 4037 loss: 0.043\n",
      "step: 4038 loss: 0.032\n",
      "step: 4039 loss: 0.036\n",
      "step: 4040 loss: 0.037\n",
      "step: 4041 loss: 0.048\n",
      "step: 4042 loss: 0.037\n",
      "step: 4043 loss: 0.026\n",
      "step: 4044 loss: 0.041\n",
      "step: 4045 loss: 0.029\n",
      "step: 4046 loss: 0.026\n",
      "step: 4047 loss: 0.049\n",
      "step: 4048 loss: 0.045\n",
      "step: 4049 loss: 0.063\n",
      "step: 4050 loss: 0.036\n",
      "step: 4051 loss: 0.036\n",
      "step: 4052 loss: 0.065\n",
      "step: 4053 loss: 0.048\n",
      "step: 4054 loss: 0.050\n",
      "step: 4055 loss: 0.043\n",
      "step: 4056 loss: 0.043\n",
      "step: 4057 loss: 0.037\n",
      "step: 4058 loss: 0.031\n",
      "step: 4059 loss: 0.048\n",
      "step: 4060 loss: 0.042\n",
      "step: 4061 loss: 0.041\n",
      "step: 4062 loss: 0.038\n",
      "step: 4063 loss: 0.030\n",
      "step: 4064 loss: 0.049\n",
      "step: 4065 loss: 0.032\n",
      "step: 4066 loss: 0.044\n",
      "step: 4067 loss: 0.033\n",
      "step: 4068 loss: 0.038\n",
      "step: 4069 loss: 0.029\n",
      "step: 4070 loss: 0.021\n",
      "step: 4071 loss: 0.042\n",
      "step: 4072 loss: 0.032\n",
      "step: 4073 loss: 0.056\n",
      "step: 4074 loss: 0.036\n",
      "step: 4075 loss: 0.046\n",
      "step: 4076 loss: 0.037\n",
      "step: 4077 loss: 0.042\n",
      "step: 4078 loss: 0.041\n",
      "step: 4079 loss: 0.039\n",
      "step: 4080 loss: 0.041\n",
      "step: 4081 loss: 0.022\n",
      "step: 4082 loss: 0.034\n",
      "step: 4083 loss: 0.041\n",
      "step: 4084 loss: 0.033\n",
      "step: 4085 loss: 0.052\n",
      "step: 4086 loss: 0.037\n",
      "step: 4087 loss: 0.032\n",
      "step: 4088 loss: 0.048\n",
      "step: 4089 loss: 0.042\n",
      "step: 4090 loss: 0.029\n",
      "step: 4091 loss: 0.032\n",
      "step: 4092 loss: 0.042\n",
      "step: 4093 loss: 0.033\n",
      "step: 4094 loss: 0.050\n",
      "step: 4095 loss: 0.035\n",
      "step: 4096 loss: 0.042\n",
      "step: 4097 loss: 0.039\n",
      "step: 4098 loss: 0.052\n",
      "step: 4099 loss: 0.039\n",
      "step: 4100 loss: 0.041\n",
      "step: 4101 loss: 0.040\n",
      "step: 4102 loss: 0.045\n",
      "step: 4103 loss: 0.048\n",
      "step: 4104 loss: 0.047\n",
      "step: 4105 loss: 0.045\n",
      "step: 4106 loss: 0.024\n",
      "step: 4107 loss: 0.041\n",
      "step: 4108 loss: 0.050\n",
      "step: 4109 loss: 0.032\n",
      "step: 4110 loss: 0.027\n",
      "step: 4111 loss: 0.044\n",
      "step: 4112 loss: 0.057\n",
      "step: 4113 loss: 0.028\n",
      "step: 4114 loss: 0.036\n",
      "step: 4115 loss: 0.039\n",
      "step: 4116 loss: 0.036\n",
      "step: 4117 loss: 0.041\n",
      "step: 4118 loss: 0.027\n",
      "step: 4119 loss: 0.038\n",
      "step: 4120 loss: 0.050\n",
      "step: 4121 loss: 0.040\n",
      "step: 4122 loss: 0.031\n",
      "step: 4123 loss: 0.029\n",
      "step: 4124 loss: 0.034\n",
      "step: 4125 loss: 0.020\n",
      "step: 4126 loss: 0.039\n",
      "step: 4127 loss: 0.055\n",
      "step: 4128 loss: 0.026\n",
      "step: 4129 loss: 0.033\n",
      "step: 4130 loss: 0.044\n",
      "step: 4131 loss: 0.038\n",
      "step: 4132 loss: 0.035\n",
      "step: 4133 loss: 0.054\n",
      "step: 4134 loss: 0.031\n",
      "step: 4135 loss: 0.041\n",
      "step: 4136 loss: 0.036\n",
      "step: 4137 loss: 0.035\n",
      "step: 4138 loss: 0.034\n",
      "step: 4139 loss: 0.048\n",
      "step: 4140 loss: 0.053\n",
      "step: 4141 loss: 0.044\n",
      "step: 4142 loss: 0.051\n",
      "step: 4143 loss: 0.050\n",
      "step: 4144 loss: 0.031\n",
      "step: 4145 loss: 0.038\n",
      "step: 4146 loss: 0.048\n",
      "step: 4147 loss: 0.041\n",
      "step: 4148 loss: 0.038\n",
      "step: 4149 loss: 0.035\n",
      "step: 4150 loss: 0.050\n",
      "step: 4151 loss: 0.040\n",
      "step: 4152 loss: 0.045\n",
      "step: 4153 loss: 0.054\n",
      "step: 4154 loss: 0.034\n",
      "step: 4155 loss: 0.029\n",
      "step: 4156 loss: 0.039\n",
      "step: 4157 loss: 0.042\n",
      "step: 4158 loss: 0.038\n",
      "step: 4159 loss: 0.042\n",
      "step: 4160 loss: 0.034\n",
      "step: 4161 loss: 0.040\n",
      "step: 4162 loss: 0.041\n",
      "step: 4163 loss: 0.052\n",
      "step: 4164 loss: 0.041\n",
      "step: 4165 loss: 0.050\n",
      "step: 4166 loss: 0.039\n",
      "step: 4167 loss: 0.038\n",
      "step: 4168 loss: 0.059\n",
      "step: 4169 loss: 0.042\n",
      "step: 4170 loss: 0.039\n",
      "step: 4171 loss: 0.031\n",
      "step: 4172 loss: 0.034\n",
      "step: 4173 loss: 0.021\n",
      "step: 4174 loss: 0.037\n",
      "step: 4175 loss: 0.044\n",
      "step: 4176 loss: 0.034\n",
      "step: 4177 loss: 0.039\n",
      "step: 4178 loss: 0.037\n",
      "step: 4179 loss: 0.035\n",
      "step: 4180 loss: 0.031\n",
      "step: 4181 loss: 0.044\n",
      "step: 4182 loss: 0.051\n",
      "step: 4183 loss: 0.027\n",
      "step: 4184 loss: 0.036\n",
      "step: 4185 loss: 0.049\n",
      "step: 4186 loss: 0.032\n",
      "step: 4187 loss: 0.056\n",
      "step: 4188 loss: 0.048\n",
      "step: 4189 loss: 0.041\n",
      "step: 4190 loss: 0.050\n",
      "step: 4191 loss: 0.037\n",
      "step: 4192 loss: 0.046\n",
      "step: 4193 loss: 0.049\n",
      "step: 4194 loss: 0.030\n",
      "step: 4195 loss: 0.040\n",
      "step: 4196 loss: 0.044\n",
      "step: 4197 loss: 0.043\n",
      "step: 4198 loss: 0.046\n",
      "step: 4199 loss: 0.047\n",
      "step: 4200 loss: 0.042\n",
      "step: 4201 loss: 0.031\n",
      "step: 4202 loss: 0.026\n",
      "step: 4203 loss: 0.026\n",
      "step: 4204 loss: 0.045\n",
      "step: 4205 loss: 0.037\n",
      "step: 4206 loss: 0.032\n",
      "step: 4207 loss: 0.036\n",
      "step: 4208 loss: 0.031\n",
      "step: 4209 loss: 0.075\n",
      "step: 4210 loss: 0.033\n",
      "step: 4211 loss: 0.054\n",
      "step: 4212 loss: 0.037\n",
      "step: 4213 loss: 0.043\n",
      "step: 4214 loss: 0.040\n",
      "step: 4215 loss: 0.027\n",
      "step: 4216 loss: 0.055\n",
      "step: 4217 loss: 0.043\n",
      "step: 4218 loss: 0.042\n",
      "step: 4219 loss: 0.050\n",
      "step: 4220 loss: 0.039\n",
      "step: 4221 loss: 0.040\n",
      "step: 4222 loss: 0.041\n",
      "step: 4223 loss: 0.046\n",
      "step: 4224 loss: 0.039\n",
      "step: 4225 loss: 0.035\n",
      "step: 4226 loss: 0.033\n",
      "step: 4227 loss: 0.045\n",
      "step: 4228 loss: 0.030\n",
      "step: 4229 loss: 0.034\n",
      "step: 4230 loss: 0.040\n",
      "step: 4231 loss: 0.032\n",
      "step: 4232 loss: 0.048\n",
      "step: 4233 loss: 0.032\n",
      "step: 4234 loss: 0.043\n",
      "step: 4235 loss: 0.035\n",
      "step: 4236 loss: 0.034\n",
      "step: 4237 loss: 0.024\n",
      "step: 4238 loss: 0.047\n",
      "step: 4239 loss: 0.042\n",
      "step: 4240 loss: 0.041\n",
      "step: 4241 loss: 0.028\n",
      "step: 4242 loss: 0.037\n",
      "step: 4243 loss: 0.045\n",
      "step: 4244 loss: 0.036\n",
      "step: 4245 loss: 0.035\n",
      "step: 4246 loss: 0.035\n",
      "step: 4247 loss: 0.025\n",
      "step: 4248 loss: 0.031\n",
      "step: 4249 loss: 0.037\n",
      "step: 4250 loss: 0.041\n",
      "step: 4251 loss: 0.046\n",
      "step: 4252 loss: 0.035\n",
      "step: 4253 loss: 0.025\n",
      "step: 4254 loss: 0.048\n",
      "step: 4255 loss: 0.034\n",
      "step: 4256 loss: 0.047\n",
      "step: 4257 loss: 0.042\n",
      "step: 4258 loss: 0.037\n",
      "step: 4259 loss: 0.072\n",
      "step: 4260 loss: 0.050\n",
      "step: 4261 loss: 0.036\n",
      "step: 4262 loss: 0.042\n",
      "step: 4263 loss: 0.044\n",
      "step: 4264 loss: 0.059\n",
      "step: 4265 loss: 0.034\n",
      "step: 4266 loss: 0.042\n",
      "step: 4267 loss: 0.040\n",
      "step: 4268 loss: 0.046\n",
      "step: 4269 loss: 0.030\n",
      "step: 4270 loss: 0.045\n",
      "step: 4271 loss: 0.038\n",
      "step: 4272 loss: 0.043\n",
      "step: 4273 loss: 0.031\n",
      "step: 4274 loss: 0.023\n",
      "step: 4275 loss: 0.029\n",
      "step: 4276 loss: 0.026\n",
      "step: 4277 loss: 0.026\n",
      "step: 4278 loss: 0.040\n",
      "step: 4279 loss: 0.045\n",
      "step: 4280 loss: 0.030\n",
      "step: 4281 loss: 0.051\n",
      "step: 4282 loss: 0.033\n",
      "step: 4283 loss: 0.020\n",
      "step: 4284 loss: 0.040\n",
      "step: 4285 loss: 0.033\n",
      "step: 4286 loss: 0.039\n",
      "step: 4287 loss: 0.045\n",
      "step: 4288 loss: 0.036\n",
      "step: 4289 loss: 0.036\n",
      "step: 4290 loss: 0.050\n",
      "step: 4291 loss: 0.049\n",
      "step: 4292 loss: 0.037\n",
      "step: 4293 loss: 0.033\n",
      "step: 4294 loss: 0.038\n",
      "step: 4295 loss: 0.036\n",
      "step: 4296 loss: 0.059\n",
      "step: 4297 loss: 0.043\n",
      "step: 4298 loss: 0.045\n",
      "step: 4299 loss: 0.026\n",
      "step: 4300 loss: 0.032\n",
      "step: 4301 loss: 0.040\n",
      "step: 4302 loss: 0.034\n",
      "step: 4303 loss: 0.048\n",
      "step: 4304 loss: 0.035\n",
      "step: 4305 loss: 0.032\n",
      "step: 4306 loss: 0.035\n",
      "step: 4307 loss: 0.041\n",
      "step: 4308 loss: 0.033\n",
      "step: 4309 loss: 0.037\n",
      "step: 4310 loss: 0.043\n",
      "step: 4311 loss: 0.037\n",
      "step: 4312 loss: 0.029\n",
      "step: 4313 loss: 0.045\n",
      "step: 4314 loss: 0.046\n",
      "step: 4315 loss: 0.026\n",
      "step: 4316 loss: 0.039\n",
      "step: 4317 loss: 0.035\n",
      "step: 4318 loss: 0.045\n",
      "step: 4319 loss: 0.030\n",
      "step: 4320 loss: 0.037\n",
      "step: 4321 loss: 0.045\n",
      "step: 4322 loss: 0.037\n",
      "step: 4323 loss: 0.038\n",
      "step: 4324 loss: 0.040\n",
      "step: 4325 loss: 0.038\n",
      "step: 4326 loss: 0.045\n",
      "step: 4327 loss: 0.036\n",
      "step: 4328 loss: 0.032\n",
      "step: 4329 loss: 0.039\n",
      "step: 4330 loss: 0.044\n",
      "step: 4331 loss: 0.064\n",
      "step: 4332 loss: 0.034\n",
      "step: 4333 loss: 0.045\n",
      "step: 4334 loss: 0.043\n",
      "step: 4335 loss: 0.031\n",
      "step: 4336 loss: 0.051\n",
      "step: 4337 loss: 0.037\n",
      "step: 4338 loss: 0.038\n",
      "step: 4339 loss: 0.048\n",
      "step: 4340 loss: 0.022\n",
      "step: 4341 loss: 0.052\n",
      "step: 4342 loss: 0.046\n",
      "step: 4343 loss: 0.036\n",
      "step: 4344 loss: 0.055\n",
      "step: 4345 loss: 0.028\n",
      "step: 4346 loss: 0.042\n",
      "step: 4347 loss: 0.039\n",
      "step: 4348 loss: 0.033\n",
      "step: 4349 loss: 0.045\n",
      "step: 4350 loss: 0.033\n",
      "step: 4351 loss: 0.045\n",
      "step: 4352 loss: 0.042\n",
      "step: 4353 loss: 0.056\n",
      "step: 4354 loss: 0.036\n",
      "step: 4355 loss: 0.040\n",
      "step: 4356 loss: 0.036\n",
      "step: 4357 loss: 0.021\n",
      "step: 4358 loss: 0.032\n",
      "step: 4359 loss: 0.036\n",
      "step: 4360 loss: 0.047\n",
      "step: 4361 loss: 0.051\n",
      "step: 4362 loss: 0.037\n",
      "step: 4363 loss: 0.029\n",
      "step: 4364 loss: 0.019\n",
      "step: 4365 loss: 0.041\n",
      "step: 4366 loss: 0.055\n",
      "step: 4367 loss: 0.036\n",
      "step: 4368 loss: 0.031\n",
      "step: 4369 loss: 0.044\n",
      "step: 4370 loss: 0.041\n",
      "step: 4371 loss: 0.032\n",
      "step: 4372 loss: 0.045\n",
      "step: 4373 loss: 0.043\n",
      "step: 4374 loss: 0.050\n",
      "step: 4375 loss: 0.036\n",
      "step: 4376 loss: 0.041\n",
      "step: 4377 loss: 0.039\n",
      "step: 4378 loss: 0.057\n",
      "step: 4379 loss: 0.041\n",
      "step: 4380 loss: 0.039\n",
      "step: 4381 loss: 0.040\n",
      "step: 4382 loss: 0.029\n",
      "step: 4383 loss: 0.043\n",
      "step: 4384 loss: 0.027\n",
      "step: 4385 loss: 0.052\n",
      "step: 4386 loss: 0.035\n",
      "step: 4387 loss: 0.036\n",
      "step: 4388 loss: 0.050\n",
      "step: 4389 loss: 0.031\n",
      "step: 4390 loss: 0.046\n",
      "step: 4391 loss: 0.061\n",
      "step: 4392 loss: 0.042\n",
      "step: 4393 loss: 0.058\n",
      "step: 4394 loss: 0.054\n",
      "step: 4395 loss: 0.048\n",
      "step: 4396 loss: 0.036\n",
      "step: 4397 loss: 0.019\n",
      "step: 4398 loss: 0.041\n",
      "step: 4399 loss: 0.042\n",
      "step: 4400 loss: 0.028\n",
      "step: 4401 loss: 0.044\n",
      "step: 4402 loss: 0.044\n",
      "step: 4403 loss: 0.046\n",
      "step: 4404 loss: 0.030\n",
      "step: 4405 loss: 0.037\n",
      "step: 4406 loss: 0.048\n",
      "step: 4407 loss: 0.037\n",
      "step: 4408 loss: 0.034\n",
      "step: 4409 loss: 0.047\n",
      "step: 4410 loss: 0.027\n",
      "step: 4411 loss: 0.033\n",
      "step: 4412 loss: 0.041\n",
      "step: 4413 loss: 0.039\n",
      "step: 4414 loss: 0.052\n",
      "step: 4415 loss: 0.035\n",
      "step: 4416 loss: 0.035\n",
      "step: 4417 loss: 0.053\n",
      "step: 4418 loss: 0.025\n",
      "step: 4419 loss: 0.029\n",
      "step: 4420 loss: 0.043\n",
      "step: 4421 loss: 0.037\n",
      "step: 4422 loss: 0.030\n",
      "step: 4423 loss: 0.044\n",
      "step: 4424 loss: 0.036\n",
      "step: 4425 loss: 0.029\n",
      "step: 4426 loss: 0.034\n",
      "step: 4427 loss: 0.047\n",
      "step: 4428 loss: 0.027\n",
      "step: 4429 loss: 0.031\n",
      "step: 4430 loss: 0.025\n",
      "step: 4431 loss: 0.041\n",
      "step: 4432 loss: 0.038\n",
      "step: 4433 loss: 0.054\n",
      "step: 4434 loss: 0.037\n",
      "step: 4435 loss: 0.047\n",
      "step: 4436 loss: 0.029\n",
      "step: 4437 loss: 0.041\n",
      "step: 4438 loss: 0.035\n",
      "step: 4439 loss: 0.038\n",
      "step: 4440 loss: 0.040\n",
      "step: 4441 loss: 0.044\n",
      "step: 4442 loss: 0.039\n",
      "step: 4443 loss: 0.042\n",
      "step: 4444 loss: 0.046\n",
      "step: 4445 loss: 0.040\n",
      "step: 4446 loss: 0.036\n",
      "step: 4447 loss: 0.025\n",
      "step: 4448 loss: 0.045\n",
      "step: 4449 loss: 0.031\n",
      "step: 4450 loss: 0.025\n",
      "step: 4451 loss: 0.038\n",
      "step: 4452 loss: 0.034\n",
      "step: 4453 loss: 0.027\n",
      "step: 4454 loss: 0.050\n",
      "step: 4455 loss: 0.039\n",
      "step: 4456 loss: 0.039\n",
      "step: 4457 loss: 0.029\n",
      "step: 4458 loss: 0.055\n",
      "step: 4459 loss: 0.028\n",
      "step: 4460 loss: 0.029\n",
      "step: 4461 loss: 0.033\n",
      "step: 4462 loss: 0.046\n",
      "step: 4463 loss: 0.017\n",
      "step: 4464 loss: 0.036\n",
      "step: 4465 loss: 0.026\n",
      "step: 4466 loss: 0.040\n",
      "step: 4467 loss: 0.031\n",
      "step: 4468 loss: 0.061\n",
      "step: 4469 loss: 0.029\n",
      "step: 4470 loss: 0.026\n",
      "step: 4471 loss: 0.037\n",
      "step: 4472 loss: 0.032\n",
      "step: 4473 loss: 0.031\n",
      "step: 4474 loss: 0.033\n",
      "step: 4475 loss: 0.036\n",
      "step: 4476 loss: 0.026\n",
      "step: 4477 loss: 0.039\n",
      "step: 4478 loss: 0.035\n",
      "step: 4479 loss: 0.042\n",
      "step: 4480 loss: 0.036\n",
      "step: 4481 loss: 0.027\n",
      "step: 4482 loss: 0.032\n",
      "step: 4483 loss: 0.050\n",
      "step: 4484 loss: 0.064\n",
      "step: 4485 loss: 0.046\n",
      "step: 4486 loss: 0.035\n",
      "step: 4487 loss: 0.034\n",
      "step: 4488 loss: 0.041\n",
      "step: 4489 loss: 0.042\n",
      "step: 4490 loss: 0.038\n",
      "step: 4491 loss: 0.042\n",
      "step: 4492 loss: 0.045\n",
      "step: 4493 loss: 0.032\n",
      "step: 4494 loss: 0.045\n",
      "step: 4495 loss: 0.040\n",
      "step: 4496 loss: 0.039\n",
      "step: 4497 loss: 0.044\n",
      "step: 4498 loss: 0.028\n",
      "step: 4499 loss: 0.049\n",
      "step: 4500 loss: 0.041\n",
      "step: 4501 loss: 0.038\n",
      "step: 4502 loss: 0.037\n",
      "step: 4503 loss: 0.042\n",
      "step: 4504 loss: 0.041\n",
      "step: 4505 loss: 0.031\n",
      "step: 4506 loss: 0.037\n",
      "step: 4507 loss: 0.053\n",
      "step: 4508 loss: 0.032\n",
      "step: 4509 loss: 0.038\n",
      "step: 4510 loss: 0.031\n",
      "step: 4511 loss: 0.047\n",
      "step: 4512 loss: 0.026\n",
      "step: 4513 loss: 0.045\n",
      "step: 4514 loss: 0.047\n",
      "step: 4515 loss: 0.045\n",
      "step: 4516 loss: 0.044\n",
      "step: 4517 loss: 0.034\n",
      "step: 4518 loss: 0.029\n",
      "step: 4519 loss: 0.033\n",
      "step: 4520 loss: 0.038\n",
      "step: 4521 loss: 0.025\n",
      "step: 4522 loss: 0.041\n",
      "step: 4523 loss: 0.035\n",
      "step: 4524 loss: 0.041\n",
      "step: 4525 loss: 0.045\n",
      "step: 4526 loss: 0.051\n",
      "step: 4527 loss: 0.028\n",
      "step: 4528 loss: 0.029\n",
      "step: 4529 loss: 0.042\n",
      "step: 4530 loss: 0.047\n",
      "step: 4531 loss: 0.029\n",
      "step: 4532 loss: 0.032\n",
      "step: 4533 loss: 0.050\n",
      "step: 4534 loss: 0.022\n",
      "step: 4535 loss: 0.028\n",
      "step: 4536 loss: 0.025\n",
      "step: 4537 loss: 0.043\n",
      "step: 4538 loss: 0.036\n",
      "step: 4539 loss: 0.033\n",
      "step: 4540 loss: 0.028\n",
      "step: 4541 loss: 0.031\n",
      "step: 4542 loss: 0.048\n",
      "step: 4543 loss: 0.034\n",
      "step: 4544 loss: 0.035\n",
      "step: 4545 loss: 0.031\n",
      "step: 4546 loss: 0.041\n",
      "step: 4547 loss: 0.047\n",
      "step: 4548 loss: 0.029\n",
      "step: 4549 loss: 0.028\n",
      "step: 4550 loss: 0.031\n",
      "step: 4551 loss: 0.034\n",
      "step: 4552 loss: 0.057\n",
      "step: 4553 loss: 0.043\n",
      "step: 4554 loss: 0.042\n",
      "step: 4555 loss: 0.042\n",
      "step: 4556 loss: 0.049\n",
      "step: 4557 loss: 0.059\n",
      "step: 4558 loss: 0.046\n",
      "step: 4559 loss: 0.032\n",
      "step: 4560 loss: 0.033\n",
      "step: 4561 loss: 0.041\n",
      "step: 4562 loss: 0.042\n",
      "step: 4563 loss: 0.040\n",
      "step: 4564 loss: 0.038\n",
      "step: 4565 loss: 0.033\n",
      "step: 4566 loss: 0.038\n",
      "step: 4567 loss: 0.030\n",
      "step: 4568 loss: 0.033\n",
      "step: 4569 loss: 0.034\n",
      "step: 4570 loss: 0.050\n",
      "step: 4571 loss: 0.027\n",
      "step: 4572 loss: 0.039\n",
      "step: 4573 loss: 0.045\n",
      "step: 4574 loss: 0.024\n",
      "step: 4575 loss: 0.057\n",
      "step: 4576 loss: 0.037\n",
      "step: 4577 loss: 0.037\n",
      "step: 4578 loss: 0.040\n",
      "step: 4579 loss: 0.024\n",
      "step: 4580 loss: 0.027\n",
      "step: 4581 loss: 0.044\n",
      "step: 4582 loss: 0.037\n",
      "step: 4583 loss: 0.025\n",
      "step: 4584 loss: 0.027\n",
      "step: 4585 loss: 0.040\n",
      "step: 4586 loss: 0.036\n",
      "step: 4587 loss: 0.031\n",
      "step: 4588 loss: 0.032\n",
      "step: 4589 loss: 0.031\n",
      "step: 4590 loss: 0.047\n",
      "step: 4591 loss: 0.023\n",
      "step: 4592 loss: 0.032\n",
      "step: 4593 loss: 0.042\n",
      "step: 4594 loss: 0.046\n",
      "step: 4595 loss: 0.028\n",
      "step: 4596 loss: 0.042\n",
      "step: 4597 loss: 0.030\n",
      "step: 4598 loss: 0.031\n",
      "step: 4599 loss: 0.041\n",
      "step: 4600 loss: 0.031\n",
      "step: 4601 loss: 0.056\n",
      "step: 4602 loss: 0.034\n",
      "step: 4603 loss: 0.031\n",
      "step: 4604 loss: 0.035\n",
      "step: 4605 loss: 0.036\n",
      "step: 4606 loss: 0.032\n",
      "step: 4607 loss: 0.020\n",
      "step: 4608 loss: 0.023\n",
      "step: 4609 loss: 0.028\n",
      "step: 4610 loss: 0.029\n",
      "step: 4611 loss: 0.026\n",
      "step: 4612 loss: 0.037\n",
      "step: 4613 loss: 0.041\n",
      "step: 4614 loss: 0.038\n",
      "step: 4615 loss: 0.039\n",
      "step: 4616 loss: 0.034\n",
      "step: 4617 loss: 0.040\n",
      "step: 4618 loss: 0.027\n",
      "step: 4619 loss: 0.036\n",
      "step: 4620 loss: 0.031\n",
      "step: 4621 loss: 0.062\n",
      "step: 4622 loss: 0.038\n",
      "step: 4623 loss: 0.042\n",
      "step: 4624 loss: 0.032\n",
      "step: 4625 loss: 0.037\n",
      "step: 4626 loss: 0.036\n",
      "step: 4627 loss: 0.030\n",
      "step: 4628 loss: 0.041\n",
      "step: 4629 loss: 0.044\n",
      "step: 4630 loss: 0.047\n",
      "step: 4631 loss: 0.044\n",
      "step: 4632 loss: 0.032\n",
      "step: 4633 loss: 0.048\n",
      "step: 4634 loss: 0.044\n",
      "step: 4635 loss: 0.046\n",
      "step: 4636 loss: 0.042\n",
      "step: 4637 loss: 0.040\n",
      "step: 4638 loss: 0.045\n",
      "step: 4639 loss: 0.024\n",
      "step: 4640 loss: 0.035\n",
      "step: 4641 loss: 0.063\n",
      "step: 4642 loss: 0.034\n",
      "step: 4643 loss: 0.023\n",
      "step: 4644 loss: 0.026\n",
      "step: 4645 loss: 0.047\n",
      "step: 4646 loss: 0.032\n",
      "step: 4647 loss: 0.050\n",
      "step: 4648 loss: 0.040\n",
      "step: 4649 loss: 0.043\n",
      "step: 4650 loss: 0.046\n",
      "step: 4651 loss: 0.024\n",
      "step: 4652 loss: 0.043\n",
      "step: 4653 loss: 0.043\n",
      "step: 4654 loss: 0.035\n",
      "step: 4655 loss: 0.035\n",
      "step: 4656 loss: 0.021\n",
      "step: 4657 loss: 0.026\n",
      "step: 4658 loss: 0.029\n",
      "step: 4659 loss: 0.021\n",
      "step: 4660 loss: 0.020\n",
      "step: 4661 loss: 0.025\n",
      "step: 4662 loss: 0.022\n",
      "step: 4663 loss: 0.028\n",
      "step: 4664 loss: 0.040\n",
      "step: 4665 loss: 0.042\n",
      "step: 4666 loss: 0.041\n",
      "step: 4667 loss: 0.036\n",
      "step: 4668 loss: 0.031\n",
      "step: 4669 loss: 0.047\n",
      "step: 4670 loss: 0.030\n",
      "step: 4671 loss: 0.031\n",
      "step: 4672 loss: 0.038\n",
      "step: 4673 loss: 0.033\n",
      "step: 4674 loss: 0.045\n",
      "step: 4675 loss: 0.031\n",
      "step: 4676 loss: 0.034\n",
      "step: 4677 loss: 0.029\n",
      "step: 4678 loss: 0.050\n",
      "step: 4679 loss: 0.034\n",
      "step: 4680 loss: 0.038\n",
      "step: 4681 loss: 0.031\n",
      "step: 4682 loss: 0.047\n",
      "step: 4683 loss: 0.038\n",
      "step: 4684 loss: 0.037\n",
      "step: 4685 loss: 0.040\n",
      "step: 4686 loss: 0.044\n",
      "step: 4687 loss: 0.036\n",
      "step: 4688 loss: 0.029\n",
      "step: 4689 loss: 0.041\n",
      "step: 4690 loss: 0.050\n",
      "step: 4691 loss: 0.028\n",
      "step: 4692 loss: 0.055\n",
      "step: 4693 loss: 0.042\n",
      "step: 4694 loss: 0.037\n",
      "step: 4695 loss: 0.038\n",
      "step: 4696 loss: 0.025\n",
      "step: 4697 loss: 0.036\n",
      "step: 4698 loss: 0.046\n",
      "step: 4699 loss: 0.040\n",
      "step: 4700 loss: 0.028\n",
      "step: 4701 loss: 0.040\n",
      "step: 4702 loss: 0.030\n",
      "step: 4703 loss: 0.044\n",
      "step: 4704 loss: 0.026\n",
      "step: 4705 loss: 0.045\n",
      "step: 4706 loss: 0.028\n",
      "step: 4707 loss: 0.030\n",
      "step: 4708 loss: 0.032\n",
      "step: 4709 loss: 0.041\n",
      "step: 4710 loss: 0.039\n",
      "step: 4711 loss: 0.048\n",
      "step: 4712 loss: 0.033\n",
      "step: 4713 loss: 0.036\n",
      "step: 4714 loss: 0.033\n",
      "step: 4715 loss: 0.019\n",
      "step: 4716 loss: 0.027\n",
      "step: 4717 loss: 0.037\n",
      "step: 4718 loss: 0.021\n",
      "step: 4719 loss: 0.041\n",
      "step: 4720 loss: 0.055\n",
      "step: 4721 loss: 0.036\n",
      "step: 4722 loss: 0.034\n",
      "step: 4723 loss: 0.029\n",
      "step: 4724 loss: 0.042\n",
      "step: 4725 loss: 0.050\n",
      "step: 4726 loss: 0.031\n",
      "step: 4727 loss: 0.035\n",
      "step: 4728 loss: 0.041\n",
      "step: 4729 loss: 0.025\n",
      "step: 4730 loss: 0.037\n",
      "step: 4731 loss: 0.034\n",
      "step: 4732 loss: 0.047\n",
      "step: 4733 loss: 0.048\n",
      "step: 4734 loss: 0.053\n",
      "step: 4735 loss: 0.045\n",
      "step: 4736 loss: 0.028\n",
      "step: 4737 loss: 0.039\n",
      "step: 4738 loss: 0.047\n",
      "step: 4739 loss: 0.054\n",
      "step: 4740 loss: 0.032\n",
      "step: 4741 loss: 0.036\n",
      "step: 4742 loss: 0.042\n",
      "step: 4743 loss: 0.038\n",
      "step: 4744 loss: 0.050\n",
      "step: 4745 loss: 0.035\n",
      "step: 4746 loss: 0.040\n",
      "step: 4747 loss: 0.039\n",
      "step: 4748 loss: 0.038\n",
      "step: 4749 loss: 0.046\n",
      "step: 4750 loss: 0.045\n",
      "step: 4751 loss: 0.054\n",
      "step: 4752 loss: 0.029\n",
      "step: 4753 loss: 0.028\n",
      "step: 4754 loss: 0.037\n",
      "step: 4755 loss: 0.043\n",
      "step: 4756 loss: 0.044\n",
      "step: 4757 loss: 0.036\n",
      "step: 4758 loss: 0.035\n",
      "step: 4759 loss: 0.036\n",
      "step: 4760 loss: 0.031\n",
      "step: 4761 loss: 0.031\n",
      "step: 4762 loss: 0.050\n",
      "step: 4763 loss: 0.056\n",
      "step: 4764 loss: 0.039\n",
      "step: 4765 loss: 0.035\n",
      "step: 4766 loss: 0.034\n",
      "step: 4767 loss: 0.029\n",
      "step: 4768 loss: 0.040\n",
      "step: 4769 loss: 0.050\n",
      "step: 4770 loss: 0.060\n",
      "step: 4771 loss: 0.046\n",
      "step: 4772 loss: 0.027\n",
      "step: 4773 loss: 0.046\n",
      "step: 4774 loss: 0.036\n",
      "step: 4775 loss: 0.034\n",
      "step: 4776 loss: 0.048\n",
      "step: 4777 loss: 0.028\n",
      "step: 4778 loss: 0.053\n",
      "step: 4779 loss: 0.059\n",
      "step: 4780 loss: 0.047\n",
      "step: 4781 loss: 0.025\n",
      "step: 4782 loss: 0.043\n",
      "step: 4783 loss: 0.041\n",
      "step: 4784 loss: 0.029\n",
      "step: 4785 loss: 0.032\n",
      "step: 4786 loss: 0.052\n",
      "step: 4787 loss: 0.045\n",
      "step: 4788 loss: 0.043\n",
      "step: 4789 loss: 0.026\n",
      "step: 4790 loss: 0.032\n",
      "step: 4791 loss: 0.043\n",
      "step: 4792 loss: 0.038\n",
      "step: 4793 loss: 0.043\n",
      "step: 4794 loss: 0.029\n",
      "step: 4795 loss: 0.035\n",
      "step: 4796 loss: 0.047\n",
      "step: 4797 loss: 0.070\n",
      "step: 4798 loss: 0.025\n",
      "step: 4799 loss: 0.042\n",
      "step: 4800 loss: 0.054\n",
      "step: 4801 loss: 0.047\n",
      "step: 4802 loss: 0.037\n",
      "step: 4803 loss: 0.044\n",
      "step: 4804 loss: 0.031\n",
      "step: 4805 loss: 0.032\n",
      "step: 4806 loss: 0.046\n",
      "step: 4807 loss: 0.036\n",
      "step: 4808 loss: 0.057\n",
      "step: 4809 loss: 0.040\n",
      "step: 4810 loss: 0.037\n",
      "step: 4811 loss: 0.036\n",
      "step: 4812 loss: 0.046\n",
      "step: 4813 loss: 0.046\n",
      "step: 4814 loss: 0.049\n",
      "step: 4815 loss: 0.041\n",
      "step: 4816 loss: 0.036\n",
      "step: 4817 loss: 0.054\n",
      "step: 4818 loss: 0.046\n",
      "step: 4819 loss: 0.042\n",
      "step: 4820 loss: 0.020\n",
      "step: 4821 loss: 0.062\n",
      "step: 4822 loss: 0.033\n",
      "step: 4823 loss: 0.043\n",
      "step: 4824 loss: 0.032\n",
      "step: 4825 loss: 0.036\n",
      "step: 4826 loss: 0.047\n",
      "step: 4827 loss: 0.044\n",
      "step: 4828 loss: 0.025\n",
      "step: 4829 loss: 0.043\n",
      "step: 4830 loss: 0.038\n",
      "step: 4831 loss: 0.028\n",
      "step: 4832 loss: 0.050\n",
      "step: 4833 loss: 0.038\n",
      "step: 4834 loss: 0.048\n",
      "step: 4835 loss: 0.055\n",
      "step: 4836 loss: 0.050\n",
      "step: 4837 loss: 0.038\n",
      "step: 4838 loss: 0.029\n",
      "step: 4839 loss: 0.045\n",
      "step: 4840 loss: 0.029\n",
      "step: 4841 loss: 0.022\n",
      "step: 4842 loss: 0.035\n",
      "step: 4843 loss: 0.042\n",
      "step: 4844 loss: 0.040\n",
      "step: 4845 loss: 0.028\n",
      "step: 4846 loss: 0.026\n",
      "step: 4847 loss: 0.035\n",
      "step: 4848 loss: 0.028\n",
      "step: 4849 loss: 0.053\n",
      "step: 4850 loss: 0.044\n",
      "step: 4851 loss: 0.028\n",
      "step: 4852 loss: 0.044\n",
      "step: 4853 loss: 0.031\n",
      "step: 4854 loss: 0.023\n",
      "step: 4855 loss: 0.053\n",
      "step: 4856 loss: 0.031\n",
      "step: 4857 loss: 0.048\n",
      "step: 4858 loss: 0.028\n",
      "step: 4859 loss: 0.043\n",
      "step: 4860 loss: 0.033\n",
      "step: 4861 loss: 0.048\n",
      "step: 4862 loss: 0.039\n",
      "step: 4863 loss: 0.032\n",
      "step: 4864 loss: 0.043\n",
      "step: 4865 loss: 0.035\n",
      "step: 4866 loss: 0.039\n",
      "step: 4867 loss: 0.031\n",
      "step: 4868 loss: 0.055\n",
      "step: 4869 loss: 0.030\n",
      "step: 4870 loss: 0.039\n",
      "step: 4871 loss: 0.035\n",
      "step: 4872 loss: 0.043\n",
      "step: 4873 loss: 0.048\n",
      "step: 4874 loss: 0.036\n",
      "step: 4875 loss: 0.053\n",
      "step: 4876 loss: 0.045\n",
      "step: 4877 loss: 0.032\n",
      "step: 4878 loss: 0.037\n",
      "step: 4879 loss: 0.030\n",
      "step: 4880 loss: 0.051\n",
      "step: 4881 loss: 0.025\n",
      "step: 4882 loss: 0.032\n",
      "step: 4883 loss: 0.053\n",
      "step: 4884 loss: 0.067\n",
      "step: 4885 loss: 0.041\n",
      "step: 4886 loss: 0.036\n",
      "step: 4887 loss: 0.020\n",
      "step: 4888 loss: 0.059\n",
      "step: 4889 loss: 0.027\n",
      "step: 4890 loss: 0.026\n",
      "step: 4891 loss: 0.046\n",
      "step: 4892 loss: 0.034\n",
      "step: 4893 loss: 0.035\n",
      "step: 4894 loss: 0.051\n",
      "step: 4895 loss: 0.043\n",
      "step: 4896 loss: 0.029\n",
      "step: 4897 loss: 0.037\n",
      "step: 4898 loss: 0.050\n",
      "step: 4899 loss: 0.033\n",
      "step: 4900 loss: 0.033\n",
      "step: 4901 loss: 0.036\n",
      "step: 4902 loss: 0.047\n",
      "step: 4903 loss: 0.046\n",
      "step: 4904 loss: 0.040\n",
      "step: 4905 loss: 0.029\n",
      "step: 4906 loss: 0.049\n",
      "step: 4907 loss: 0.037\n",
      "step: 4908 loss: 0.048\n",
      "step: 4909 loss: 0.040\n",
      "step: 4910 loss: 0.037\n",
      "step: 4911 loss: 0.052\n",
      "step: 4912 loss: 0.036\n",
      "step: 4913 loss: 0.036\n",
      "step: 4914 loss: 0.027\n",
      "step: 4915 loss: 0.039\n",
      "step: 4916 loss: 0.035\n",
      "step: 4917 loss: 0.055\n",
      "step: 4918 loss: 0.029\n",
      "step: 4919 loss: 0.031\n",
      "step: 4920 loss: 0.023\n",
      "step: 4921 loss: 0.043\n",
      "step: 4922 loss: 0.026\n",
      "step: 4923 loss: 0.038\n",
      "step: 4924 loss: 0.022\n",
      "step: 4925 loss: 0.035\n",
      "step: 4926 loss: 0.024\n",
      "step: 4927 loss: 0.037\n",
      "step: 4928 loss: 0.057\n",
      "step: 4929 loss: 0.037\n",
      "step: 4930 loss: 0.054\n",
      "step: 4931 loss: 0.043\n",
      "step: 4932 loss: 0.022\n",
      "step: 4933 loss: 0.028\n",
      "step: 4934 loss: 0.027\n",
      "step: 4935 loss: 0.028\n",
      "step: 4936 loss: 0.021\n",
      "step: 4937 loss: 0.036\n",
      "step: 4938 loss: 0.034\n",
      "step: 4939 loss: 0.049\n",
      "step: 4940 loss: 0.043\n",
      "step: 4941 loss: 0.033\n",
      "step: 4942 loss: 0.042\n",
      "step: 4943 loss: 0.041\n",
      "step: 4944 loss: 0.046\n",
      "step: 4945 loss: 0.040\n",
      "step: 4946 loss: 0.028\n",
      "step: 4947 loss: 0.037\n",
      "step: 4948 loss: 0.038\n",
      "step: 4949 loss: 0.032\n",
      "step: 4950 loss: 0.023\n",
      "step: 4951 loss: 0.025\n",
      "step: 4952 loss: 0.032\n",
      "step: 4953 loss: 0.037\n",
      "step: 4954 loss: 0.031\n",
      "step: 4955 loss: 0.037\n",
      "step: 4956 loss: 0.034\n",
      "step: 4957 loss: 0.046\n",
      "step: 4958 loss: 0.032\n",
      "step: 4959 loss: 0.025\n",
      "step: 4960 loss: 0.025\n",
      "step: 4961 loss: 0.038\n",
      "step: 4962 loss: 0.040\n",
      "step: 4963 loss: 0.040\n",
      "step: 4964 loss: 0.035\n",
      "step: 4965 loss: 0.023\n",
      "step: 4966 loss: 0.040\n",
      "step: 4967 loss: 0.049\n",
      "step: 4968 loss: 0.035\n",
      "step: 4969 loss: 0.043\n",
      "step: 4970 loss: 0.041\n",
      "step: 4971 loss: 0.026\n",
      "step: 4972 loss: 0.031\n",
      "step: 4973 loss: 0.041\n",
      "step: 4974 loss: 0.029\n",
      "step: 4975 loss: 0.037\n",
      "step: 4976 loss: 0.045\n",
      "step: 4977 loss: 0.025\n",
      "step: 4978 loss: 0.039\n",
      "step: 4979 loss: 0.035\n",
      "step: 4980 loss: 0.044\n",
      "step: 4981 loss: 0.028\n",
      "step: 4982 loss: 0.045\n",
      "step: 4983 loss: 0.044\n",
      "step: 4984 loss: 0.035\n",
      "step: 4985 loss: 0.041\n",
      "step: 4986 loss: 0.041\n",
      "step: 4987 loss: 0.046\n",
      "step: 4988 loss: 0.052\n",
      "step: 4989 loss: 0.033\n",
      "step: 4990 loss: 0.031\n",
      "step: 4991 loss: 0.054\n",
      "step: 4992 loss: 0.050\n",
      "step: 4993 loss: 0.041\n",
      "step: 4994 loss: 0.068\n",
      "step: 4995 loss: 0.053\n",
      "step: 4996 loss: 0.050\n",
      "step: 4997 loss: 0.036\n",
      "step: 4998 loss: 0.024\n",
      "step: 4999 loss: 0.031\n"
     ]
    }
   ],
   "source": [
    "# Run training loop.\n",
    "step = 0\n",
    "done = False\n",
    "while not done:\n",
    "    for batch in dataloader:\n",
    "        batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in batch.items()}\n",
    "        loss, _ = policy.forward(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % log_freq == 0:\n",
    "            print(f\"step: {step} loss: {loss.item():.3f}\")\n",
    "        step += 1\n",
    "        if step >= training_steps:\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "# Save a policy checkpoint.\n",
    "policy.save_pretrained(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
