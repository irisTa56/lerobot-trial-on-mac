{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi0 Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ALOHA simulation environment to try out the following approaches:\n",
    "\n",
    "1. Use the Hugging Face version (i.e., PyTorch version) of `pi0_base` (general-purpose model)\n",
    "2. Fine-tune the Hugging Face version of `pi0_base`, then use it\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "- Blog post on porting the model to LeRobot:\n",
    "  - https://huggingface.co/blog/pi0\n",
    "- Visualization of the transfer cube task:\n",
    "  - https://lerobot-visualize-dataset.hf.space/lerobot/aloha_sim_transfer_cube_human\n",
    "- Scripts to convert the model from Jax to PyTorch:\n",
    "  - https://github.com/huggingface/lerobot/blob/main/lerobot/common/policies/pi0/conversion_scripts\n",
    "- Training configuration of `pi0_aloha_sim`:\n",
    "  - [https://github.com/Physical-Intelligence/openpi/blob/main/src/openpi/training/config.py#L616-L627](https://github.com/Physical-Intelligence/openpi/blob/581e07d73af36d336cef1ec9d7172553b2332193/src/openpi/training/config.py#L616-L627)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pprint  # noqa: F401\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.display import Video\n",
    "from huggingface_hub import whoami, interpreter_login\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
    "from lerobot.common.envs.factory import make_env_config, make_env\n",
    "from lerobot.common.envs.utils import preprocess_observation\n",
    "from lerobot.common.policies.factory import make_policy_config, make_policy\n",
    "from lerobot.common.utils.io_utils import write_video\n",
    "from lerobot.common.utils.utils import init_logging\n",
    "from lerobot.configs import parser\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to acknowledge PaliGemma's license and login to Hugging Face.\n",
    "# To acknowledge, visit https://huggingface.co/google/paligemma-3b-pt-224\n",
    "\n",
    "try:\n",
    "    assert whoami()\n",
    "except Exception:\n",
    "    interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Hugging Face Version of `pi0_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store outputs.\n",
    "output_dir = Path(\"outputs/eval/example_aloha_pi0\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_cfg = make_policy_config(\n",
    "    \"pi0\",\n",
    "    device=\"mps\",\n",
    "    adapt_to_pi_aloha=True,\n",
    "    use_delta_joint_actions_aloha=False,\n",
    ")\n",
    "\n",
    "# Load the pretrained model from Hugging Face.\n",
    "policy_cfg.pretrained_path = \"lerobot/pi0\"\n",
    "\n",
    "ds_meta = LeRobotDatasetMetadata(\"lerobot/aloha_sim_transfer_cube_human\")\n",
    "\n",
    "policy = make_policy(policy_cfg, ds_meta=ds_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-03-17 11:16:39 /__init__.py:88 MUJOCO_GL is not set, so an OpenGL backend will be chosen automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-03-17 11:16:39 /__init__.py:96 Successfully imported OpenGL backend: %s\n",
      "INFO 2025-03-17 11:16:39 /__init__.py:31 MuJoCo library version is: %s\n"
     ]
    }
   ],
   "source": [
    "env_cfg = make_env_config(\n",
    "    \"aloha\",\n",
    "    task=\"AlohaTransferCube-v0\",\n",
    "    episode_length=100,\n",
    ")\n",
    "\n",
    "env = make_env(env_cfg)  # batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation.images.top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(14,))}\n",
      "Dict('agent_pos': Box(-1000.0, 1000.0, (1, 14), float64), 'pixels': Dict('top': Box(0, 255, (1, 480, 640, 3), uint8)))\n"
     ]
    }
   ],
   "source": [
    "# Verify compatibility of the input shape between the policy and the environment.\n",
    "print(policy.config.input_features)  # channel first\n",
    "print(env.observation_space)  # channel last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(14,))}\n",
      "Box(-1.0, 1.0, (1, 14), float32)\n"
     ]
    }
   ],
   "source": [
    "# Verify compatibility of the output shape between the policy and the environment.\n",
    "print(policy.config.output_features)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pick up the cube with the right arm and transfer it to the left arm.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_meta.tasks[0]  # language instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_env_observation_to_policy_observation(\n",
    "    observation: dict[str, Tensor],\n",
    ") -> dict[str, Tensor | list[str]]:\n",
    "    preprocessed = preprocess_observation(observation)\n",
    "    converted = {k: v.to(\"mps\") for k, v in preprocessed.items()}\n",
    "    converted[\"task\"] = [ds_meta.tasks[0]]  # batch size 1\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1 reward=0.000000\n",
      "step=2 reward=0.000000\n",
      "step=3 reward=0.000000\n",
      "step=4 reward=3.000000\n",
      "step=5 reward=3.000000\n",
      "step=6 reward=3.000000\n",
      "step=7 reward=0.000000\n",
      "step=8 reward=0.000000\n",
      "step=9 reward=4.000000\n",
      "step=10 reward=0.000000\n",
      "step=11 reward=0.000000\n",
      "step=12 reward=0.000000\n",
      "step=13 reward=0.000000\n",
      "step=14 reward=0.000000\n",
      "step=15 reward=0.000000\n",
      "step=16 reward=0.000000\n",
      "step=17 reward=0.000000\n",
      "step=18 reward=0.000000\n",
      "step=19 reward=0.000000\n",
      "step=20 reward=0.000000\n",
      "step=21 reward=0.000000\n",
      "step=22 reward=0.000000\n",
      "step=23 reward=0.000000\n",
      "step=24 reward=0.000000\n",
      "step=25 reward=0.000000\n",
      "step=26 reward=0.000000\n",
      "step=27 reward=0.000000\n",
      "step=28 reward=0.000000\n",
      "step=29 reward=0.000000\n",
      "step=30 reward=0.000000\n",
      "step=31 reward=0.000000\n",
      "step=32 reward=0.000000\n",
      "step=33 reward=0.000000\n",
      "step=34 reward=0.000000\n",
      "step=35 reward=0.000000\n",
      "step=36 reward=0.000000\n",
      "step=37 reward=0.000000\n",
      "step=38 reward=0.000000\n",
      "step=39 reward=0.000000\n",
      "step=40 reward=0.000000\n",
      "step=41 reward=0.000000\n",
      "step=42 reward=0.000000\n",
      "step=43 reward=0.000000\n",
      "step=44 reward=0.000000\n",
      "step=45 reward=0.000000\n",
      "step=46 reward=0.000000\n",
      "step=47 reward=0.000000\n",
      "step=48 reward=0.000000\n",
      "step=49 reward=0.000000\n",
      "step=50 reward=0.000000\n",
      "step=51 reward=0.000000\n",
      "step=52 reward=0.000000\n",
      "step=53 reward=0.000000\n",
      "step=54 reward=0.000000\n",
      "step=55 reward=0.000000\n",
      "step=56 reward=0.000000\n",
      "step=57 reward=0.000000\n",
      "step=58 reward=0.000000\n",
      "step=59 reward=0.000000\n",
      "step=60 reward=0.000000\n",
      "step=61 reward=0.000000\n",
      "step=62 reward=0.000000\n",
      "step=63 reward=0.000000\n",
      "step=64 reward=0.000000\n",
      "step=65 reward=0.000000\n",
      "step=66 reward=0.000000\n",
      "step=67 reward=0.000000\n",
      "step=68 reward=0.000000\n",
      "step=69 reward=0.000000\n",
      "step=70 reward=0.000000\n",
      "step=71 reward=0.000000\n",
      "step=72 reward=0.000000\n",
      "step=73 reward=0.000000\n",
      "step=74 reward=0.000000\n",
      "step=75 reward=0.000000\n",
      "step=76 reward=0.000000\n",
      "step=77 reward=0.000000\n",
      "step=78 reward=0.000000\n",
      "step=79 reward=0.000000\n",
      "step=80 reward=0.000000\n",
      "step=81 reward=0.000000\n",
      "step=82 reward=0.000000\n",
      "step=83 reward=0.000000\n",
      "step=84 reward=0.000000\n",
      "step=85 reward=0.000000\n",
      "step=86 reward=0.000000\n",
      "step=87 reward=0.000000\n",
      "step=88 reward=0.000000\n",
      "step=89 reward=0.000000\n",
      "step=90 reward=0.000000\n",
      "step=91 reward=0.000000\n",
      "step=92 reward=0.000000\n",
      "step=93 reward=0.000000\n",
      "step=94 reward=0.000000\n",
      "step=95 reward=0.000000\n",
      "step=96 reward=0.000000\n",
      "step=97 reward=0.000000\n",
      "step=98 reward=0.000000\n",
      "step=99 reward=0.000000\n",
      "step=100 reward=0.000000\n",
      "step=101 reward=0.000000\n",
      "step=102 reward=0.000000\n",
      "step=103 reward=0.000000\n",
      "step=104 reward=0.000000\n",
      "step=105 reward=0.000000\n",
      "step=106 reward=0.000000\n",
      "step=107 reward=0.000000\n",
      "step=108 reward=0.000000\n",
      "step=109 reward=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Reset the policy and environments to prepare for rollout.\n",
    "policy.reset()\n",
    "observation, _info = env.reset(seed=42)\n",
    "\n",
    "rewards = []\n",
    "frames = [env.envs[0].render()]  # with the initial frame\n",
    "\n",
    "for step in itertools.count(1):\n",
    "    observation = convert_env_observation_to_policy_observation(observation)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment.\n",
    "    action = action.to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation.\n",
    "    observation, [reward], _terminated, [truncated], _info = env.step(action)\n",
    "    print(f\"{step=} {reward=:f}\")\n",
    "\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.envs[0].render())\n",
    "\n",
    "    if truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = output_dir / \"rollout.mp4\"\n",
    "write_video(video_path, frames, env.metadata[\"render_fps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/eval/example_aloha_pi0/rollout.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Hugging Face Version of `pi0_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: Populate the config object by modifying sys.argv\n",
    "@parser.wrap()\n",
    "def load_config(config: TrainPipelineConfig):\n",
    "    return config\n",
    "\n",
    "\n",
    "sys.argv = sys.argv[:1] + [\n",
    "    \"--policy.path=lerobot/pi0\",\n",
    "    \"--policy.device=mps\",\n",
    "    \"--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human\",\n",
    "    \"--env.type=aloha\",\n",
    "    \"--env.task=AlohaTransferCube-v0\",\n",
    "    \"--batch_size=4\",  # 8 (default) seems to be too large for 32GB M3\n",
    "    \"--steps=2000\",\n",
    "    \"--log_freq=100\",\n",
    "    \"--wandb.enable=true\",  # `uv run wandb login` is required\n",
    "]\n",
    "\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human',\n",
      "                                          root=None,\n",
      "                                          episodes=None,\n",
      "                                          image_transforms=ImageTransformsConfig(enable=False,\n",
      "                                                                                 max_num_transforms=3,\n",
      "                                                                                 random_order=False,\n",
      "                                                                                 tfs={'brightness': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                         type='ColorJitter',\n",
      "                                                                                                                         kwargs={'brightness': (0.8,\n",
      "                                                                                                                                                1.2)}),\n",
      "                                                                                      'contrast': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                       type='ColorJitter',\n",
      "                                                                                                                       kwargs={'contrast': (0.8,\n",
      "                                                                                                                                            1.2)}),\n",
      "                                                                                      'saturation': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                         type='ColorJitter',\n",
      "                                                                                                                         kwargs={'saturation': (0.5,\n",
      "                                                                                                                                                1.5)}),\n",
      "                                                                                      'hue': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                  type='ColorJitter',\n",
      "                                                                                                                  kwargs={'hue': (-0.05,\n",
      "                                                                                                                                  0.05)}),\n",
      "                                                                                      'sharpness': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                        type='SharpnessJitter',\n",
      "                                                                                                                        kwargs={'sharpness': (0.5,\n",
      "                                                                                                                                              1.5)})}),\n",
      "                                          revision=None,\n",
      "                                          use_imagenet_stats=True,\n",
      "                                          video_backend='pyav'),\n",
      "                    env=AlohaEnv(task='AlohaTransferCube-v0',\n",
      "                                 fps=50,\n",
      "                                 features={'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>,\n",
      "                                                                   shape=(14,)),\n",
      "                                           'agent_pos': PolicyFeature(type=<FeatureType.STATE: 'STATE'>,\n",
      "                                                                      shape=(14,)),\n",
      "                                           'pixels/top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>,\n",
      "                                                                       shape=(480,\n",
      "                                                                              640,\n",
      "                                                                              3))},\n",
      "                                 features_map={'action': 'action',\n",
      "                                               'agent_pos': 'observation.state',\n",
      "                                               'top': 'observation.image.top',\n",
      "                                               'pixels/top': 'observation.images.top'},\n",
      "                                 episode_length=400,\n",
      "                                 obs_type='pixels_agent_pos',\n",
      "                                 render_mode='rgb_array'),\n",
      "                    policy=None,\n",
      "                    output_dir=None,\n",
      "                    job_name=None,\n",
      "                    resume=False,\n",
      "                    seed=1000,\n",
      "                    num_workers=4,\n",
      "                    batch_size=4,\n",
      "                    steps=2000,\n",
      "                    eval_freq=20000,\n",
      "                    log_freq=100,\n",
      "                    save_checkpoint=True,\n",
      "                    save_freq=20000,\n",
      "                    use_policy_training_preset=True,\n",
      "                    optimizer=None,\n",
      "                    scheduler=None,\n",
      "                    eval=EvalConfig(n_episodes=50,\n",
      "                                    batch_size=50,\n",
      "                                    use_async_envs=False),\n",
      "                    wandb=WandBConfig(enable=True,\n",
      "                                      disable_artifact=False,\n",
      "                                      project='lerobot',\n",
      "                                      entity=None,\n",
      "                                      notes=None,\n",
      "                                      run_id=None))\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(config)\n",
    "# train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
