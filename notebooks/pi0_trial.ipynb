{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi0 Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ALOHA simulation environment to try out the following approaches:\n",
    "\n",
    "1. Use the Hugging Face version (i.e., PyTorch version) of `pi0_base` (general-purpose model)\n",
    "2. Fine-tune the Hugging Face version of `pi0_base`, then use it\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "- Blog post on porting the model to LeRobot:\n",
    "  - https://huggingface.co/blog/pi0\n",
    "- Visualization of the transfer cube task:\n",
    "  - https://lerobot-visualize-dataset.hf.space/lerobot/aloha_sim_transfer_cube_human\n",
    "- Scripts to convert the model from Jax to PyTorch:\n",
    "  - https://github.com/huggingface/lerobot/blob/main/lerobot/common/policies/pi0/conversion_scripts\n",
    "- Training configuration of `pi0_aloha_sim`:\n",
    "  - [https://github.com/Physical-Intelligence/openpi/blob/main/src/openpi/training/config.py#L616-L627](https://github.com/Physical-Intelligence/openpi/blob/581e07d73af36d336cef1ec9d7172553b2332193/src/openpi/training/config.py#L616-L627)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import pprint  # noqa: F401\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from IPython.display import Video\n",
    "from huggingface_hub import whoami, interpreter_login\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
    "from lerobot.common.envs.factory import make_env_config, make_env\n",
    "from lerobot.common.envs.utils import preprocess_observation\n",
    "from lerobot.common.policies.factory import make_policy_config, make_policy\n",
    "from lerobot.common.utils.io_utils import write_video\n",
    "from lerobot.common.utils.utils import init_logging\n",
    "from lerobot.configs import parser\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from lerobot.scripts.train import train  # noqa: F401\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to https://github.com/huggingface/transformers/issues/5486\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to acknowledge PaliGemma's license and login to Hugging Face.\n",
    "# To acknowledge, visit https://huggingface.co/google/paligemma-3b-pt-224\n",
    "\n",
    "try:\n",
    "    assert whoami()\n",
    "except Exception:\n",
    "    interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Hugging Face Version of `pi0_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store outputs.\n",
    "output_dir = Path(\"outputs/eval/example_aloha_pi0\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_cfg = make_policy_config(\n",
    "    \"pi0\",\n",
    "    device=\"mps\",\n",
    "    # From \"pi0_aloha_sim\" setting in convert_pi0_to_hf_lerobot.py\n",
    "    empty_cameras=2,\n",
    "    adapt_to_pi_aloha=True,\n",
    "    use_delta_joint_actions_aloha=False,\n",
    ")\n",
    "\n",
    "# Load the pretrained model from Hugging Face.\n",
    "policy_cfg.pretrained_path = \"lerobot/pi0\"\n",
    "\n",
    "ds_meta = LeRobotDatasetMetadata(\"lerobot/aloha_sim_transfer_cube_human\")\n",
    "\n",
    "policy = make_policy(policy_cfg, ds_meta=ds_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-03-18 18:03:27 /__init__.py:88 MUJOCO_GL is not set, so an OpenGL backend will be chosen automatically.\n",
      "INFO 2025-03-18 18:03:28 /__init__.py:96 Successfully imported OpenGL backend: %s\n",
      "INFO 2025-03-18 18:03:28 /__init__.py:31 MuJoCo library version is: %s\n"
     ]
    }
   ],
   "source": [
    "env_cfg = make_env_config(\n",
    "    \"aloha\",\n",
    "    task=\"AlohaTransferCube-v0\",\n",
    "    # episode_length=100,\n",
    ")\n",
    "\n",
    "env = make_env(env_cfg)  # batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation.images.top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>, shape=(3, 480, 640)), 'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(14,))}\n",
      "Dict('agent_pos': Box(-1000.0, 1000.0, (1, 14), float64), 'pixels': Dict('top': Box(0, 255, (1, 480, 640, 3), uint8)))\n"
     ]
    }
   ],
   "source": [
    "# Verify compatibility of the input shape between the policy and the environment.\n",
    "input_features_without_empty_camera = {\n",
    "    k: v for k, v in policy.config.input_features.items() if \"empty_camera\" not in k\n",
    "}\n",
    "print(input_features_without_empty_camera)  # channel first\n",
    "print(env.observation_space)  # channel last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(14,))}\n",
      "Box(-1.0, 1.0, (1, 14), float32)\n"
     ]
    }
   ],
   "source": [
    "# Verify compatibility of the output shape between the policy and the environment.\n",
    "print(policy.config.output_features)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pick up the cube with the right arm and transfer it to the left arm.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_meta.tasks[0]  # language instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_env_observation_to_policy_observation(\n",
    "    observation: dict[str, Tensor],\n",
    ") -> dict[str, Tensor | list[str]]:\n",
    "    preprocessed = preprocess_observation(observation)\n",
    "    converted = {k: v.to(\"mps\") for k, v in preprocessed.items()}\n",
    "    converted[\"task\"] = [ds_meta.tasks[0]]  # batch size 1\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=1 reward=0.000000\n",
      "step=2 reward=0.000000\n",
      "step=3 reward=0.000000\n",
      "step=4 reward=0.000000\n",
      "step=5 reward=4.000000\n",
      "step=6 reward=0.000000\n",
      "step=7 reward=0.000000\n",
      "step=8 reward=0.000000\n",
      "step=9 reward=0.000000\n",
      "step=10 reward=0.000000\n",
      "step=11 reward=0.000000\n",
      "step=12 reward=0.000000\n",
      "step=13 reward=0.000000\n",
      "step=14 reward=4.000000\n",
      "step=15 reward=0.000000\n",
      "step=16 reward=0.000000\n",
      "step=17 reward=0.000000\n",
      "step=18 reward=0.000000\n",
      "step=19 reward=0.000000\n",
      "step=20 reward=0.000000\n",
      "step=21 reward=0.000000\n",
      "step=22 reward=0.000000\n",
      "step=23 reward=0.000000\n",
      "step=24 reward=0.000000\n",
      "step=25 reward=0.000000\n",
      "step=26 reward=0.000000\n",
      "step=27 reward=0.000000\n",
      "step=28 reward=0.000000\n",
      "step=29 reward=0.000000\n",
      "step=30 reward=0.000000\n",
      "step=31 reward=0.000000\n",
      "step=32 reward=0.000000\n",
      "step=33 reward=0.000000\n",
      "step=34 reward=0.000000\n",
      "step=35 reward=0.000000\n",
      "step=36 reward=0.000000\n",
      "step=37 reward=0.000000\n",
      "step=38 reward=0.000000\n",
      "step=39 reward=0.000000\n",
      "step=40 reward=0.000000\n",
      "step=41 reward=0.000000\n",
      "step=42 reward=0.000000\n",
      "step=43 reward=0.000000\n",
      "step=44 reward=0.000000\n",
      "step=45 reward=0.000000\n",
      "step=46 reward=0.000000\n",
      "step=47 reward=0.000000\n",
      "step=48 reward=0.000000\n",
      "step=49 reward=0.000000\n",
      "step=50 reward=0.000000\n",
      "step=51 reward=0.000000\n",
      "step=52 reward=0.000000\n",
      "step=53 reward=0.000000\n",
      "step=54 reward=0.000000\n",
      "step=55 reward=0.000000\n",
      "step=56 reward=0.000000\n",
      "step=57 reward=0.000000\n",
      "step=58 reward=0.000000\n",
      "step=59 reward=0.000000\n",
      "step=60 reward=0.000000\n",
      "step=61 reward=0.000000\n",
      "step=62 reward=0.000000\n",
      "step=63 reward=0.000000\n",
      "step=64 reward=0.000000\n",
      "step=65 reward=0.000000\n",
      "step=66 reward=0.000000\n",
      "step=67 reward=0.000000\n",
      "step=68 reward=0.000000\n",
      "step=69 reward=0.000000\n",
      "step=70 reward=0.000000\n",
      "step=71 reward=0.000000\n",
      "step=72 reward=0.000000\n",
      "step=73 reward=0.000000\n",
      "step=74 reward=0.000000\n",
      "step=75 reward=0.000000\n",
      "step=76 reward=0.000000\n",
      "step=77 reward=0.000000\n",
      "step=78 reward=0.000000\n",
      "step=79 reward=0.000000\n",
      "step=80 reward=0.000000\n",
      "step=81 reward=0.000000\n",
      "step=82 reward=0.000000\n",
      "step=83 reward=0.000000\n",
      "step=84 reward=0.000000\n",
      "step=85 reward=0.000000\n",
      "step=86 reward=0.000000\n",
      "step=87 reward=0.000000\n",
      "step=88 reward=0.000000\n",
      "step=89 reward=0.000000\n",
      "step=90 reward=0.000000\n",
      "step=91 reward=0.000000\n",
      "step=92 reward=0.000000\n",
      "step=93 reward=0.000000\n",
      "step=94 reward=0.000000\n",
      "step=95 reward=0.000000\n",
      "step=96 reward=0.000000\n",
      "step=97 reward=0.000000\n",
      "step=98 reward=0.000000\n",
      "step=99 reward=0.000000\n",
      "step=100 reward=0.000000\n",
      "step=101 reward=0.000000\n",
      "step=102 reward=0.000000\n",
      "step=103 reward=0.000000\n",
      "step=104 reward=0.000000\n",
      "step=105 reward=0.000000\n",
      "step=106 reward=0.000000\n",
      "step=107 reward=0.000000\n",
      "step=108 reward=0.000000\n",
      "step=109 reward=0.000000\n",
      "step=110 reward=0.000000\n",
      "step=111 reward=0.000000\n",
      "step=112 reward=0.000000\n",
      "step=113 reward=0.000000\n",
      "step=114 reward=0.000000\n",
      "step=115 reward=0.000000\n",
      "step=116 reward=0.000000\n",
      "step=117 reward=0.000000\n",
      "step=118 reward=0.000000\n",
      "step=119 reward=0.000000\n",
      "step=120 reward=0.000000\n",
      "step=121 reward=0.000000\n",
      "step=122 reward=0.000000\n",
      "step=123 reward=0.000000\n",
      "step=124 reward=0.000000\n",
      "step=125 reward=0.000000\n",
      "step=126 reward=0.000000\n",
      "step=127 reward=0.000000\n",
      "step=128 reward=0.000000\n",
      "step=129 reward=0.000000\n",
      "step=130 reward=0.000000\n",
      "step=131 reward=0.000000\n",
      "step=132 reward=0.000000\n",
      "step=133 reward=0.000000\n",
      "step=134 reward=0.000000\n",
      "step=135 reward=0.000000\n",
      "step=136 reward=0.000000\n",
      "step=137 reward=0.000000\n",
      "step=138 reward=0.000000\n",
      "step=139 reward=0.000000\n",
      "step=140 reward=0.000000\n",
      "step=141 reward=0.000000\n",
      "step=142 reward=0.000000\n",
      "step=143 reward=0.000000\n",
      "step=144 reward=0.000000\n",
      "step=145 reward=0.000000\n",
      "step=146 reward=0.000000\n",
      "step=147 reward=0.000000\n",
      "step=148 reward=0.000000\n",
      "step=149 reward=0.000000\n",
      "step=150 reward=0.000000\n",
      "step=151 reward=0.000000\n",
      "step=152 reward=0.000000\n",
      "step=153 reward=0.000000\n",
      "step=154 reward=0.000000\n",
      "step=155 reward=0.000000\n",
      "step=156 reward=0.000000\n",
      "step=157 reward=0.000000\n",
      "step=158 reward=0.000000\n",
      "step=159 reward=0.000000\n",
      "step=160 reward=0.000000\n",
      "step=161 reward=0.000000\n",
      "step=162 reward=0.000000\n",
      "step=163 reward=0.000000\n",
      "step=164 reward=0.000000\n",
      "step=165 reward=0.000000\n",
      "step=166 reward=0.000000\n",
      "step=167 reward=0.000000\n",
      "step=168 reward=0.000000\n",
      "step=169 reward=0.000000\n",
      "step=170 reward=0.000000\n",
      "step=171 reward=0.000000\n",
      "step=172 reward=0.000000\n",
      "step=173 reward=0.000000\n",
      "step=174 reward=0.000000\n",
      "step=175 reward=0.000000\n",
      "step=176 reward=0.000000\n",
      "step=177 reward=0.000000\n",
      "step=178 reward=0.000000\n",
      "step=179 reward=0.000000\n",
      "step=180 reward=0.000000\n",
      "step=181 reward=0.000000\n",
      "step=182 reward=0.000000\n",
      "step=183 reward=0.000000\n",
      "step=184 reward=0.000000\n",
      "step=185 reward=0.000000\n",
      "step=186 reward=0.000000\n",
      "step=187 reward=0.000000\n",
      "step=188 reward=0.000000\n",
      "step=189 reward=0.000000\n",
      "step=190 reward=0.000000\n",
      "step=191 reward=0.000000\n",
      "step=192 reward=0.000000\n",
      "step=193 reward=0.000000\n",
      "step=194 reward=0.000000\n",
      "step=195 reward=0.000000\n",
      "step=196 reward=0.000000\n",
      "step=197 reward=0.000000\n",
      "step=198 reward=0.000000\n",
      "step=199 reward=0.000000\n",
      "step=200 reward=0.000000\n",
      "step=201 reward=0.000000\n",
      "step=202 reward=0.000000\n",
      "step=203 reward=0.000000\n",
      "step=204 reward=0.000000\n",
      "step=205 reward=0.000000\n",
      "step=206 reward=0.000000\n",
      "step=207 reward=0.000000\n",
      "step=208 reward=0.000000\n",
      "step=209 reward=0.000000\n",
      "step=210 reward=0.000000\n",
      "step=211 reward=0.000000\n",
      "step=212 reward=0.000000\n",
      "step=213 reward=0.000000\n",
      "step=214 reward=0.000000\n",
      "step=215 reward=0.000000\n",
      "step=216 reward=0.000000\n",
      "step=217 reward=0.000000\n",
      "step=218 reward=0.000000\n",
      "step=219 reward=0.000000\n",
      "step=220 reward=0.000000\n",
      "step=221 reward=0.000000\n",
      "step=222 reward=0.000000\n",
      "step=223 reward=0.000000\n",
      "step=224 reward=0.000000\n",
      "step=225 reward=0.000000\n",
      "step=226 reward=0.000000\n",
      "step=227 reward=0.000000\n",
      "step=228 reward=0.000000\n",
      "step=229 reward=0.000000\n",
      "step=230 reward=0.000000\n",
      "step=231 reward=0.000000\n",
      "step=232 reward=0.000000\n",
      "step=233 reward=0.000000\n",
      "step=234 reward=0.000000\n",
      "step=235 reward=0.000000\n",
      "step=236 reward=0.000000\n",
      "step=237 reward=0.000000\n",
      "step=238 reward=0.000000\n",
      "step=239 reward=0.000000\n",
      "step=240 reward=0.000000\n",
      "step=241 reward=0.000000\n",
      "step=242 reward=0.000000\n",
      "step=243 reward=0.000000\n",
      "step=244 reward=0.000000\n",
      "step=245 reward=0.000000\n",
      "step=246 reward=0.000000\n",
      "step=247 reward=0.000000\n",
      "step=248 reward=0.000000\n",
      "step=249 reward=0.000000\n",
      "step=250 reward=0.000000\n",
      "step=251 reward=0.000000\n",
      "step=252 reward=0.000000\n",
      "step=253 reward=0.000000\n",
      "step=254 reward=0.000000\n",
      "step=255 reward=0.000000\n",
      "step=256 reward=0.000000\n",
      "step=257 reward=0.000000\n",
      "step=258 reward=0.000000\n",
      "step=259 reward=0.000000\n",
      "step=260 reward=0.000000\n",
      "step=261 reward=0.000000\n",
      "step=262 reward=0.000000\n",
      "step=263 reward=0.000000\n",
      "step=264 reward=0.000000\n",
      "step=265 reward=0.000000\n",
      "step=266 reward=0.000000\n",
      "step=267 reward=0.000000\n",
      "step=268 reward=0.000000\n",
      "step=269 reward=0.000000\n",
      "step=270 reward=0.000000\n",
      "step=271 reward=0.000000\n",
      "step=272 reward=0.000000\n",
      "step=273 reward=0.000000\n",
      "step=274 reward=0.000000\n",
      "step=275 reward=0.000000\n",
      "step=276 reward=0.000000\n",
      "step=277 reward=0.000000\n",
      "step=278 reward=0.000000\n",
      "step=279 reward=0.000000\n",
      "step=280 reward=0.000000\n",
      "step=281 reward=0.000000\n",
      "step=282 reward=0.000000\n",
      "step=283 reward=0.000000\n",
      "step=284 reward=0.000000\n",
      "step=285 reward=0.000000\n",
      "step=286 reward=0.000000\n",
      "step=287 reward=0.000000\n",
      "step=288 reward=0.000000\n",
      "step=289 reward=0.000000\n",
      "step=290 reward=0.000000\n",
      "step=291 reward=0.000000\n",
      "step=292 reward=0.000000\n",
      "step=293 reward=0.000000\n",
      "step=294 reward=0.000000\n",
      "step=295 reward=0.000000\n",
      "step=296 reward=0.000000\n",
      "step=297 reward=0.000000\n",
      "step=298 reward=0.000000\n",
      "step=299 reward=0.000000\n",
      "step=300 reward=0.000000\n",
      "step=301 reward=0.000000\n",
      "step=302 reward=0.000000\n",
      "step=303 reward=0.000000\n",
      "step=304 reward=0.000000\n",
      "step=305 reward=0.000000\n",
      "step=306 reward=0.000000\n",
      "step=307 reward=0.000000\n",
      "step=308 reward=0.000000\n",
      "step=309 reward=0.000000\n",
      "step=310 reward=0.000000\n",
      "step=311 reward=0.000000\n",
      "step=312 reward=0.000000\n",
      "step=313 reward=0.000000\n",
      "step=314 reward=0.000000\n",
      "step=315 reward=0.000000\n",
      "step=316 reward=0.000000\n",
      "step=317 reward=0.000000\n",
      "step=318 reward=0.000000\n",
      "step=319 reward=0.000000\n",
      "step=320 reward=0.000000\n",
      "step=321 reward=0.000000\n",
      "step=322 reward=0.000000\n",
      "step=323 reward=0.000000\n",
      "step=324 reward=0.000000\n",
      "step=325 reward=0.000000\n",
      "step=326 reward=0.000000\n",
      "step=327 reward=0.000000\n",
      "step=328 reward=0.000000\n",
      "step=329 reward=0.000000\n",
      "step=330 reward=0.000000\n",
      "step=331 reward=0.000000\n",
      "step=332 reward=0.000000\n",
      "step=333 reward=0.000000\n",
      "step=334 reward=0.000000\n",
      "step=335 reward=0.000000\n",
      "step=336 reward=0.000000\n",
      "step=337 reward=0.000000\n",
      "step=338 reward=0.000000\n",
      "step=339 reward=0.000000\n",
      "step=340 reward=0.000000\n",
      "step=341 reward=0.000000\n",
      "step=342 reward=0.000000\n",
      "step=343 reward=0.000000\n",
      "step=344 reward=0.000000\n",
      "step=345 reward=0.000000\n",
      "step=346 reward=0.000000\n",
      "step=347 reward=0.000000\n",
      "step=348 reward=0.000000\n",
      "step=349 reward=0.000000\n",
      "step=350 reward=0.000000\n",
      "step=351 reward=0.000000\n",
      "step=352 reward=0.000000\n",
      "step=353 reward=0.000000\n",
      "step=354 reward=0.000000\n",
      "step=355 reward=0.000000\n",
      "step=356 reward=0.000000\n",
      "step=357 reward=0.000000\n",
      "step=358 reward=0.000000\n",
      "step=359 reward=0.000000\n",
      "step=360 reward=0.000000\n",
      "step=361 reward=0.000000\n",
      "step=362 reward=0.000000\n",
      "step=363 reward=0.000000\n",
      "step=364 reward=0.000000\n",
      "step=365 reward=0.000000\n",
      "step=366 reward=0.000000\n",
      "step=367 reward=0.000000\n",
      "step=368 reward=0.000000\n",
      "step=369 reward=0.000000\n",
      "step=370 reward=0.000000\n",
      "step=371 reward=0.000000\n",
      "step=372 reward=0.000000\n",
      "step=373 reward=0.000000\n",
      "step=374 reward=0.000000\n",
      "step=375 reward=0.000000\n",
      "step=376 reward=0.000000\n",
      "step=377 reward=0.000000\n",
      "step=378 reward=0.000000\n",
      "step=379 reward=0.000000\n",
      "step=380 reward=0.000000\n",
      "step=381 reward=0.000000\n",
      "step=382 reward=0.000000\n",
      "step=383 reward=0.000000\n",
      "step=384 reward=0.000000\n",
      "step=385 reward=0.000000\n",
      "step=386 reward=0.000000\n",
      "step=387 reward=0.000000\n",
      "step=388 reward=0.000000\n",
      "step=389 reward=0.000000\n",
      "step=390 reward=0.000000\n",
      "step=391 reward=0.000000\n",
      "step=392 reward=0.000000\n",
      "step=393 reward=0.000000\n",
      "step=394 reward=0.000000\n",
      "step=395 reward=0.000000\n",
      "step=396 reward=0.000000\n",
      "step=397 reward=0.000000\n",
      "step=398 reward=0.000000\n",
      "step=399 reward=0.000000\n",
      "step=400 reward=0.000000\n",
      "step=401 reward=0.000000\n",
      "step=402 reward=0.000000\n",
      "step=403 reward=0.000000\n",
      "step=404 reward=0.000000\n",
      "step=405 reward=0.000000\n",
      "step=406 reward=0.000000\n",
      "step=407 reward=0.000000\n",
      "step=408 reward=0.000000\n",
      "step=409 reward=0.000000\n",
      "step=410 reward=0.000000\n",
      "step=411 reward=0.000000\n",
      "step=412 reward=0.000000\n",
      "step=413 reward=0.000000\n",
      "step=414 reward=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Reset the policy and environments to prepare for rollout.\n",
    "policy.reset()\n",
    "observation, _info = env.reset(seed=42)\n",
    "\n",
    "rewards = []\n",
    "frames = [env.envs[0].render()]  # with the initial frame\n",
    "\n",
    "for step in itertools.count(1):\n",
    "    observation = convert_env_observation_to_policy_observation(observation)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(observation)\n",
    "\n",
    "    # Prepare the action for the environment.\n",
    "    action = action.to(\"cpu\").numpy()\n",
    "\n",
    "    # Step through the environment and receive a new observation.\n",
    "    observation, [reward], _terminated, [truncated], _info = env.step(action)\n",
    "    print(f\"{step=} {reward=:f}\")\n",
    "\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.envs[0].render())\n",
    "\n",
    "    if truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = output_dir / \"rollout.mp4\"\n",
    "write_video(video_path, frames, env.metadata[\"render_fps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"outputs/eval/example_aloha_pi0/rollout.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Hugging Face Version of `pi0_base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: Populate the config object by modifying sys.argv\n",
    "@parser.wrap()\n",
    "def load_config(config: TrainPipelineConfig):\n",
    "    return config\n",
    "\n",
    "\n",
    "sys.argv = sys.argv[:1] + [\n",
    "    \"--policy.path=lerobot/pi0\",\n",
    "    \"--policy.device=mps\",\n",
    "    \"--dataset.repo_id=lerobot/aloha_sim_transfer_cube_human\",\n",
    "    \"--env.type=aloha\",\n",
    "    \"--env.task=AlohaTransferCube-v0\",\n",
    "    \"--batch_size=4\",  # 8 (default) seems to be too large for 32GB M3\n",
    "    \"--steps=2000\",\n",
    "    \"--log_freq=100\",\n",
    "    \"--wandb.enable=true\",  # `uv run wandb login` is required\n",
    "]\n",
    "\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainPipelineConfig(dataset=DatasetConfig(repo_id='lerobot/aloha_sim_transfer_cube_human',\n",
      "                                          root=None,\n",
      "                                          episodes=None,\n",
      "                                          image_transforms=ImageTransformsConfig(enable=False,\n",
      "                                                                                 max_num_transforms=3,\n",
      "                                                                                 random_order=False,\n",
      "                                                                                 tfs={'brightness': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                         type='ColorJitter',\n",
      "                                                                                                                         kwargs={'brightness': (0.8,\n",
      "                                                                                                                                                1.2)}),\n",
      "                                                                                      'contrast': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                       type='ColorJitter',\n",
      "                                                                                                                       kwargs={'contrast': (0.8,\n",
      "                                                                                                                                            1.2)}),\n",
      "                                                                                      'saturation': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                         type='ColorJitter',\n",
      "                                                                                                                         kwargs={'saturation': (0.5,\n",
      "                                                                                                                                                1.5)}),\n",
      "                                                                                      'hue': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                  type='ColorJitter',\n",
      "                                                                                                                  kwargs={'hue': (-0.05,\n",
      "                                                                                                                                  0.05)}),\n",
      "                                                                                      'sharpness': ImageTransformConfig(weight=1.0,\n",
      "                                                                                                                        type='SharpnessJitter',\n",
      "                                                                                                                        kwargs={'sharpness': (0.5,\n",
      "                                                                                                                                              1.5)})}),\n",
      "                                          revision=None,\n",
      "                                          use_imagenet_stats=True,\n",
      "                                          video_backend='pyav'),\n",
      "                    env=AlohaEnv(task='AlohaTransferCube-v0',\n",
      "                                 fps=50,\n",
      "                                 features={'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>,\n",
      "                                                                   shape=(14,)),\n",
      "                                           'agent_pos': PolicyFeature(type=<FeatureType.STATE: 'STATE'>,\n",
      "                                                                      shape=(14,)),\n",
      "                                           'pixels/top': PolicyFeature(type=<FeatureType.VISUAL: 'VISUAL'>,\n",
      "                                                                       shape=(480,\n",
      "                                                                              640,\n",
      "                                                                              3))},\n",
      "                                 features_map={'action': 'action',\n",
      "                                               'agent_pos': 'observation.state',\n",
      "                                               'top': 'observation.image.top',\n",
      "                                               'pixels/top': 'observation.images.top'},\n",
      "                                 episode_length=400,\n",
      "                                 obs_type='pixels_agent_pos',\n",
      "                                 render_mode='rgb_array'),\n",
      "                    policy=None,\n",
      "                    output_dir=None,\n",
      "                    job_name=None,\n",
      "                    resume=False,\n",
      "                    seed=1000,\n",
      "                    num_workers=4,\n",
      "                    batch_size=4,\n",
      "                    steps=2000,\n",
      "                    eval_freq=20000,\n",
      "                    log_freq=100,\n",
      "                    save_checkpoint=True,\n",
      "                    save_freq=20000,\n",
      "                    use_policy_training_preset=True,\n",
      "                    optimizer=None,\n",
      "                    scheduler=None,\n",
      "                    eval=EvalConfig(n_episodes=50,\n",
      "                                    batch_size=50,\n",
      "                                    use_async_envs=False),\n",
      "                    wandb=WandBConfig(enable=True,\n",
      "                                      disable_artifact=False,\n",
      "                                      project='lerobot',\n",
      "                                      entity=None,\n",
      "                                      notes=None,\n",
      "                                      run_id=None))\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(config)\n",
    "# train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
